[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multivariable Mathematics for Data Science",
    "section": "",
    "text": "This book will introduce students to multivariable Calculus and linear algebra methods and techniques to be successful in data science, statistics, computer science, and other data-driven, computational disciplines.\nThe motiviation for this text is to provide both a theoretical understanding of important multivariable methods used in data science as well as giving a hands-on experience using software. Throughout this text, we assume the reader has a solid foundation in univariate calculus (typically two semesters) as well as familiarity with a scripting language (e.g., R or python).\n\n\nTBD\n\n\n\n\n3 Blue 1 Brown – Essence of Linear Algebra\n3 Blue 1 Brown – Vectors\n\n\n\n\nFor notation, we let lowercase Roman letters represent scalar numbers (e.g., n = 5, d = 7), lowercase bold letters represent vectors\n\\[\n\\begin{aligned}\n\\textbf{x} = \\begin{pmatrix}  x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix},\n\\end{aligned}\n\\]\nwhere the elements \\(x_1, \\ldots, x_n\\) are scalars written in lowercase Roman. Note that vectors are assumed to follow a vertical notation where the elements of the vector (the \\(x_i\\)s are stacked on top of one another) and the order matters. For example, the vector\n\\[\n\\begin{aligned}\n\\mathbf{x} & = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\n\\end{aligned}\n\\]\nhas the first element \\(x_1 = 5\\), second element \\(x_2 = 2\\) and third element \\(x_3 = 8\\). Note that the vector \\(\\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) is not the same as the vector \\(\\begin{pmatrix} 8 \\\\ 2 \\\\ 5 \\end{pmatrix}\\) because the order of the elements matters.\nWe can also write the vector as\n\\[\n\\begin{aligned}\n\\textbf{x} = \\left(  x_1, x_2, \\ldots, x_n \\right)',\n\\end{aligned}\n\\]\nwhere the \\('\\) symbol represents the transpose function. For our example matrix, we have \\(\\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}' = \\begin{pmatrix} 5 & 2 & 8 \\end{pmatrix}\\) which is the original vector but arranged in a row rather than a column. Likewise, the transpose of a row vector \\(\\begin{pmatrix} 5 & 2 & 8 \\end{pmatrix}' = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) is a column vector. If \\(\\mathbf{x}\\) is a column vector, we say that \\(\\mathbf{x}'\\) is a row vector and if \\(\\mathbf{x}\\) is a row vector, the \\(\\mathbf{x}'\\) is a column vector.\nTo create a vector we can use the concatenate function c(). For example, the vector \\(\\mathbf{x} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) can be created as the R object using\n\nx <- c(5, 2, 8)\n\nwhere the <- assigns the values in the vector c(5, 2, 8) to the object named x. To print the values of x, we can use\n\nx\n\n[1] 5 2 8\n\n\nwhich prints the elements of x. Notice that R prints the elements of \\(\\mathbf{x}\\) in a row; however, \\(\\mathbf{x}\\) is a column vector. This inconsistency is present to allow the output to be printed in a manner easier to read (more numbers fit on a row). If we put the column vector into a data.frame, then the vector will be presented as a column vector\n\ndata.frame(x)\n\n  x\n1 5\n2 2\n3 8\n\n\nOne can use the index operator \\([\\ ]\\) to select specific elements of the vector \\(\\mathbf{x}\\). For example, the first element of \\(\\mathbf{x}\\), \\(x_1\\), is\n\nx[1]\n\n[1] 5\n\n\nand the third element of \\(\\mathbf{x}\\), \\(x_3\\), is\n\nx[3]\n\n[1] 8\n\n\nThe transpose function t() turns a column vector into a row vector (or a row vector into a column vector). For example the transpose \\(\\mathbf{x}'\\) of \\(\\mathbf{x}\\) is\n\ntx <- t(x)\ntx\n\n     [,1] [,2] [,3]\n[1,]    5    2    8\n\n\nwhere tx is R object storing the transpose of \\(\\mathbf{x}\\) and is a row vector. The transpose of tx. Notice the indices on the output of the row vector tx. The index operator [1, ] selects the first row to tx and the index operator [, 1] gives the first column tx. Taking the transpose again gives us back the original column vector\n\nt(tx)\n\n     [,1]\n[1,]    5\n[2,]    2\n[3,]    8\n\n\n\n\nWe let uppercase bold letters \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), etc., represent matrices. We define the matrix \\(\\mathbf{A}\\) with \\(m\\) rows and \\(n\\) columns as\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{pmatrix},\n\\end{aligned}\n\\]\nwith \\(a_{ij}\\) being the value of the matrix \\(\\mathbf{A}\\) in the \\(i\\)th row and the \\(j\\)th column.\nIf the matrix\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\begin{pmatrix} 5 & 7 & 1 \\\\\n5 & -22  & 2 \\\\\n-14 & 5 & 99 \\\\\n42 & -3 & 0\\end{pmatrix},\n\\end{aligned}\n\\]\nthe elements \\(a_{11}\\) = 5, \\(a_{12}\\) = 7, \\(a_{21}\\) = 5, and \\(a_{33}\\) = 99, etc.\nIn R, we can define the matrix A using the matrix() function\n\nA <- matrix(\n    data = c(5, 5, -14, 42, 7, -22, 5, -3, 1, 2, 99, 0),\n    nrow = 4,\n    ncol = 3\n)\n\nA\n\n     [,1] [,2] [,3]\n[1,]    5    7    1\n[2,]    5  -22    2\n[3,]  -14    5   99\n[4,]   42   -3    0\n\n\nNotice in the above creation of \\(\\mathbf{A}\\), we wrote defined the elements of the \\(\\mathbf{A}\\) using the columns stacked on top of one another. If we want to fill in the elements of \\(\\mathbf{A}\\) using the rows, we can add the option byrow = TRUE to the matrix() function\n\nA <- matrix(\n    data  = c(5, 7, 1, 5, -22, 2, -14, 5, 99, 42, -3, 0), \n    nrow  = 4,\n    ncol  = 3,\n    byrow = TRUE\n)\nA\n\n     [,1] [,2] [,3]\n[1,]    5    7    1\n[2,]    5  -22    2\n[3,]  -14    5   99\n[4,]   42   -3    0\n\n\nTo select the \\(ij\\)th elements of \\(\\mathbf{A}\\), we use the subset operator [ to select the element. For example, to get the element \\(a_{11} = 5\\) in the first row and first column of \\(\\mathbf{A}\\), we use\n\nA[1, 1]\n\n[1] 5\n\n\nThe element \\(a_{3, 3} = 99\\) in the third row and third column can be selected using\n\nA[3, 3]\n\n[1] 99\n\n\nThe matrix \\(\\mathbf{A}\\) can also be represented as a set of either column vectors \\(\\{\\mathbf{c}_j \\}_{j=1}^n\\) or row vectors \\(\\{\\mathbf{r}_i \\}_{i=1}^m\\). For example, the column vector representation is\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\left( \\mathbf{c}_{1} \\middle| \\mathbf{c}_{2} \\middle| \\cdots \\middle| \\mathbf{c}_{n} \\right),\n\\end{aligned}\n\\]\nwhere the notation \\(|\\) is used to separate the vectors\n\\[\n\\begin{aligned}\n\\mathbf{c}_1 & = \\begin{pmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1}\n\\end{pmatrix},\n& \\mathbf{c}_2 & = \\begin{pmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2}\n\\end{pmatrix},\n& \\cdots, &\n& \\mathbf{c}_n & = \\begin{pmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn}\n\\end{pmatrix}\n\\end{aligned}\n\\]\nIn R you can extract the columns using the [ selection operator\n\nc1 <- A[, 1] # first column\nc2 <- A[, 2] # second column\nc3 <- A[, 3] # third column\n\nand you can give the column representation of the matrix A with with column bind function cbind()\n\ncbind(c1, c2, c3)\n\n      c1  c2 c3\n[1,]   5   7  1\n[2,]   5 -22  2\n[3,] -14   5 99\n[4,]  42  -3  0\n\n\nThe row vector representation of \\(\\mathbf{A}\\) is\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\begin{pmatrix} \\mathbf{r}_{1} \\\\ \\mathbf{r}_{2} \\\\ \\vdots \\\\ \\mathbf{r}_{m} \\end{pmatrix},\n\\end{aligned}\n\\]\nwhere the row vectors \\(\\mathbf{r}_i\\) are\n\\[\n\\begin{aligned}\n\\mathbf{r}_1 & = \\left( a_{11}, a_{12}, \\ldots, a_{1n} \\right) \\\\\n\\mathbf{r}_2 & = \\left( a_{21}, a_{22}, \\ldots, a_{2n} \\right) \\\\\n& \\vdots \\\\\n\\mathbf{r}_m & = \\left( a_{m1}, a_{m2}, \\ldots, a_{mn} \\right)\n\\end{aligned}\n\\]\nIn R you can extract the rows using the [ selection operator\n\nr1 <- A[1, ] # first row\nr2 <- A[2, ] # second row\nr3 <- A[3, ] # third row\nr4 <- A[4, ] # fourth row\n\nand you can give the row representation of the matrix A with with row bind function rbind()\n\nrbind(r1, r2, r3, r4)\n\n   [,1] [,2] [,3]\nr1    5    7    1\nr2    5  -22    2\nr3  -14    5   99\nr4   42   -3    0"
  },
  {
    "objectID": "01-matrix-operations.html",
    "href": "01-matrix-operations.html",
    "title": "2  Matrix operations",
    "section": "",
    "text": "3 Blue 1 Brown – Matrix Multiplication\nNote: add examples:"
  },
  {
    "objectID": "01-matrix-operations.html#the-dotinner-product",
    "href": "01-matrix-operations.html#the-dotinner-product",
    "title": "2  Matrix operations",
    "section": "2.2 The dot/inner product",
    "text": "2.2 The dot/inner product\n\nDefinition 2.1 Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors in \\(\\mathcal{R}^n\\). Then, the dot product (also called the inner product) of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is \\(\\mathbf{u}' \\mathbf{v}\\). The vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are \\(n \\times 1\\) matrices (\\(n\\) rows and one column) where \\(\\mathbf{u}'\\) is a \\(1 \\times n\\) matrix and the inner product \\(\\mathbf{u}' \\mathbf{v}\\) is a scalar (\\(1 \\times 1\\) matrix). The inner product is also sometimes called the dot product and written as \\(\\mathbf{u} \\cdot \\mathbf{v}\\).\nIf the vectors\n\\[\n\\begin{aligned}\n\\mathbf{u} = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{pmatrix} & & \\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix}\n\\end{aligned}\n\\]\nthen \\(\\mathbf{u}' \\mathbf{v} = u_1 v_1 + u_2 v_2 + \\cdots u_n v_n\\)\n\n\nExample 2.1 Find the inner product \\(\\mathbf{u}'\\mathbf{v}\\) and \\(\\mathbf{v}'\\mathbf{u}\\) of\n\\[\n\\begin{aligned}\n\\mathbf{u} = \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} & & \\mathbf{v} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 3 \\end{pmatrix}\n\\end{aligned}\n\\]\n\ndo by hand\n\n\nu <- c(2, -3, 1)\nv <- c(4, -2, 3)\n# u'v\nsum(u*v)\n\n[1] 17\n\nt(u) %*% v\n\n     [,1]\n[1,]   17\n\n# v'u\nsum(v*u)\n\n[1] 17\n\nt(v) %*% u\n\n     [,1]\n[1,]   17\n\n\n\nThe properties of inner products are defined with the following theorem.\n\nTheorem 2.1 (Inner Product) Let \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) be vectors in \\(\\mathcal{R}^n\\) and let \\(c\\) be a scalar. Then\n\n\\(\\mathbf{u}'\\mathbf{v} = \\mathbf{v}'\\mathbf{u}\\)\n\\((\\mathbf{u} + \\mathbf{v})' \\mathbf{w} = \\mathbf{u}' \\mathbf{w} + \\mathbf{v}' \\mathbf{w}\\)\n\\(( c \\mathbf{u} )' \\mathbf{v} = c ( \\mathbf{v}'\\mathbf{u} )\\)\n\\(\\mathbf{u}'\\mathbf{u} \\geq 0\\) with \\(\\mathbf{u}'\\mathbf{u} = 0\\) only when \\(\\mathbf{u} = \\mathbf{0}\\)"
  },
  {
    "objectID": "01-matrix-operations.html#properties-of-matrices",
    "href": "01-matrix-operations.html#properties-of-matrices",
    "title": "2  Matrix operations",
    "section": "2.3 Properties of matrices",
    "text": "2.3 Properties of matrices\n\n2.3.1 Matrix Addition\nMatrix Addition: If the matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of the same dimension (e.g., both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) have the same number of rows \\(m\\) and the same number of columns \\(n\\)), then\n\\[\n\\begin{aligned}\n\\mathbf{A} + \\mathbf{B} & = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{pmatrix} +\n\\begin{pmatrix} b_{11} & b_{12} & \\cdots & b_{1n} \\\\\nb_{21} & b_{22} & \\cdots & b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{m1} & b_{m2} & \\cdots & b_{mn}\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix} a_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn}\n\\end{pmatrix} \\\\\n& = \\left\\{ a_{ij} + b_{ij} \\right\\}\n\\end{aligned}\n\\tag{2.1}\\]\n… Another way to \nIf \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of the same dimension (same number of rows and columns) you can add the matrices together\n\nA <- matrix(c(4, 1, 33, 2, 0, -4), 3, 2)\nB <- matrix(c(7, -24, 3, 9, 11, -9), 3, 2)\nA\n\n     [,1] [,2]\n[1,]    4    2\n[2,]    1    0\n[3,]   33   -4\n\nB\n\n     [,1] [,2]\n[1,]    7    9\n[2,]  -24   11\n[3,]    3   -9\n\nA + B\n\n     [,1] [,2]\n[1,]   11   11\n[2,]  -23   11\n[3,]   36  -13\n\n\nWe can also write this using for loops\n\n# initialize an empty matrix to fill\nC <- matrix(0, 3, 2)\n\nfor (i in 1:nrow(A)) {         # loop over the rows\n    for (j in 1:ncol(A)) {     # loop over the columns\n        C[i, j] <- A[i, j] + B[i, j]\n    }\n}\nC\n\n     [,1] [,2]\n[1,]   11   11\n[2,]  -23   11\n[3,]   36  -13\n\n\nIf \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of different dimensions (they differ in either the number of rows or columns), R will return an error warning you that the matrices are of different sizes and can’t be added\n\nA <- matrix(c(4, 1, 33, 2, 0, -4), 3, 2)\nB <- matrix(c(7, -24, 3, 9), 2, 2)\nA\n\n     [,1] [,2]\n[1,]    4    2\n[2,]    1    0\n[3,]   33   -4\n\nB\n\n     [,1] [,2]\n[1,]    7    3\n[2,]  -24    9\n\nA + B\n\nError in A + B: non-conformable arrays\n\n\n\nTheorem 2.2 Let \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) be \\(m \\times n\\) matrices and let \\(a\\) and \\(b\\) be scalars, then:\n\n\\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\) \n\\((\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\) \n\\(\\mathbf{A} + \\mathbf{0} = \\mathbf{A}\\) \n\\(a (\\mathbf{A} + \\mathbf{B}) = a \\mathbf{A} + a \\mathbf{B}\\) \n\\((a + b)\\mathbf{A} = a \\mathbf{A} + b \\mathbf{A}\\) \n\\((ab)\\mathbf{A} = a (b\\mathbf{A})\\) \n\n\n\n\n2.3.2 Matrix Multiplication\nMatrix Multiplication: If \\(\\mathbf{A} = \\left\\{ a_{ij} \\right\\}\\) is an \\(m \\times n\\) matrix and \\(\\mathbf{B} = \\left\\{ b_{jk} \\right\\}\\) is a \\(n \\times p\\) matrix, then the matrix product \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) is an \\(m \\times p\\) matrix where \\(\\mathbf{C} = \\left\\{ \\sum_{j=1}^n a_{ij} b_{jk} \\right\\}\\)\n\\[\n\\begin{aligned}\n\\mathbf{A} \\mathbf{B} & = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{pmatrix}\n\\begin{pmatrix} b_{11} & b_{12} & \\cdots & b_{1p} \\\\\nb_{21} & b_{22} & \\cdots & b_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{np}\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix} \\sum_{j=1}^n a_{1j} b_{j1} & \\sum_{j=1}^n a_{1j} b_{j2} & \\cdots & \\sum_{j=1}^n a_{1j} b_{jp} \\\\\n\\sum_{j=1}^n a_{2j} b_{j1} &\\sum_{j=1}^n a_{2j} b_{j2} & \\cdots & \\sum_{j=1}^n a_{2j} b_{jp} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sum_{j=1}^n a_{mj} b_{j1} &\\sum_{j=1}^n a_{mj} b_{j2} & \\cdots & \\sum_{j=1}^n a_{mj} b_{jp}\n\\end{pmatrix}\n\\end{aligned}\n\\tag{2.2}\\]\nAnother way to define matrix multiplication is through inner product notation. Define the \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and the \\(n \\times p\\) matrix \\(\\mathbf{B}\\) as the partition\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\begin{pmatrix} \\mathbf{a}_{1}' \\\\\n\\mathbf{a}_{2}' \\\\\n\\vdots \\\\\n\\mathbf{a}_{m}' \\end{pmatrix}\n& \\mbox{ and }\n&& \\mathbf{B} & = \\begin{pmatrix} \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{p} \\end{pmatrix}\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{a}_i\\) and \\(\\mathbf{b}_k\\) are both \\(n\\)-vectors. Then, we have \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) can be written as\n\\[\n\\begin{aligned}\n\\mathbf{A} \\mathbf{B}  = \\mathbf{A} \\begin{pmatrix} \\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p\n\\end{pmatrix}  = \\begin{pmatrix} \\mathbf{A} \\mathbf{b}_1 & \\mathbf{A} \\mathbf{b}_2 & \\cdots & \\mathbf{A} \\mathbf{b}_p\n\\end{pmatrix}\n\\end{aligned}\n\\]\nNote that in this representation, each column of the matrix \\(\\mathbf{A}\\mathbf{B}\\) is a linear combination the the columns of \\(\\mathbf{A}\\) with coefficients given by the corresponding column of \\(\\mathbf{B}\\).\n\\[\n\\begin{aligned}\n\\mathbf{A} \\mathbf{B} & = \\begin{pmatrix} \\mathbf{a}_1' \\mathbf{b}_1 & \\mathbf{a}_1' \\mathbf{b}_2 & \\cdots & \\mathbf{a}_1' \\mathbf{b}_p \\\\\n\\mathbf{a}_2' \\mathbf{b}_1 & \\mathbf{a}_2' \\mathbf{b}_2 & \\cdots & \\mathbf{a}_2' \\mathbf{b}_p \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{a}_m' \\mathbf{b}_1 & \\mathbf{a}_m' \\mathbf{b}_2  & \\cdots & \\mathbf{a}_m' \\mathbf{b}_p\n\\end{pmatrix} \\\\\n& = \\left\\{ \\mathbf{a}_i' \\mathbf{b}_k  \\right\\}.\n\\end{aligned}\n\\]\nWritten in this notation, we arrive at the multiplication rule for \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) – the \\(ik\\)th element \\(c_{ik}\\) of \\(\\mathbf{C}\\) is the inner product of the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\).\n\n\n2.3.3 Properties of Matrix Multiplication\nDefine the \\(m \\times m\\) identity matrix \\(\\mathbf{I}_m\\) with ones on the diagonal and zeros off diagonal as\n\\[\n\\mathbf{I}_m = \\begin{pmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ 0 & 0 & \\ddots & 0 \\\\ 0 & 0 & \\cdots & 1\\end{pmatrix}\n\\]\nLet \\(\\mathbf{A}\\) be an \\(m \\times n\\) matrix, then:\n\nLet \\(\\mathbf{B}\\) be an \\(n \\times p\\) matrix and \\(\\mathbf{C}\\) a \\(p \\times q\\) matrix. Then \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\) is an \\(m \\times q\\) matrix.\nLet \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) be \\(n \\times p\\) matrices. Then \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\\) is an \\(m \\times p\\) matrix.\nLet \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) be \\(p \\times m\\) matrices. Then \\((\\mathbf{B} + \\mathbf{C})\\mathbf{A} = \\mathbf{B}\\mathbf{A} + \\mathbf{C}\\mathbf{A}\\) is an \\(p \\times m\\) matrix.\nLet \\(\\mathbf{B}\\) be an \\(n \\times p\\) matrix and \\(c\\) a scalar. Then \\(c(\\mathbf{A} \\mathbf{B}) = (c \\mathbf{A}) \\mathbf{B} = \\mathbf{A}(c\\mathbf{B})\\) is an \\(m \\times p\\) matrix.\n\\(\\mathbf{I}_m \\mathbf{A} = \\mathbf{A} \\mathbf{I}_n = \\mathbf{A}\\)\n\nExamples: in class\nNote: Matrix multiplication violates some of the rules of multiplication that you might be used to. Pay attention for the following:\n\nIn general \\(\\mathbf{A} \\mathbf{B} \\neq \\mathbf{B} \\mathbf{A}\\) (sometimes these are equal, but usually are not)\n\\(\\mathbf{A}\\mathbf{B} = \\mathbf{A} \\mathbf{C}\\) does not imply \\(\\mathbf{B} = \\mathbf{C}\\)\n\\(\\mathbf{A}\\mathbf{B} = \\mathbf{0}\\) does not imply that \\(\\mathbf{A} = \\mathbf{0}\\) or \\(\\mathbf{B} = \\mathbf{0}\\)\n\n\n\n2.3.4 Matrix Multiplication complexity (Big O notation)\nIn the study of algorithms, the notation \\(O(n)\\) is used to describe the number of calculations that need to be done to evaluate the equation. As an example, consider \\(\\mathbf{A} = \\begin{pmatrix}3 & 1 \\\\ 2 & -3 \\end{pmatrix}\\), \\(\\mathbf{B} = \\begin{pmatrix} -2 & 4 \\\\ -1 & 2 \\end{pmatrix}\\), and \\(\\mathbf{x} = \\begin{pmatrix} -3 \\\\ 1 \\end{pmatrix}\\).\nBy hand: Calculate\n\n\\((\\mathbf{A} \\mathbf{B}) \\mathbf{x}\\)\n\\(\\mathbf{A} (\\mathbf{B} \\mathbf{x})\\)\n\nWhich was easier? Which required less calculation?\n\nMatrix-matrix multiplication of and \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and an \\(n \\times p\\) matrix \\(\\mathbf{B}\\) has complexity \\(O(m n p)\\).\nMatrix-vector multiplication of and \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and an \\(n\\)-vector \\(\\mathbf{x}\\) has complexity \\(O(n m)\\).\n\nFrom example above:\n\n\\(O(m n p)\\) matrix-matrix multiplication \\((\\mathbf{A} \\mathbf{B})\\) followed by \\(O(m n)\\) matrix-vector multiplication \\((\\mathbf{A} \\mathbf{B}) \\mathbf{x}\\) which has computational complexity \\(O(m n p) + O(m n)\\)\n\\(O(m n)\\) matrix-vector multiplication \\((\\mathbf{B} \\mathbf{x})\\) followed by \\(O(m n)\\) matrix-vector multiplication \\(\\mathbf{A} (\\mathbf{B} \\mathbf{x})\\) which has computational complexity \\(O(m n) + O(m n)\\)\n\n\n\n2.3.5 Matrix powers\nPowers of a \\(n \\times n\\) (square) matrix are defined as the product of \\(\\mathbf{A}\\) multiplied \\(k\\) times\n\\[\n\\mathbf{A}^k = \\underbrace{\\mathbf{A} \\cdots \\mathbf{A}}_k\n\\]\n\n\n2.3.6 Matrix Transpose\nThe matrix transpose is an operator that swaps the rows and columns of a matrix. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix (\\(m\\) rows and \\(n\\) columns), then \\(\\mathbf{A}'\\) is a \\(n \\times m\\) matrix (\\(n\\) rows and \\(m\\) columns. Note: some use \\(\\mathbf{A}^T\\) to denote a transpose; I prefer the \\('\\) notation as it is much simpler and cleaner notation). The matrix\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{pmatrix}\n\\end{aligned}\n\\]\nhas transpose\n\\[\n\\begin{aligned}\n\\mathbf{A}' & = \\begin{pmatrix} a_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn}\n\\end{pmatrix},\n\\end{aligned}\n\\]\n\nTheorem 2.3 Let \\(\\mathbf{A}\\) be an \\(m \\times n\\) matrix, then\n\n\\((\\mathbf{A}')' = \\mathbf{A}\\).\nLet \\(\\mathbf{B}\\) be an \\(m \\times n\\) matrix, then \\((\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}'\\).\nFor any scalar \\(c\\), \\((c \\mathbf{A})' = c \\mathbf{A}'\\).\nLet \\(\\mathbf{B}\\) be an \\(n \\times p\\) matrix, then \\(( \\mathbf{A} \\mathbf{B})' = \\mathbf{B}' \\mathbf{A}'\\) is an \\(p \\times m\\) matrix.\n\n\nNote: The power of video games: GPUs and modern CPUs are becoming more and more parallelized. Because the \\(ij\\)th element of \\(\\mathbf{A}\\mathbf{B}\\) requires only the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\), matrix multiplication is easily parallelized under modern computing architectures. Thanks to video games, this parallelization has been made faster than ever.\nExamples: in class"
  },
  {
    "objectID": "03-vector-spaces.html",
    "href": "03-vector-spaces.html",
    "title": "3  Vectors spaces",
    "section": "",
    "text": "3 Blue 1 Brown – Linear combinations, span, and basis vectors"
  },
  {
    "objectID": "03-vector-spaces.html#vectors",
    "href": "03-vector-spaces.html#vectors",
    "title": "3  Vectors spaces",
    "section": "3.1 Vectors",
    "text": "3.1 Vectors\n\n3.1.1 Properties of Vectors\nFor any real valued scalars \\(a, b \\in \\mathcal{R}\\) and any vectors \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathcal{R}^n\\) (vectors of real numbers of length \\(n\\)),\n\nscalar multiplication\n\n\\[\n\\begin{aligned}\na \\mathbf{x} & = a \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\\\\n& = \\begin{pmatrix} a x_1 \\\\ a x_2 \\\\ \\vdots \\\\ a x_n \\end{pmatrix}\n\\end{aligned}\n\\]\nwhere the scalar \\(a\\) is multiplied by each element of the vector. For example,\n\\[\n\\begin{aligned}\n4 \\begin{pmatrix} 4 \\\\ 6 \\\\ 7 \\\\ 12 \\end{pmatrix}\n& = \\begin{pmatrix} 4 * 4 \\\\ 4 * 6 \\\\ 4 * 7 \\\\ 4 * 12 \\end{pmatrix} \\\\\n& = \\begin{pmatrix} 16 \\\\ 24 \\\\ 28 \\\\ 48 \\end{pmatrix}\n\\end{aligned}\n\\]\nIn R, we can multiply the vector by a scalar as\n\n4 * c(4, 6, 7, 12)\n\n[1] 16 24 28 48\n\n\nor if the vector \\(\\mathbf{x} = \\left( 4, 6, 7, 12 \\right)'\\) we can write this as\n\nx <- c(4, 6, 7, 12)\n4 * x\n\n[1] 16 24 28 48\n\n\n\nscalar multiplicative commutivity\n\n\\[\n\\begin{aligned}\na (b \\mathbf{x}) & = (ab) \\mathbf{x} & = b (a \\mathbf{x})\n\\end{aligned}\n\\]\n\n4 * (6 * x) \n\n[1]  96 144 168 288\n\n(4 * 6) * x\n\n[1]  96 144 168 288\n\n\n\nscalar additive associativity\n\n\\[\n\\begin{aligned}\na \\mathbf{x} + b \\mathbf{x} & = (a + b) \\mathbf{x}\n\\end{aligned}\n\\]\n\nvector additive associativity\n\n\\[\n\\begin{aligned}\na \\mathbf{x} + a \\mathbf{y} & = a (\\mathbf{x} + \\mathbf{y})\n\\end{aligned}\n\\]\n\nvector associativity\n\n\\[\n\\begin{aligned}\n\\mathbf{x} + \\mathbf{y} & = \\mathbf{y} + \\mathbf{x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n(\\mathbf{x} + \\mathbf{y}) + \\mathbf{z} & = \\mathbf{x} + (\\mathbf{y} + \\mathbf{z})\n\\end{aligned}\n\\]\n\nx <- c(1, 2, 3, 4)\ny <- c(4, 3, 5, 1)\nz <- c(5, 2, 4, 6)\n\nx + y\n\n[1] 5 5 8 5\n\ny + x\n\n[1] 5 5 8 5\n\n(x + y) + z\n\n[1] 10  7 12 11\n\nx + (y + z)\n\n[1] 10  7 12 11\n\n\n\nIdentity Element of Addition: For any vector \\(\\mathbf{x}\\) of length \\(n\\), there exists a vector \\(\\mathbf{0}\\), known as the zero vector, such that\n\n\\[\n\\begin{aligned}\n\\mathbf{x} + \\mathbf{0} & = \\mathbf{x}\n\\end{aligned}\n\\]\n\nx + 0\n\n[1] 1 2 3 4\n\nx + rep(0, 4)\n\n[1] 1 2 3 4\n\n\n\nInverse Element of Addition: For any vector \\(\\mathbf{x}\\) of length \\(n\\), there exists a vector \\(-\\mathbf{x}\\), known as the additive inverse vector, such that\n\n\\[\n\\begin{aligned}\n\\mathbf{x} + (- \\mathbf{x}) & = \\mathbf{0}\n\\end{aligned}\n\\]\n\nx + (-x)\n\n[1] 0 0 0 0"
  },
  {
    "objectID": "03-vector-spaces.html#vector-addition",
    "href": "03-vector-spaces.html#vector-addition",
    "title": "3  Vectors spaces",
    "section": "3.2 Vector addition",
    "text": "3.2 Vector addition\nTwo vectors of length \\(n\\) can be added elementwise\n\\[\n\\begin{aligned}\n\\mathbf{x} + \\mathbf{y} & = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} + \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} \\\\\n& = \\begin{pmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\end{aligned}\n\\]\nFor example,\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 3 \\\\ 1 \\\\ -4 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} -3 \\\\ 17 \\\\ -39 \\\\ 4 \\end{pmatrix} & = \\begin{pmatrix} 3 + (-3) \\\\ 1 + 17 \\\\ -4 + (-39) \\\\ 3 + 4 \\end{pmatrix} \\\\\n& = \\begin{pmatrix} 0 \\\\ 18 \\\\ -43 \\\\ 7 \\end{pmatrix}\n\\end{aligned}\n\\]\nIn R, we have\n\nx <- c(3, 1, -4, 3)\ny <- c(-3, 17, -39, 4)\nx + y\n\n[1]   0  18 -43   7\n\n\nIf two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are of different lengths, then they cannot be added together. Using R, we get the following error:\n\nx <- c(1, 2, 3)\ny <- c(1, 2, 3, 4)\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 5\n\n\nThe error is telling us that the vector \\(\\mathbf{x}\\) and the vector \\(\\mathbf{y}\\) do not have the same length.\nBe careful when adding vectors in R. R uses “recycling” which means two vectors of different lengths can be added together if one vector is of a length that is a multiple of the other vector. For example, if \\(\\mathbf{x} = (1, 2)'\\) is a vector of length 2 and \\(\\mathbf{y} = (1, 2, 3, 4)\\) is a vector of length 4, R will add \\(\\mathbf{x} + \\mathbf{y}\\) by replicating the vector \\(\\mathbf{x}\\) twice (i.e., \\(\\mathbf{x} + \\mathbf{y} = \\left( \\mathbf{x}', \\mathbf{x}' \\right)' = \\left(1, 2, 1, 2 \\right)' + \\mathbf{y}\\))\n\nx <- c(1, 2)\ny <- c(1, 2, 3, 4)\nx + y\n\n[1] 2 4 4 6\n\n# replicated x = c(1, 2, 1, 2)\nc(1, 2, 1, 2) + y\n\n[1] 2 4 4 6\n\n\n\n3.2.1 The geometric interpretation of vectors in \\(\\mathcal{R}^2\\)\nLet \\(\\mathcal{R}^2\\) be a real coordinate space of \\(2\\) dimensions. You are already familiar with the Cartesian plane that consists of ordered pairs \\((x, y)\\). The Cartesian plane defines the real coordinate space \\(\\mathbf{R}^2\\) of two dimensions. In \\(\\mathbf{R}^2\\), the location of any point of interest can be defined using the \\(x\\) and \\(y\\). For example, the plot below shows the location of the point (2, 3)\n\ndat <- data.frame(\n    x = 2,\n    y = 3\n)\n\nggplot(data = dat, aes(x = x, y = y)) +\n    geom_point() +\n    geom_vline(xintercept = 0) + \n    geom_hline(yintercept = 0) +\n    coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4))\n\n\n\n\nA vector space is a generalization of this representation. In \\(\\mathcal{R}^2\\), we say that the vector \\(\\mathbf{z} = c(2, 3)\\) is centered at the origin (0, 0) and has length 2 in the \\(x\\)-axis and length 3 in the \\(y\\)-axis. The plot below shows this vector\n\n\n\n\n\nWe can also decompose the vector \\(\\mathbf{z}\\) into its \\(x\\) and \\(y\\) components. The \\(x\\) component of \\(\\mathbf{z}\\) is (2, 0) and the \\(y\\) component of \\(\\mathbf{z}\\) is (0, 3). The following plot shows the \\(x\\) component (2, 0) in blue and the \\(y\\) component (0, 3) in red.\n\n\n\n\n\nThe below Shiny app allows you to plot the vector for any \\((x, y)\\) pair of your choosing.\n\n\n\nThe shiny app can be downloaded and run on your own computer using\n\nlibrary(shiny)\nrunGitHub(rep = \"multivariable-math\", \n          username = \"jtipton25\",\n          subdir = \"shiny-apps/chapter-03/vector-space\") \n\n\nAddition of vectors\nWe can represent the addition of vectors geometrically as well. Consider the two vectors \\(\\mathbf{u}\\) = (3, 2) and \\(\\mathbf{v}\\) = (-2, 1) where \\(\\mathbf{u} + \\mathbf{v}\\) = (1, 3).\n\ndata.frame(x = c(3, -2, 1), y = c(2, 1, 3), vector = c(\"u\", \"v\", \"u+v\")) %>%\n    ggplot() +\n    geom_point(aes(x = x, y = y, color = vector)) +\n    geom_vline(xintercept = 0) + \n    geom_hline(yintercept = 0) +\n    coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4))\n\n\n\n\nWe can represent the sum using vectors by adding \\(\\mathbf{u}\\) first then adding \\(\\mathbf{v}\\) to \\(\\mathbf{u}\\) or by adding \\(\\mathbf{v}\\) first and then \\(\\mathbf{u}\\) to get\n\ndf <- data.frame(x = c(0, 3, 1, -2), y = c(0, 2, 3, 1))\np1 <- ggplot() +\n    geom_segment(aes(x = 0, xend = 3, y = 0, yend = 2), arrow = arrow(), color = \"blue\") +\n    geom_segment(aes(x = 3, xend = 3 - 2, y = 2, yend = 2 + 1), arrow = arrow(), color = \"red\") +\n    geom_segment(aes(x = 0, xend = 3 - 2, y = 0, yend = 2 + 1), arrow = arrow(), color = \"black\") +\n    geom_vline(xintercept = 0) + \n    geom_hline(yintercept = 0) +\n    coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + \n    geom_polygon(data = df, aes(x = x, y = y), fill = \"grey\", alpha = 0.5) +\n    ggtitle(\"u + v\")\np2 <- ggplot() +\n    geom_segment(aes(x = 0, xend = -2, y = 0, yend = 1), arrow = arrow(), color = \"red\") +\n    geom_segment(aes(x = -2, xend = -2 + 3, y = 1, yend = 1 + 2), arrow = arrow(), color = \"blue\") +\n    geom_segment(aes(x = 0, xend = 3 - 2, y = 0, yend = 2 + 1), arrow = arrow(), color = \"black\") +\n    geom_vline(xintercept = 0) + \n    geom_hline(yintercept = 0) +\n    coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) +\n    geom_polygon(data = df, aes(x = x, y = y), fill = \"grey\", alpha = 0.5) +\n    ggtitle(\"v + u\")\np1 + p2\n\n\n\n\nNotice that the sum of these vectors defines a parallelogram where the sum \\(\\mathbf{u} + \\mathbf{v}\\) is the diagonal of the shaded parallelogram. This geometric interpretation will serve as a basis for interpreting vector equations in higher dimensions where typical visualization methods fail.\n\n\n\n3.2.2 Scalar multiplication of vectors\nWe can represent the multiplication of a vector by a scalar geometrically as well. Consider the vector \\(\\mathbf{u}\\) = (3, 2) and the scalars \\(a = 2\\) and \\(b = -1\\). Then, we can plot \\(\\mathbf{u}\\), \\(a\\mathbf{u}\\), and \\(b\\mathbf{u}\\).\n\ndata.frame(x = c(3, 2 * 3, -1 * 3), y = c(2, 2 * 2, -1 * 2), vector = c(\"u\", \"a*u\", \"b*u\")) %>%\n    ggplot() +\n    geom_point(aes(x = x, y = y, color = vector)) +\n    geom_segment(aes(x = 0, xend = x, y = 0, yend = y, color = vector), arrow = arrow(), alpha = 0.75) +\n    geom_vline(xintercept = 0) + \n    geom_hline(yintercept = 0) +\n    coord_cartesian(xlim = c(-6, 6), ylim = c(-6, 6))\n\n\n\n\nIn fact, if \\(a\\) is allowed to take on any values, then the set of all possible values of \\(a \\mathbf{u}\\) for all values of \\(a\\) defines an infinite line\n\nggplot() +\n    geom_abline(slope = 2/3, intercept = 0) +  \n    geom_point(aes(x = 3, y = 2)) +\n    geom_segment(aes(x = 0, xend = 3, y = 0, yend = 2), arrow = arrow(), color = \"black\") +\n    geom_vline(xintercept = 0) + \n    geom_hline(yintercept = 0) +\n    coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) \n\n\n\n\n\n\n3.2.3 The geometric interpretation of vectors in \\(\\mathcal{R}^3\\)\nLet the vector \\(\\mathbf{u} = c(-2, 3, 5)\\). Then, the figure below shows the vector in 3 dimensions.\nDraw picture by hand\n\n\n\n\n\n3.2.4 The geometric interpretation of vectors in \\(\\mathcal{R}^n\\)\nAs the number of dimensions increases, the same interpretation can be used, but the ability to visualize higher dimensions becomes more difficult.\n\n\n3.2.5 Linear Combinations of Vectors\nWe say that for any two scalars \\(a\\) and \\(b\\) and any two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of length \\(n\\), the sum\n\\[\n\\begin{aligned}\na \\mathbf{x} + b \\mathbf{y} & = \\begin{pmatrix}\na x_1 + b y_1 \\\\\na x_2 + b y_2 \\\\\n\\vdots \\\\\na x_n + b y_n \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\]\nis called a linear combination. The idea of a linear combination can be extended to \\(K\\) different scalars \\(\\{ a_1, \\ldots, a_K \\}\\) and \\(K\\) different vectors \\(\\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_K\\}\\) each of length \\(n\\) as\n\\[\n\\begin{aligned}\na_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 +  \\ldots + a_K \\mathbf{x}_K =\n\\sum_{k=1}^K a_k \\mathbf{x}_k & = \\begin{pmatrix}\n\\sum_{k=1}^K a_k x_{k1} \\\\\n\\sum_{k=1}^K a_k x_{k2} \\\\\n\\vdots \\\\\n\\sum_{k=1}^K a_k x_{kn} \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\]\nThe scalars \\(a_k\\) are called coefficients (sometimes also called weights).\n\nExample: Consider the linear combination \\(a \\mathbf{u} + b \\mathbf{v}\\) where\n\n\\[\n\\begin{aligned}\n\\mathbf{u} = \\begin{pmatrix} 3 \\\\ 6\\end{pmatrix} && \\mathbf{v} = \\begin{pmatrix} -2 \\\\ 1\\end{pmatrix}. \\end{aligned}\n\\]\nAre there values of \\(a\\) and \\(b\\) such \\(a \\mathbf{u} + b \\mathbf{v} = \\begin{pmatrix} 9 \\\\ - 4 \\end{pmatrix}\\)? To answer this question, we can write the linear combination as\n\\[\n\\begin{aligned}\na \\begin{pmatrix} 3 \\\\ 6\\end{pmatrix} + b \\begin{pmatrix} -2 \\\\ 1\\end{pmatrix} & = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich can be written using the property of scalar multiplication as\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 3a \\\\ 6a \\end{pmatrix} + \\begin{pmatrix} -2b \\\\ b \\end{pmatrix} & = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix}\n\\end{aligned}\n\\]\nand using properties of vector addition can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 3a - 2b \\\\ 6a + b \\end{pmatrix} & = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix}\n\\end{aligned}\n\\]\nRecognizing this as a system of linear equations\n\\[\n\\begin{aligned}\n3a - 2b & = 9 \\\\\n6a + b & = -4,\n\\end{aligned}\n\\]\nthe system of equations can be written in an augmented matrix form as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n3  & - 2 & 9\\\\\n6  &   1 & -4\n\\end{pmatrix}\n\\end{aligned}\n\\]\nReducing the augmented matrix to reduced row echelon form gives\n\nrref(matrix(c(3, 6, -2, 1, 9, -4), 2, 3))\n\n     [,1] [,2]        [,3]\n[1,]    1    0  0.06666667\n[2,]    0    1 -4.40000000\n\n\nwhich has solutions \\(a = 0.0667\\) and \\(b = -4.4\\).\n\nResult: Any vector equation \\(a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 + \\ldots + a_K \\mathbf{x}_K = \\mathbf{c}\\) for a given constant vector \\(\\mathbf{b}\\) has the same solution set as the augmented matrix\n\n\\[\n\\begin{aligned}\n\\begin{pmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_K & \\mathbf{b} \\end{pmatrix}\n\\end{aligned}\n\\]\nEquivalently, the set of vectors \\(\\{\\mathbf{x}_k\\}_{k=1}^K\\) can only be combined with linear coefficients \\(\\{a_k\\}_{k=1}^K\\) to equal the vector \\(\\mathbf{b}\\) if the linear system of equations is consistent.\n\n\n3.2.6 The geometric interpretation of linear combinations of vectors\nConsider the vectors \\(\\mathbf{u} = \\begin{pmatrix} \\sqrt{2} \\\\ - \\sqrt{2} \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) shown in the figure below on the left.\n\n\n\n\n\n\nExercise: Given \\(\\mathbf{u} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\), estimate the linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) that gives the point \\(\\mathbf{w}\\) in the figure below."
  },
  {
    "objectID": "03-vector-spaces.html#span",
    "href": "03-vector-spaces.html#span",
    "title": "3  Vectors spaces",
    "section": "3.3 Span",
    "text": "3.3 Span\n\nDefinition 3.1 (Span) Let \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) be vectors in \\(\\mathcal{R}^n\\). We say the vector \\(\\mathbf{w}\\) is in the span of \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) (\\(\\mathbf{w} \\in \\mbox{span}\\{ \\mathbf{a}_1, \\ldots, \\mathbf{a}_K \\}\\)) if there exists coefficients \\(x_1, \\ldots, x_K\\) such that \\(\\mathbf{w} = \\sum_{k=1}^K x_k \\mathbf{a}_k\\).\n\n\nExample 3.1 While not a vector notation, you already understand the span from polynomial functions. For example, assume you have the functions 1, \\(x\\), and \\(x^2\\). Then, the functions \\(-4 + 3x^2\\) (\\(a_1 = -4, a_2 = 0, a_3 = 3\\)) and \\(-3 + 4x - 2x^2\\) (\\(a_1 = -3, a_2 = 4, a_3 = -2\\)) are in the span of functions \\(\\{1, x, x^2\\}\\), but the functions \\(x^3\\), \\(x^4 - 2x^2\\), etc., are not in the span of \\(\\{1, x, x^2\\}\\) because you cannot write these as a linear combination of \\(a_1 1 + a_2 x + a_3 x^2\\).\n\n\n3.3.1 Geometric example of the span\n\nExample: Consider the vector \\(\\mathbf{u} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\). Then, the vector \\(\\mathbf{w} = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}\\) is in the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) because \\(\\mathbf{w} = 2 \\mathbf{u}\\) but the vector \\(\\mathbf{v} = \\begin{pmatrix} 4 \\\\ -4 \\end{pmatrix}\\) is not in the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) because there is no coefficient \\(a\\) such that \\(\\mathbf{w} = a \\mathbf{u}\\). In this example, the vector \\(\\mathbf{u}\\) is a 2-dimensional vector (lives in \\(\\mathcal{R}^2\\)–a plane) but the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) lives in 1-dimension (a line).\n\n\nggplot() +\n    geom_abline(slope = 1/2, intercept = 0, color = \"blue\", size = 2) +  \n    geom_segment(aes(x = 0, xend = 2, y = 0, yend = 1), arrow = arrow(length = unit(0.1, \"inches\")), size = 1.5, color = \"red\") +\n    geom_segment(aes(x = 0, xend = 4, y = 0, yend = 2), arrow = arrow(length = unit(0.1, \"inches\")), size = 1.5, color = \"red\") +\n    geom_segment(aes(x = 0, xend = 4, y = 0, yend = -4), arrow = arrow(length = unit(0.1, \"inches\")), size = 1.5, color = \"orange\") +\n    geom_vline(xintercept = 0) + \n    geom_hline(yintercept = 0) +\n    coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4))  +\n    geom_text(data = data.frame(x = c(2, 4, 4),  y = c(1, 2, -4), text = c(\"u\", \"w\", \"v\")),\n              aes(x = x, y = y + 0.5, label = text), size = 5, inherit.aes = FALSE,\n              color = c(\"red\", \"red\", \"orange\")) +\n    ggtitle(\"span{u} is the blue line \\nw is in span{u}\\nv is not in span{u}\")\n\n\n\n\nFrom the example above, we can answer the question “Is the point (a, b) on the line defined by the vector \\(\\mathbf{u}\\)?” by asking whether the point (a, b) is in the \\(span\\{\\mathbf{u}\\}\\). While this is trivial for such a simple problem, the use of the span will make things easier in higher dimensions.\n\nExample: do in class 2 3-d vectors that are not scalar multiples of each other define a plane. Does a point lie within the plane? Use the span to answer this question.\n\nThe shiny app below demonstrates how the concept of span can be understood in 2 dimensions. The app can be downloaded and run\n\nlibrary(shiny)\nrunGitHub(rep = \"multivariable-math\",\n          username = \"jtipton25\", \n          subdir = \"shiny-apps/chapter-03/span\")"
  },
  {
    "objectID": "04-linear-systems-of-equations.html",
    "href": "04-linear-systems-of-equations.html",
    "title": "4  Linear Systems of Equations",
    "section": "",
    "text": "library(tidyverse)\n\n# For 3-d plotting\n# if devtools package not installed, install the package\nif (!require(devtools)) {\n    install.packages(\"devtools\")\n}\n# if gg3D package not installed, install the package\nif (!require(gg3D)) {\n    devtools::install_github(\"AckerDWM/gg3D\")\n    library(gg3D)\n}\n\n# if dasc2594 package not installed, install the package\nif (!require(dasc2594)) {\n    devtools::install_github(\"jtipton25/dasc2594\")\n    library(dasc2594)\n}"
  },
  {
    "objectID": "04-linear-systems-of-equations.html#linear-systems-of-equations",
    "href": "04-linear-systems-of-equations.html#linear-systems-of-equations",
    "title": "4  Linear Systems of Equations",
    "section": "4.1 Linear Systems of equations",
    "text": "4.1 Linear Systems of equations\n\n4.1.1 Linear equations\n\nDefinition 4.1 (Linear Equations) Let \\(x_1, x_2, \\ldots, x_n\\) be variables with coefficients \\(a_1, a_2, \\ldots, a_n\\), and \\(b\\) are fixed and known numbers. Then, we say\n\\[\n\\begin{aligned}\na_1 x_1 + a_2 x_2 + \\cdots + a_n x_n & = b\n\\end{aligned}\n\\tag{4.1}\\]\nis a linear equation.\n\n\nExample 4.1 Show the equation for a line with slope \\(m\\) and \\(y\\)-intercept \\(b\\) is\n\\[\n\\begin{aligned}\ny & = m x + b,\n\\end{aligned}\n\\]\nis a linear equation.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe equation for a line with slope \\(m\\) and \\(y\\)-intercept \\(b\\) is\n\\[\n\\begin{aligned}\ny & = m x + b,\n\\end{aligned}\n\\]\nis a linear equation because it can be re-written as\n\\[\n\\begin{aligned}\ny - m x & = b,\n\\end{aligned}\n\\]\nwhere \\(a_1 = 1\\), \\(a_2 = m\\), \\(x_1 = y\\), \\(x_2 = x\\), and \\(b=b\\).\n\n\n\n\nExample 4.2 Determine if the equation below is a linear equation\n\\[\n\\begin{aligned}\n\\sqrt{19} x_1 & = (4 + \\sqrt{2}) x_2 - x_3 - 9.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe equation\n\\[\n\\begin{aligned}\n\\sqrt{19} x_1 & = (4 + \\sqrt{2}) x_2 - x_3 - 9\n\\end{aligned}\n\\]\nis a linear equation because it can be written as\n\\[\n\\begin{aligned}\n\\sqrt{19} x_1 - (4 + \\sqrt{2}) x_2 + x_3 & = - 9\n\\end{aligned}\n\\] where \\(a_1 = \\sqrt{19}\\), \\(a_2 = 4 + \\sqrt{2}\\), \\(a_3 = 1\\) and \\(b = -9\\)\n\n\n\n\nExample 4.3 Determine if the equation below is a linear equation\n\\[\n\\begin{aligned}\n-4 x_1 + 5 x_2 - 11 & = x_3.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe equation\n\\[\n\\begin{aligned}\n-4 x_1 + 5 x_2 - 11 & = x_3\n\\end{aligned}\n\\] is a linear equation because it can be written as\n\\[\n\\begin{aligned}\n-4 x_1 + 5 x_2 - x_3 & = 11.\n\\end{aligned}\n\\]\n\n\n\n\nExample 4.4 Determine if the equation\n\\[\n\\begin{aligned}\nx_1 & = x_2^2 + 3\n\\end{aligned}\n\\]\nis a linear equation.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe equation\n\\[\n\\begin{aligned}\nx_1 & = x_2^2 + 3\n\\end{aligned}\n\\]\nis not a linear equation because it does not meet the form of Equation 4.1 because the equation has a quadratic power of \\(x_2\\).\n\n\n\n\nExample 4.5 Determine if the equation\n\\[\n\\begin{aligned}\nx_1 + x_2 - x_1 x_2 & = 16\n\\end{aligned}\n\\]\nis a linear equation.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe equation\n\\[\n\\begin{aligned}\nx_1 + x_2 - x_1 x_2 & = 16\n\\end{aligned}\n\\]\nis not a linear equation because it does not meet the form of Equation 4.1 because there is a product \\(x_1x_2\\) of \\(x_1\\) and \\(x_2\\).\n\n\n\n\nExample 4.6 Is the following equation a linear equation? \\(x_1 + 3 x_1 x_2 = 5\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a linear equation because the equation can be written as \\(a_1 x_1 + a_2 x_2 = b\\) where \\(a_1 = 1\\), \\(a_2 = 3\\) and \\(b = 5\\)\n\n\n\n\nExample 4.7 Is the following equation a linear equation? \\(5x + 7y + 8z = 11.2\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a linear equation because the equation can be written as \\(a_1 x_1 + a_2 x_2 + a_3 x_3= b\\) where \\(a_1 = 5\\), \\(a_2 = 7\\), \\(a_3 = 8\\) and \\(b = 11.2\\). The variables \\(x_1 = x\\), \\(x_2 = y\\), and \\(x_3 = z\\).\n\n\n\n\nExample 4.8 Is the following equation a linear equation? \\(\\frac{1}{4} y + \\sqrt{2} z = 2^6\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a linear equation because the equation can be written as \\(a_1 x_1 + a_2 x_2 = b\\) where \\(a_1 = \\frac{1}{4}\\), \\(a_2 = \\sqrt{2}\\), and \\(b = 2^6\\). The variables \\(x_1 = y\\), \\(x_2 = z\\) enter the equation linearly.\n\n\n\n\nExample 4.9 Is the following equation a linear equation? \\(x + 4 y^2 = 9\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is not a linear equation because the equation cannot be written as \\(a_1 x_1 + a_2 x_2 = b\\) because \\(x_1 = x\\) and \\(x_2 = y\\) so that the equation is written as \\(a_1 x_1 + a_2 x_2^2 = b\\) where \\(a_1 = 1\\), \\(a_2 = 4\\), and \\(b = 9\\). The variable \\(x_2 = y\\) enters the equation in a non-linear (quadratic) manner.\n\n\n\n\n\n4.1.2 Systems of linear equations\n\nDefinition 4.2 (System of Equations) A set of two or more linear equations that each contain the same set of variables is called a system of linear equations.\n\n\nExample 4.10 Determine if the following equations are a system of linear equations.\n\\[\\begin{alignat*}{4}\nx_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\\\\n4 x_1 & {}+{} & 5 x_2 & {}{} &     & {}={} & 9.\n\\end{alignat*}\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe set of equations \\[\\begin{alignat*}{4}\nx_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\\\\n4 x_1 & {}+{} & 5 x_2 & {}{} &     & {}={} & 9.\n\\end{alignat*}\\] are a system of equations. Note that in the second equation, the coefficient for \\(x_3\\) is 0, meaning we could re-write the above example as\n\\[\\begin{alignat*}{4}\nx_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\\\\n4 x_1 & {}+{} & 5 x_2 & {}+{} & 0 x_3 & {}={} & 9.\n\\end{alignat*}\\]\n\n\n\n\nExample 4.11 Determine if the following equations are a system of linear equations.\n\\[\\begin{alignat*}{4}\nx_1   & {}+{} & 4 x_1 x_2 & {}-{} & x_3 & {}={} & 11 \\\\\n4 x_1 & {}+{} & 5 x_2 & {}+{} & 2 x_3 & {}={} & \\pi.\n\\end{alignat*}\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe set of equations \\[\\begin{alignat*}{4}\nx_1   & {}+{} & 4 x_1 x_2 & {}-{} & x_3 & {}={} & 11 \\\\\n4 x_1 & {}+{} & 5 x_2 & {}+{} & 2 x_3 & {}={} & \\pi\n\\end{alignat*}\\] are not a linear system of equations. Note that in the first equation, there is a nonlinear term \\(x_1 x_2\\) and because of this, equation one cannot be written in the form of Equation 4.1.\n\n\n\nFor the remainder of the section on linear algebra, we will focus on linear equations.\n\n\n4.1.3 Solutions of linear systems\nA fundamental question when presented with a linear system of equations is whether the system has a solution.\n\nDefinition 4.3 (Solution of Systems of Equations) A solution to a system means that there are numbers \\((s_1, s_2, \\ldots, s_n)\\) that each of the variables \\(x_1, x_2, \\ldots, x_n\\) take that allow for all the equations to simultaneously be true.\n\nChecking if a vector is a solution is straightforward. You just substitute the values of the vector into the equations and see if the equations are satisfied. For example, if we consider the equation \\[\nx_1 + x_2 = 7,\n\\] the vector \\(\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}\\) is a valid solution because \\[\n4 + 3 = 7\n\\] but the vector \\(\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 9 \\end{pmatrix}\\) is not a valid solution because \\[\n5 + 9 \\neq 7.\n\\]\n\n\n\n\nExample 4.12 Is \\(\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 3 \\\\ 6 \\end{pmatrix}\\) a solution to the system of equations \\[\\begin{alignat*}{4}\nx_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\\\\n4 x_1 & {}+{} & 5 x_2 & {}+{} & 0 x_3 & {}={} & 9?\n\\end{alignat*}\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we plug in the values for \\(x_1 = 5\\), \\(x_2 = 3\\) and \\(x_3 = 6\\) into the system of equations. In the first equation, we get \\[\\begin{alignat*}{4}\n5   & {}+{} & 4 \\times 3 & {}-{} & 6 & {}={} & 11 \\\\\n\\end{alignat*}\\] which is true because \\(11 = 11\\). Now we consider the second equation\n\\[\\begin{alignat*}{4}\n4 \\times 5 & {}+{} & 5 \\times 3 & {}+{} & 0 \\times 6 & {}={} & 9.\n\\end{alignat*}\\] which is false because \\(35 \\neq 9\\). Thus, \\(\\mathbf{x} = \\begin{pmatrix} 5 \\\\ 3 \\\\ 6 \\end{pmatrix}\\) is not a solution to the system of equations in the given example.\n\n\n\n\nExample 4.13 Finding a solution a system of equations is more challenging. To find a solution, we add/subtract equations to cancel out variables. For example, consider the following example. You work in a small zoo. In the zoo, there are ostriches (one head, two legs, and no horns), one-horned rhinos (one head, four legs, and one horn), and two-horned antelope (one head, four legs, and two horns). Consider the following statements about the set of animals in the zoo:\n\nThere are 12 heads\nThere are 38 feet\nThere are 10 horns\n\nUsing this information, how many ostriches, rhinos, and antelope are there.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLetting the variable \\(o\\) represent the number of ostriches, \\(r\\) represent the number of rhinos, and \\(a\\) represent the number of antelope. Then, the three statements above can be represented mathematically by the system of equations \\[\\begin{alignat*}{4}\no   & {}+{} &  r  & {}+{} & a  & {}={} & 12 \\\\\n2o  & {}+{} &  4r & {}+{} & 4a & {}={} & 38 \\\\\n0o  & {}+{} &  r  & {}+{} & 2a & {}={} & 10.\n\\end{alignat*}\\] Looking at the third equation, we can solve for the number of rhinos \\(r\\) as \\(r = 10 - 2a\\). Substituting this into the first two equations gives \\[\\begin{alignat*}{4}\no   & {}+{} &  10 - 2a  & {}+{} & a  & {}={} & 12\n\\end{alignat*}\\] and \\[\\begin{alignat*}{4}\n2o  & {}+{} &  4(10 - 2a) & {}+{} & 4a & {}={} & 38\n\\end{alignat*}\\] The first equation from above gives \\(o + 10 - 2a + a = 12\\) and solving for \\(o\\) as a function of \\(a\\) gives \\(o = 2 + a\\). The second equation from above gives \\(2o + 4(10 - 2a) + 4a = 38\\) and solving for \\(o\\) as a function of \\(a\\) gives \\(o = 2a - 1\\). Combining both of these gives \\(2 + a = 2a - 1\\) which gives \\(a = 3\\) so that we know there are 3 antelope. Going backwards, we know that \\(o = 2a - 1\\) and plugging in \\(a = 3\\) gives \\(o = 5\\) which tells us that there are 5 ostriches in the zoo. In addition, we know that \\(r = 10 - 2a\\) and, when plugging in the value \\(a = 3\\), we find that there are \\(r = 4\\) rhinos at the zoo.\nWhile solving for variables like above is possible, it can be quite difficult to keep track of the steps in the calculation. To simplify the process, one can add and subtract entire equations. For example, starting with the system of equations from the zoo example \\[\\begin{alignat*}{4}\no   & {}+{} &  r  & {}+{} & a  & {}={} & 12 \\\\\n2o  & {}+{} &  4r & {}+{} & 4a & {}={} & 38 \\\\\n0o  & {}+{} &  r  & {}+{} & 2a & {}={} & 10.\n\\end{alignat*}\\] we can add and subtract entire equations. For example, take the second equation and subtract 2 times the first equation to get the system of equations \\[\\begin{alignat*}{4}\no   & {}+{} &  r  & {}+{} & a  & {}={} & 12 \\\\\n2o - 2 \\times o & {}+{} &  4r - 2 \\times r & {}+{} & 4a -2 \\times a& {}={} & 38 - 2 \\times 12\\\\\n0o  & {}+{} &  r  & {}+{} & 2a & {}={} & 10,\n\\end{alignat*}\\] which, when simplified is \\[\\begin{alignat*}{4}\no   & {}+{} &  r  & {}+{} & a  & {}={} & 12 \\\\\n0o  & {}+{} &  2r & {}+{} & 2a & {}={} & 14 \\\\\n0o  & {}+{} &  r  & {}+{} & 2a & {}={} & 10.\n\\end{alignat*}\\]\nTaking the third equation and subtracting \\(\\frac{1}{2}\\) times the second equation gives \\[\\begin{alignat*}{4}\no   & {}+{} &  r  & {}+{} & a  & {}={} & 12 \\\\\n0o  & {}+{} &  2r & {}+{} & 2a & {}={} & 14 \\\\\n0o  & {}+{} &  r - \\frac{1}{2} \\times 2r  & {}+{} & 2a - \\frac{1}{2} \\times 2a & {}={} & 10 - \\frac{1}{2} \\times 14,\n\\end{alignat*}\\] which is the system of equations \\[\\begin{alignat*}{4}\no   & {}+{} &  r  & {}+{} & a  & {}={} & 12 \\\\\n0o  & {}+{} &  2r & {}+{} & 2a & {}={} & 14 \\\\\n0o  & {}+{} &  0r  & {}+{} & a & {}={} & 3.\n\\end{alignat*}\\] Because \\(a = 3\\), there are 3 antelope at the zoo. Now, we can use these systems of equations to subtract 2 times the third equation from the second equation to get \\[\\begin{alignat*}{4}\no   & {}+{} &  r  & {}+{} & a  & {}={} & 12 \\\\\n0o  & {}+{} &  2r & {}+{} & 2a - 2 \\times a & {}={} & 14 - 2 \\times 3 \\\\\n0o  & {}+{} &  0r  & {}+{} & a & {}={} & 3,\n\\end{alignat*}\\] which gives the system of equations \\[\\begin{alignat*}{4}\no   & {}+{} &  r  & {}+{} & a  & {}={} & 12 \\\\\n0o  & {}+{} &  2r & {}+{} & 0a & {}={} & 8 \\\\\n0o  & {}+{} &  0r  & {}+{} & a & {}={} & 3,\n\\end{alignat*}\\] and dividing the second equation by 2 gives \\[\\begin{alignat*}{4}\no   & {}+{} &  r  & {}+{} & a  & {}={} & 12 \\\\\n0o  & {}+{} &  r & {}+{} & 0a & {}={} & 4 \\\\\n0o  & {}+{} &  0r  & {}+{} & a & {}={} & 3,\n\\end{alignat*}\\] so that we know there are 4 rhinos in the zoo. Taking the first equation and subtracting the second equation gives the system of equations \\[\\begin{alignat*}{4}\no   & {}+{} &  r - r  & {}+{} & a  & {}={} & 12 - 4\\\\\n0o  & {}+{} &  r & {}+{} & 0a & {}={} & 4 \\\\\n0o  & {}+{} &  0r  & {}+{} & a & {}={} & 3,\n\\end{alignat*}\\] which is \\[\\begin{alignat*}{4}\no   & {}+{} &  0r  & {}+{} & a  & {}={} & 8 \\\\\n0o  & {}+{} &  r & {}+{} & 0a & {}={} & 4 \\\\\n0o  & {}+{} &  0r  & {}+{} & a & {}={} & 3,\n\\end{alignat*}\\] and subtracting the third equation from the first equation gives \\[\\begin{alignat*}{4}\no   & {}+{} &  0r  & {}+{} & a - a  & {}={} & 8 - 3 \\\\\n0o  & {}+{} &  r & {}+{} & 0a & {}={} & 4 \\\\\n0o  & {}+{} &  0r  & {}+{} & a & {}={} & 3,\n\\end{alignat*}\\] which gives \\[\\begin{alignat*}{4}\no   & {}+{} &  0r  & {}+{} & 0a  & {}={} & 5 \\\\\n0o  & {}+{} &  r & {}+{} & 0a & {}={} & 4 \\\\\n0o  & {}+{} &  0r  & {}+{} & a & {}={} & 3,\n\\end{alignat*}\\] which tells us that there are \\(o=5\\) ostriches at the zoo.\n\n\n\n\nExample 4.14 Consider the system of equations\n\\[\\begin{alignat*}{3}\nx   & {}+{} & 4 y & {}={} & 8 \\\\\n4 x & {}+{} & 5 y & {}={} & 7.\n\\end{alignat*}\\]\nFind a solution to the system of equations.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo find if a solution to this equation exists, we can do some algebra and take 4 times the top equation and then subtract the bottom equation, replacing the bottom equation with this new sum like\n\\[\\begin{alignat*}{3}\nx   & {}+{} & 4 y & {}={} & 8 \\\\\n4 x - 4 \\times (x) & {}+{} & 5 y - 4 \\times (4y) & {}={} & 7 - 4 \\times (8),\n\\end{alignat*}\\] where the part of the equations in (\\(\\cdot\\)) is the top equation. This system of equations now simplifies to \\[\\begin{alignat*}{3}\nx & {}+{} & 4 y & {}={} & 8 \\\\\n0 & {}+{} & - 11y & {}={} & -25,\n\\end{alignat*}\\] which gives \\(y = \\frac{25}{11}\\). Plugging this value into the top equation gives \\[\\begin{alignat*}{3}\nx & {}+{} & 4 \\frac{25}{11} & {}={} & 8 \\\\\n0 & {}+{} & y & {}={} & \\frac{25}{11},\n\\end{alignat*}\\] where we can solve \\(x = 8 - \\frac{100}{11} = -\\frac{12}{11}\\) giving the solution of the form \\[\\begin{alignat*}{3}\nx & {}+{} & 0 & {}={} & -\\frac{12}{11} \\\\\n0 & {}+{} & y & {}={} & \\frac{25}{11},\n\\end{alignat*}\\] In this case, the system of equation has the solution \\(x = -\\frac{12}{11}\\) and \\(y = \\frac{25}{11}\\). While finding the solution can be done algebraically, what does this mean visually (geometrically)? The original equations were \\[\\begin{alignat*}{3}\nx   & {}+{} & 4 y & {}={} & 8 \\\\\n4 x & {}+{} & 5 y & {}={} & 7,\n\\end{alignat*}\\]\nwhich, writing \\(y\\) as a function of \\(x\\) define two lines:\n\n\\(y = -\\frac{x}{4} + 2\\)\n\\(y = -\\frac{4x}{5} + \\frac{7}{5}\\)\n\nLet’s plot these equations in R and see what they look like\n\n# define some grid points to evaluate the line\nx <- seq(-2, 2, length = 1000)\ndat <- data.frame(\n    x = c(x, x),\n    y = c(-x / 4 + 2, - 4 / 5 * x + 7/5),\n    equation = factor(rep(c(1, 2), each = 1000))\n)\nglimpse(dat)\n\nRows: 2,000\nColumns: 3\n$ x        <dbl> -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.979…\n$ y        <dbl> 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, 2…\n$ equation <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\ndat %>%\n    ggplot(aes(x = x, y = y, color = equation, group = equation)) +\n    geom_line() +\n    scale_color_viridis_d(end = 0.8) +\n    # solution x = -12/11, y = 25/11\n    geom_point(aes(x = -12/11, y = 25/11), color = \"red\", size = 2) +\n    ggtitle(\"Linear system of equations\")\n\n\n\n\nFigure 4.1: Linear system of equations with one solution\n\n\n\n\nFrom this plot, it is clear that the solution to the system of equations is the location where the two lines intersect!\n\n\n\nSTART BACK HERE WITH MORE EXAMPLES OF CANCELING VARIABLES\n\n\n4.1.4 Types of solutions\nTypically, there are 3 cases for the solutions to a system of linear equations\n\nThere are no solutions\nThere is one solution (Figure 4.1)\nThere are infinitely many solutions\n\n\nDefinition 4.4 A linear system of equations is called consistent if the system has either one or infinitely many solutions and is called inconsistent if the system has no solution.\n\n\nThere are no solutions:\nConsider the system of linear equations\n\\[\\begin{alignat*}{3}\nx   & {}+{} & 4 y & {}={} & 8 \\\\\n4 x & {}+{} & 16 y & {}={} & 18.\n\\end{alignat*}\\]\n\n# define some grid points to evaluate the line\nx <- seq(-2, 2, length = 1000)\ndat <- data.frame(\n    x = c(x, x),\n    y = c(-x / 4 + 8 / 4, - x / 4 + 18 / 4),\n    equation = factor(rep(c(1, 2), each = 1000))\n)\nglimpse(dat)\n\nRows: 2,000\nColumns: 3\n$ x        <dbl> -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.979…\n$ y        <dbl> 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, 2…\n$ equation <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\ndat %>%\n    ggplot(aes(x = x, y = y, color = equation, group = equation)) +\n    geom_line() +\n    scale_color_viridis_d(end = 0.8) +\n    # solution x = -12/11, y = 25/11\n    ggtitle(\"Linear system of equations\")\n\n\n\n\nLinear system of equations with no solution\n\n\n\n\nIn this case, the linear equations are parallel lines and will never intersect so therefore there is no solution.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo find if a solution to this equation exists, we can do some algebra and take 4 times the top equation and then subtract the bottom equation, replacing the bottom equation with this new sum like\n\\[\\begin{alignat*}{3}\nx   & {}+{} & 4 y & {}={} & 8 \\\\\n4 x  - 4(x) & {}+{} & 16 y - 4\\times (4 y) & {}={} & 18 - 4 \\times (8).\n\\end{alignat*}\\] where the part of the equations in () is the top equation. This system of equations now simplifies to \\[\\begin{alignat*}{3}\nx & {}+{} & 4 y & {}={} & 8 \\\\\n0 x & {}+{} &  0 y & {}={} & -14,\n\\end{alignat*}\\]\nwhich gives \\(0 = -14\\). From this, we see that we have reached a contradiction so there is not a solution to the system of equations.\n\n\n\n\n\nThere is one solution:\nConsider the system of equations from Example 4.14\n\\[\\begin{alignat*}{3}\nx   & {}+{} & 4 y & {}={} & 8 \\\\\n4 x & {}+{} & 5 y & {}={} & 7.\n\\end{alignat*}\\]\nWhere we found that there was a unique solution \\(x = -\\frac{12}{11}\\) and \\(y = \\frac{25}{11}\\). While finding the solution can be done algebraically, what does this mean visually (geometrically)? The original equations were \\[\\begin{alignat*}{3}\nx   & {}+{} & 4 y & {}={} & 8 \\\\\n4 x & {}+{} & 5 y & {}={} & 7,\n\\end{alignat*}\\] which, writing \\(y\\) as a function of \\(x\\) define two lines:\n\n\\(y = -\\frac{x}{4} + 2\\)\n\\(y = -\\frac{4x}{5} + \\frac{7}{5}\\)\n\nLet’s plot these equations in R and see what they look like\n\n# define some grid points to evaluate the line\nx <- seq(-2, 2, length = 1000)\ndat <- data.frame(\n    x = c(x, x),\n    y = c(-x / 4 + 2, - 4 / 5 * x + 7/5),\n    equation = factor(rep(c(1, 2), each = 1000))\n)\nglimpse(dat)\n\nRows: 2,000\nColumns: 3\n$ x        <dbl> -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.979…\n$ y        <dbl> 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, 2…\n$ equation <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\ndat %>%\n    ggplot(aes(x = x, y = y, color = equation, group = equation)) +\n    geom_line() +\n    scale_color_viridis_d(end = 0.8) +\n    # solution x = -12/11, y = 25/11\n    geom_point(aes(x = -12/11, y = 25/11), color = \"red\", size = 2) +\n    ggtitle(\"Linear system of equations\")\n\n\n\n\nLinear system of equations with one solution\n\n\n\n\nFrom this plot, it is clear that the solution to the system of equations is the point where the two lines intersect!\n\n\nThere are infinitely many solutions:\nConsider the system of linear equations \\[\\begin{alignat*}{3}\nx   & {}+{} & 4 y & {}={} & 8 \\\\\n4 x & {}+{} & 16 y & {}={} & 32.\n\\end{alignat*}\\]\n\n# define some grid points to evaluate the line\nx <- seq(-2, 2, length = 1000)\ndat <- data.frame(\n    x = c(x, x),\n    y = c(-x / 4 + 8 / 4, - 4 * x / 16 + 32 / 16),\n    equation = factor(rep(c(1, 2), each = 1000))\n)\nglimpse(dat)\n\nRows: 2,000\nColumns: 3\n$ x        <dbl> -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.979…\n$ y        <dbl> 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, 2…\n$ equation <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\ndat %>%\n    ggplot(aes(x = x, y = y, color = equation, group = equation)) +\n    geom_line() +\n    scale_color_viridis_d(end = 0.8) +\n    # solution x = -12/11, y = 25/11\n    ggtitle(\"Linear system of equations\")\n\n\n\n\nLinear system of equations with no solution\n\n\n\n\nIn this case, the linear equations are perfectly overlapping lines and always intersect so therefore there are infinitely many solutions (all points on the line).\n\nDefinition 4.5 Two linear systems of equations are called equivalent if both systems share the same solution set.\n\nFor example, the system of equations \\[\\begin{alignat*}{4}\nx_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\\\\n4 x_1 & {}+{} & 5 x_2 & {}+{} & 2 x_3 & {}={} & 9\n\\end{alignat*}\\] and the system of equations \\[\\begin{alignat*}{4}\n2x_1   & {}+{} & 8 x_2 & {}-{} & 2 x_3 & {}={} & 22 \\\\\n8 x_1 & {}+{} & 10 x_2 & {}+{} & 4 x_3 & {}={} & 18.\n\\end{alignat*}\\] have the same solution set (the second set of equations is just 2 times the first set of equations).\n\nExample 4.15 For the following system of equations, determine if a solution(s) exist and if so, solve for the solution \\[\\begin{alignat*}{3}\n4 x_1 & {}+{} & 5 x_2 & {}={} & 8 \\\\\n9 x_1 & {}-{} & 3 x_2 & {}={} & 4.\n\\end{alignat*}\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMultiply the first equation by \\(\\frac{1}{4}\\) and the second equation by \\(\\frac{1}{9}\\) to get \\[\\begin{alignat*}{3}\nx_1 & {}+{} & \\frac{5}{4} x_2 & {}={} & 2 \\\\\nx_1 & {}-{} &  \\frac{1}{3}x_2 & {}={} & \\frac{4}{9}.\n\\end{alignat*}\\] Subtract the first equation from the second equation \\[\\begin{alignat*}{3}\nx_1 & {}+{} & \\frac{5}{4} x_2 & {}={} & 2 \\\\\n0 & {}-{} &  (-\\frac{1}{3} - \\frac{5}{4}) x_2 & {}={} & \\frac{4}{9} - 2,\n\\end{alignat*}\\] which reduces to \\[\\begin{alignat*}{3}\nx_1 & {}+{} & \\frac{5}{4} x_2 & {}={} & 2 \\\\\n0 & {}-{} &  -\\frac{19}{12} x_2 & {}={} & -\\frac{14}{9},\n\\end{alignat*}\\] so that, dividing both sides of the second equation by \\(-\\frac{19}{12}\\) gives \\(x_2 = \\frac{56}{57}\\). Plugging this value of \\(x_2\\) into the first equation gives \\[\\begin{alignat*}{3}\nx_1 & {}+{} & \\frac{5}{4} \\left(\\frac{56}{57}\\right) & {}={} & 2 \\\\\n0 & {}-{} &   x_2 & {}={} & \\frac{56}{57}\n\\end{alignat*}\\] and subtracting \\(\\frac{5}{4} \\frac{56}{57}\\) from both sides of the first equation gives \\(x_1 = \\frac{44}{57}\\). You can check these solutions by verifying that the following two equations hold \\[\\begin{alignat*}{3}\n4 \\left(\\frac{44}{57}\\right) & {}+{} & 5  \\left(\\frac{56}{57}\\right) & {}={} & 8 \\\\\n9 \\left(\\frac{44}{57}\\right) & {}-{} & 3 \\left(\\frac{56}{57}\\right)& {}={} & 4\n\\end{alignat*}\\] which can be done in R using\n\nx1 <- 44/57\nx2 <- 56/57\nall.equal(4 * x1 + 5 * x2, 8)\n\n[1] TRUE\n\nall.equal(9 * x1 - 3 * x2, 4)\n\n[1] TRUE\n\n\n\n\n\n\nExample 4.16 For the following system of equations, determine if a solution(s) exist and if so, solve for the solution \\[\\begin{alignat*}{4}\n7 x_1 & {}+{} & 3 x_2 & {}+{} & 4 x_3 & {}={} & 5\\\\\n4 x_1 & {}-{} & 5 x_2 &&        & {}={} & -2\n\\end{alignat*}\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMultiply the first equation by \\(\\frac{1}{7}\\) and the second equation by \\(\\frac{1}{4}\\) to get \\[\\begin{alignat*}{4}\nx_1 & {}+{} & \\frac{3}{7} x_2 & {}+{} & \\frac{4}{7} x_3 & {}={} & \\frac{5}{7} \\\\\nx_1 & {}-{} & \\frac{5}{4} x_2 & {}+{} & 0 x_3       & {}={} & \\frac{-1}{2}.\n\\end{alignat*}\\] Subtract the first equation from the second equation \\[\\begin{alignat*}{4}\nx_1 & {}+{} & \\frac{3}{7} x_2 & {}+{} & \\frac{4}{7} x_3 & {}={} & \\frac{5}{7} \\\\\n0 x_1 & {}+{} &  (-\\frac{5}{4} - \\frac{3}{7}) x_2 &{}+{}& -\\frac{4}{7} x_3  & {}={} & - \\frac{1}{2} - \\frac{5}{7},\n\\end{alignat*}\\] which reduces to \\[\\begin{alignat*}{4}\nx_1 & {}+{} & \\frac{3}{7} x_2 & {}+{} & \\frac{4}{7} x_3 & {}={} & \\frac{5}{7} \\\\\n0 & {}-{} &  -\\frac{47}{28} x_2 &{}-{}& \\frac{4}{7} x_3 & {}={} & -\\frac{17}{14}.\n\\end{alignat*}\\] Next, divide the second equation by \\(\\frac{-47}{28}\\) gives \\[\\begin{alignat*}{4}\nx_1 & {}+{} & \\frac{3}{7} x_2 & {}+{} & \\frac{4}{7} x_3 & {}={} & \\frac{5}{7} \\\\\n0 & {}+{} & x_2 &{}+{}& \\frac{16}{47} x_3 & {}={} & \\frac{34}{47}.\n\\end{alignat*}\\] Then, take the first equation and subtract \\(-\\frac{3}{7}\\) times the second row to get \\[\\begin{alignat*}{4}\nx_1 & {}+{} & 0 x_2 & {}+{} & \\frac{20}{47} x_3 & {}={} & \\frac{19}{47} \\\\\n0 & {}+{} & x_2 &{}+{}& \\frac{16}{47} x_3 & {}={} & \\frac{34}{47},\n\\end{alignat*}\\] which gives the solution \\(x_1 + \\frac{20}{47} x_3 = \\frac{19}{47}\\), \\(x_2 + \\frac{16}{47} x_3 = \\frac{34}{47}\\), and \\(x_3 = x_3\\) is a free variable.\nThe solution can be checked in R using\n\n# this is a free variable, you can assign it any value\nx3 <- 3\nx1 <- 19/47 - 20/47 * x3\nx2 <- 34/47 - 16/47 * x3\n# check the first equation\nall.equal(7 * x1 + 3 * x2 + 4 * x3, 5)\n\n[1] TRUE\n\n# check the second equation\nall.equal(4 * x1 - 5 * x2 + 0 * x3, -2)\n\n[1] TRUE\n\n\n\n\n\n\nExample 4.17 For the following system of equations, determine if a solution(s) exist and if so, solve for the solution \\[\\begin{alignat*}{3}\n4 x_1 & {}-{} & 2 x_2 & {}={} & 8\\\\\n2 x_1 & {}+{} & x_2 & {}={} & 7 \\\\\n-3 x_1 & {}+{} & 6 x_2 &{}={} & 11\n\\end{alignat*}\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMultiply the first equation by \\(\\frac{1}{4}\\), the second equation by \\(\\frac{1}{2}\\), and the third equation by \\(-\\frac{1}{3}\\) to get \\[\\begin{alignat*}{3}\nx_1 & {}-{} & \\frac{1}{2} x_2 & {}={} & 2\\\\\nx_1 & {}+{} & \\frac{1}{2} x_2 & {}={} & \\frac{7}{2} \\\\\nx_1 & {}-{} & 2 x_2 &{}={} & -\\frac{11}{3}\n\\end{alignat*}\\] Subtract the first equation from the second equation and also subtract the first equation from the third equation. Thus, we get \\[\\begin{alignat*}{3}\nx_1 & {}-{} & \\frac{1}{2} x_2 & {}={} & 2\\\\\n0 x_1 & {}+{} & x_2 & {}={} & \\frac{3}{2} \\\\\n0 x_1 & {}-{} & \\frac{3}{2} x_2 &{}={} & -\\frac{17}{3}.\n\\end{alignat*}\\] Then, multiply the third equation by \\(-\\frac{2}{3}\\) to get \\[\\begin{alignat*}{3}\nx_1 & {}-{} & \\frac{1}{2} x_2 & {}={} & 2\\\\\n0 x_1 & {}+{} & x_2 & {}={} & 3 \\\\\n0 x_1 & {}+{} & x_2 &{}={} & - \\frac{34}{9}.\n\\end{alignat*}\\]\nNotice that the second equation gives \\(x_2 = 3\\) while the third equation gives \\(x_2 = -\\frac{34}{9}\\). This is a contradiction (\\(3 \\neq -\\frac{34}{9})\\) which implies that the system of equations does not have a solution. Thus we say the system of equations is inconsistent.\n\n\n\n\n\n\n4.1.5 Elementary row and column operations on matrices\nThe elementary row (column) operations include\n\nswaps: swapping two rows (columns),\nsums: replacing a row (column) by the sum itself and a multiple of another row (column)\nscalar multiplication: replacing a row (column) by a scalar multiple times itself\n\nNote that these operations are exactly what we used to solve the equation using algebra above (except for swapping rows).\n\nExample 4.18 For the elementary row operations listed above, we demonstrate these using the matrix\n\n\n\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 1 & 4 & 7 \\\\ 2 & 5 & 8 \\\\ 3 & 6 & 9 \\end{pmatrix}\n\\end{aligned}\n\\]\nThe matrix \\(\\mathbf{A}\\) can be represented in R using\n\nA <- matrix(c(1:9), 3, 3, byrow = FALSE)\n\n\nSwap the first and second rows.\nAdd -3 times the first row to the third row.\nMultiply the second row by \\(\\frac{1}{2}\\).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere we present examples of the elementary row operations\n\nSwap the first and second rows.\n\nThe matrix with the first and second rows swapped is\n\n\n\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 2 & 5 & 8 \\\\ 1 & 4 & 7 \\end{pmatrix}\n\\end{aligned}\n\\]\nNotice that the first and second rows have now switched places. This can be done in R in a variety of ways. First, we extract the rows and place them “by hand” in R.\n\nA <- matrix(c(1:9), 3, 3, byrow = FALSE)\n# extract the rows\nrow1 <- A[1, ] \nrow2 <- A[2, ]\n# swap the rows\nA[1, ] <- row2\nA[2, ] <- row1\nA\n\n     [,1] [,2] [,3]\n[1,]    2    5    8\n[2,]    1    4    7\n[3,]    3    6    9\n\n\nAnother way to do this is to use the function rbind() that binds rows together into a matrix.\n\nA <- matrix(c(1:9), 3, 3, byrow = FALSE)\n# bind row 2, row 1, and row 3 togethter\nrbind(A[2, ], A[1, ], A[3, ])\n\n     [,1] [,2] [,3]\n[1,]    2    5    8\n[2,]    1    4    7\n[3,]    3    6    9\n\n\nYet another way to swap the first two rows is to use a vectorized operation. This allows for fast and efficient coding and computing. The vectorized row swap is\n\nA <- matrix(c(1:9), 3, 3, byrow = FALSE)\n# Take the 2nd, 1st, and 3rd row of A, in that order\nA[c(2, 1, 3), ]\n\n     [,1] [,2] [,3]\n[1,]    2    5    8\n[2,]    1    4    7\n[3,]    3    6    9\n\n\n\nAdd -3 times the first row to the third row.\n\n\n\n\nFirst, we notice that -3 times the first row gives the row vector \\(\\begin{pmatrix} -3 & -12 & -21 \\end{pmatrix}\\). Then, we add this to the row vector of the third row \\(\\begin{pmatrix} 3 & 6 & 9 \\end{pmatrix}\\) to get the row vector \\(\\begin{pmatrix} 0 & -6 & -12 \\end{pmatrix}\\). Finally, this row vector replaces the third row of the matrix to give the result \\[\n\\begin{aligned}\n\\begin{pmatrix} 1 & 4 & 7 \\\\ 2 & 5 & 8 \\\\ 0 & -6 & -12 \\end{pmatrix}\n\\end{aligned}\n\\]\nNote: Notice that by performing this row sum and replacement, we have made the first column of the third row of \\(\\mathbf{A}\\) a 0. This zeroing out of the columns will play a very important role moving forward.\nUsing R, this can be done as\n\nA <- matrix(c(1:9), 3, 3, byrow = FALSE)\n# extract the rows \nrow1 <- A[1, ]\nrow3 <- A[3, ]\nA[3, ] <- -3 * row1 + row3\nA\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    0   -6  -12\n\n\nAnother way to do this row sum and replacement is\n\nA <- matrix(c(1:9), 3, 3, byrow = FALSE)\nA[3, ] <- -3 * A[1, ] + A[3, ]\nA\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    0   -6  -12\n\n\n\nMultiply the second row by \\(\\frac{1}{2}\\).\n\n\n\n\nThe second row of \\(\\mathbf{A}\\) is the row vector \\(\\begin{pmatrix} 2 & 5 & 8 \\end{pmatrix}\\). Multiplying the second row by \\(\\frac{1}{2}\\) gives the row vector \\(\\begin{pmatrix} 1 & 5/2 & 4 \\end{pmatrix}\\). Plugging this into the matrix gives the full matrix with second row multiplied by \\(\\frac{1}{2}\\) as \\[\n\\begin{aligned}\n\\begin{pmatrix} 1 & 4 & 7 \\\\ 1 & 5/2 & 4 \\\\ 3 & 6 & 9 \\end{pmatrix}\n\\end{aligned}\n\\] In R, the second row of \\(\\mathbf{A}\\) can be multiplied by \\(\\frac{1}{2}\\) as\n\nA[2, ] <- 1/2 * A[2, ]\nA\n\n     [,1] [,2] [,3]\n[1,]    1  4.0    7\n[2,]    1  2.5    4\n[3,]    3  6.0    9\n\n\n\n\n\n\n\n4.1.6 The Augmented matrix form of a system of equations\nConsider the linear system of equations\n\\[\\begin{alignat*}{4}\nx_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\\\\n4 x_1 & {}+{} & 5 x_2 & {}+{} & 2 x_3 & {}={} & 9.\n\\end{alignat*}\\] The augmented matrix representation of this system of linear equations is given by the matrix \\[\n\\begin{aligned}\n\\begin{pmatrix}\n1 & 4 & - 1 & 11 \\\\\n4 & 5 & 2   &  9\n\\end{pmatrix},\n\\end{aligned}\n\\] where the first column of the matrix represents the variable \\(x_1\\), the second column of the matrix represents the variable \\(x_2\\), the third column of the matrix represents the variable \\(x_3\\), and the fourth column of the matrix represents the constant terms. We can express the augmented form in R using a matrix\n\naugmented_matrix <- matrix(c(1, 4, 4, 5, -1, 2, 11, 9), 2, 4)\naugmented_matrix\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4   -1   11\n[2,]    4    5    2    9\n\n\nand to make clear the respective variables, we can add in column names as a matrix attribute using the colnames() function\n\ncolnames(augmented_matrix) <- c(\"x1\", \"x2\", \"x3\", \"constants\")\naugmented_matrix\n\n     x1 x2 x3 constants\n[1,]  1  4 -1        11\n[2,]  4  5  2         9\n\n\nwhich adds labels to each of the columns.\nNow, using elementary row operations on the matrix, we can attempt to find solutions to the system of equations. First, we multiply the first row by -4 and add it to the second row of the matrix and replace the second row with this sum\n\naugmented_matrix[2, ] <- -4 * augmented_matrix[1, ] + augmented_matrix[2, ]\naugmented_matrix\n\n     x1  x2 x3 constants\n[1,]  1   4 -1        11\n[2,]  0 -11  6       -35\n\n\nNext, scale the second row to have a leading value of 1 by dividing by -11\n\naugmented_matrix[2, ] <- augmented_matrix[2, ] / (-11)\naugmented_matrix\n\n     x1 x2         x3 constants\n[1,]  1  4 -1.0000000 11.000000\n[2,]  0  1 -0.5454545  3.181818\n\n\nWe can then multiply the second row by -4 and add it to the first row and replace the first row with this value.\n\naugmented_matrix[1, ] <- augmented_matrix[1, ] - 4 * augmented_matrix[2, ]\naugmented_matrix\n\n     x1 x2         x3 constants\n[1,]  1  0  1.1818182 -1.727273\n[2,]  0  1 -0.5454545  3.181818\n\n\nNotice how the matrix has a “triangular” form (The lower part of the “triangle” is made of 0s and the upper part has numbers).\nThe triangular form tells us that There are infinitely many solutions to this system of equation. The infinite solutions are subject to the requirements that \\[x_1 = - \\frac{19}{11} - \\frac{13}{11} x_3\\] and \\[x_2 = \\frac{35}{11} + \\frac{6}{11} x_3.\\] To get this into a reasonable form, we will solve these equations as a function of \\(x_1\\). Solving the first equation for \\(x_3\\) gives \\[x_3 = - \\frac{19}{13} -\\frac{11}{13} x_1.\\] Then, plugging this into \\(x_3\\) in the second equation gives \\[\n\\begin{aligned}\nx_2 & = \\frac{35}{11} + \\frac{6}{11} \\left( - \\frac{19}{13} -\\frac{11}{13} x_1 \\right) \\\\\n& = \\frac{341}{143} - \\frac{6}{13} x_1\n\\end{aligned}\n\\] which defines a linear relationship between \\(x_1\\) and \\(x_2\\). Notice that in these last two solutions, \\(x_1\\) is a “free variable” and \\(x_2\\) and \\(x_3\\) are “determined” by \\(x_1\\).\nIn the plot below, the two planes (red and blue) are the geometric plots of the linear equations in the system of equations (the red plane is the top equation and the blue plane is the bottom equation). The purple line is the equation for the solution given the free variable \\(x_3\\) and lies at the intersection of the two planes, much like the point in the two lines in figure linking reference here lies at the intersection of the two points.\n\n# uses gg3D library\nn <- 60\nx1 <- x2 <- seq(-10, 10, length = n)\nregion <- expand.grid(x1 = x1, x2 = x2)\ndf <- data.frame(\n    x1 = region$x1,\n    x2 = region$x2,\n    x3 = - 11 + (region$x1 + 4 * region$x2)\n)\n\ndf2 <- data.frame(\n    x1 = region$x1,\n    x2 = region$x2,\n    x3 = (9 - 4 * region$x1 - 5 * region$x2) / 2\n) \n\ndf_solution <- data.frame(\n    x1 = x1, \n    x2 = 341 / 143 - 6 / 13 * x1,\n    x3 = -19/13 - 11/13 * x1\n) \n\n# theta and phi set up the \"perspective/viewing angle\" of the 3D plot\ntheta <- 63\nphi <- -12\nggplot(df, aes(x1, x2, z = x3)) +\n    axes_3D(theta = theta, phi = phi) +\n    stat_wireframe(alpha = 0.25, color = \"red\", theta = theta, phi = phi) +\n    stat_wireframe(data = df2, aes(x = x1, y = x2, z = x3), alpha = 0.25, color = \"blue\", theta = theta, phi = phi) +\n    stat_3D(data = df_solution, aes(x1, x2, z = x3), geom = \"line\", theta = theta, phi = phi, color = \"purple\") +\n    theme_void() +\n    theme(legend.position = \"none\") +\n    labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), angle=c(0, 0, 90), theta = theta, phi = phi) \n\nWarning: Removed 2 row(s) containing missing values (geom_path).\nRemoved 2 row(s) containing missing values (geom_path).\n\n\n\n\n\n\n\n4.1.7 Existence and Uniqueness\n\nDefinition 4.6 A system of linear equations is said to be consistent if at least one solution exists. The linear system of equations is said to have a unique solution if only one solution exists.\n\n\nExample 4.19 Is the system of linear equations consistent? If the system is consistent, does it have a unique solution?\n\n\n\n\\[\n\\begin{alignedat}{4}  16 x_1 & {}+{} & 2 x_2 & {}+{} & 3 x_3 & {}={} & 13\\\\ 5 x_1 & {}+{} & 11 x_2 & {}+{} & 10 x_3 & {}={} & 8\\\\ 9 x_1 & {}+{} & 7 x_2 & {}+{} & 6 x_3 & {}={} & 12\\\\ 4 x_1 & {}+{} & 14 x_2 & {}+{} & 15 x_3 & {}={} & 1 \\end{alignedat}\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nExample 4.20 Is the system of linear equations consistent? If the system is consistent, does it have a unique solution?\n\n\n\n\\[\n\\begin{alignedat}{4}   x_1 & {}+{} & 2 x_2 & {}+{} & 3 x_3 & {}={} & 5\\\\  x_1 & {}+{} & 3 x_2 & {}+{} & 2 x_3 & {}={} & 2\\\\ 3 x_1 & {}+{} & 2 x_2 & {}+{} &  x_3 & {}={} & 7 \\end{alignedat}\n\\]\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "04-linear-systems-of-equations.html#reduce-row-echelon-form",
    "href": "04-linear-systems-of-equations.html#reduce-row-echelon-form",
    "title": "4  Linear Systems of Equations",
    "section": "4.2 Reduce row echelon form",
    "text": "4.2 Reduce row echelon form\nReducing a matrix to row echelon form is a useful technique for working with matrices. The row echelon form can be used to solve systems of equations, as well as determine other properties of a matrix that are yet to be discussed, including rank, invertibility, column/row spaces, etc.\n\nDefinition 4.7 A matrix is said to be in echelon form if\n\nall nonzero rows are above any rows of zeros (all rows consisting entirely of zeros are at the bottom)\nthe leading entry/coefficient of a nonzero row (called the pivot) is always strictly to the right of the leading entry/coefficient of the row above\n\n\n\nExample 4.21 echelon matrix example in class\n\n\nDefinition 4.8 A matrix is in reduced row echelon form if it is in echelon form and\n\nthe leading entry/coefficient of each row is 1\nThe leading entry/coefficient of 1 is the only nonzero entry in its column.\n\n\n\nExample 4.22 rref matrix example in class\n\n\nDefinition 4.9 Echelon matrices have the property of being upper diagonal. A matrix is said to be upper diagonal if all entries of the matrix at or above the diagonal are nonzero.\n\n\nExample: **lower and non-lower diagonal matrices\n\n\nDefinition 4.10 Two matrices are row-equivalent if one matrix can be transformed to the other through elementary row operations.\n\n\nTheorem 4.1 A nonzero matrix can be transformed into more than one echelon forms. However, the reduced row echelon form of a nonzero matrix is unique.\n\n\nExample 4.23 Using elementary row operations, calculate the reduced row echelon form of the following matrices\n\nfill in later\nfill in later\nfill in later\n\n\n\n4.2.1 Pivot positions\nThe leading entry/coefficients of a row echelon form matrix are called pivots. The positions of the pivot positions are the same for any row echelon form of a matrix. In reduced row echelon form, these pivot positions take the value 1.\n\nDefinition 4.11 In a matrix that is in reduced echelon form, the pivot position is the first nonzero element of each row. The column in which the pivot position occurs is called a pivot column.\n\n\nExample 4.24 pivot position and pivot columns\n\n\n\n4.2.2 Finding the reduced row echelon form\nCalculating the reduced row echelon form is known as Gaussian elimination, which is named after Johann Carl Friedrich Gauss. This algorithm uses elementary row operations to calculate the reduced row echelon form. The following steps perform the Gaussian elimination algorithm.\n\nStart with the left-most nonzero column, which is a pivot column\nIf the top row is zero, swap rows so that the top row is nonzero so that the top row has a nonzero element in the pivot position.\nUse row multiplication and addition to zero out all positions in the pivot column below the top row (pivot position).\nIgnore this top row and repeat steps 1-3 until there are no more nonzero rows to apply steps 1-3 on. At the end of this step, the matrix is in row echelon form.\nStarting at the right-most pivot column, use elementary row operations to zero out all positions above each pivot and to make each pivot position 1. At the end of this step, the matrix is in reduced row echelon form.\n\n\nExample 4.25 in class\n\n\n# pracma library\n# rref example in class\n\n\n\n4.2.3 Using reduced row echelon forms to solve systems of linear equations\nWhen a system of linear equations is expressed as an augmented matrix, the reduced row echelon form can be used to find solutions to those systems of equations. Consider the systems of equations\n\n\n\n\\[\n\\begin{alignedat}{4}  3 x_1 & {}+{} & 8 x_2 & {}-{} & 4 x_3 & {}={} & 6\\\\ 2 x_1 & {}-{} & 4 x_2 & {}-{} & 1 x_3 & {}={} & 8\\\\ 4 x_1 & {}+{} & 5 x_2 & {}{} &   & {}={} & 9 \\end{alignedat}\n\\]\nwhich can be written in the augmented matrix form as \\[\n\\begin{pmatrix} 3 & 8 & -4 & 6 \\\\ 2 & -4 & -1 & 8 \\\\ 4 & 5 & 0 & 9 \\end{pmatrix}\n\\]\nIn R, this is the matrix\n\n# define matrix\n\nCalculating the reduced row echelon form, gives\n\n# calculate rref of augmented matrix\n\nwhich gives the solution …\n\n\nExample 4.26 \ncalculate the RREF for the augmented matrix in the example above by hand\n\n\n\nExample 4.27 Another example where we find a solution is\n\\[\n\\begin{aligned}\n5 x_1 && + && 4 x_2 && - && 2 x_3 && = & 0 \\\\\n-3 x_1 && - && 2 x_2 && - && 4 x_3 && = & 1 \\\\\n\\end{aligned}\n\\]\nDo same steps\n\n\nDefinition 4.12 In a system of linear equations that is underdetermined (fewer equations than unknowns), the determined/basic variables are those variable that have a 1 in the respective columns when in reduced row echelon form (i.e., variables in a pivot position). The variables that are not in a pivot position are called free variable.\n\n\nExample 4.28 in class\n\n\n\n4.2.4 Existence and uniqueness from reduced row echelon form\nThe row echelon form is useful to determine if a system of linear equations is consistent (the system of equations has a solution). To check if a solution to a linear system of equations exists, convert the system of equations to an augmented matrix form. Then, reduce the augmented matrix to row echelon form using elementary matrix operations. As long as there is not an equation of the form \\[\n0 = \\mbox{constant}\n\\] for some constant number not equal to 0, the system of linear equations is consistent. If the augmented matrix can be written in reduced row echelon form with no free variables, the solution to the linear system of equations is unique. These results give rise to the theorem\n\nTheorem 4.2 A linear system of equations is consistent (has a solution) if the furthest right column (the constant column) is not a pivot column. If the system of equations is consistent, (i.e., the furthest right column is not a pivot column), the solution is unique if there are no free variables and there are infinitely many solutions if there is at least one free variable.\n\n\nExample: consistent system of equations\n\n\\(\\begin{pmatrix} -7 & -9 & 7 & 8 \\\\ -4 & 0 & 6 & -6 \\\\ -10 & 3 & -8 & 5 \\end{pmatrix}\\)\n\nExample: inconsistent system of equations\n\n\\(\\begin{pmatrix} -7 & 0 & -8 & -5 \\\\ -4 & 3 & 8 & -2 \\\\ -10 & 7 & -6 & 4 \\\\ -9 & 6 & 5 & 1 \\end{pmatrix}\\)"
  },
  {
    "objectID": "05-matrix-equations.html",
    "href": "05-matrix-equations.html",
    "title": "5  Matrix equations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dasc2594)\nlibrary(gg3D)\nlibrary(MASS)\nHere we introduce the concept of the linear equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\). This equation is the most fundamental equation in all of statistics and data science. Given a matrix \\(\\mathbf{A}\\) and a vector of constants \\(\\mathbf{b}\\), the goal is to solve for the value (or values) of \\(\\mathbf{x}\\) that are a solution to this equation. The equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) is a matrix representation of the system of linear equations\n\\[\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x} & = \\mathbf{b} \\\\\n\\begin{pmatrix} \\mathbf{a}_1 & \\ldots & \\mathbf{a}_K \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_K \\end{pmatrix} & = \\mathbf{b} \\\\\nx_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K & = \\mathbf{b} \\\\\n\\end{aligned}\n\\tag{5.1}\\]\nas long as the matrix \\(\\mathbf{A}\\) has \\(n\\) rows and \\(K\\) columns and the vectors \\(\\mathbf{a}_k\\) are \\(n\\)-dimensional."
  },
  {
    "objectID": "05-matrix-equations.html#solutions-of-matrix-equations",
    "href": "05-matrix-equations.html#solutions-of-matrix-equations",
    "title": "5  Matrix equations",
    "section": "5.1 Solutions of matrix equations",
    "text": "5.1 Solutions of matrix equations\nBecause the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) is equivalent to a linear system of equations \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\), we can solve the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) by writing the equation in an augmented matrix form \\[\n\\begin{aligned}\n\\begin{pmatrix} \\mathbf{a}_1 & \\ldots & \\mathbf{a}_K & \\mathbf{b} \\end{pmatrix}\n\\end{aligned}\n\\] and then reducing the matrix to reduced row echelon form. This gives rise to the theorem\n\nTheorem 5.1 The matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\), the vector equation \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\), and the augmented matrix \\(\\begin{pmatrix} \\mathbf{a}_1 & \\ldots & \\mathbf{a}_K & \\mathbf{b} \\end{pmatrix}\\) all have the same solution set."
  },
  {
    "objectID": "05-matrix-equations.html#existence-of-solutions",
    "href": "05-matrix-equations.html#existence-of-solutions",
    "title": "5  Matrix equations",
    "section": "5.2 Existence of solutions",
    "text": "5.2 Existence of solutions\nA solution to the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) exists if and only if \\(\\mathbf{b}\\) is a linear combination of the columns of \\(\\mathbf{A}\\). In other words, \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a solution if and only if \\(\\mathbf{b}\\) is in the \\(\\mbox{span}\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\}\\).\n\nExample: in class Let \\(\\mathbf{A} =\\ldots\\) and \\(\\mathbf{b} = \\ldots\\). Is the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) consistent?\n\n\nTheorem 5.2 For the \\(n \\times K\\) matrix \\(\\mathbf{A}\\), the following statements are equivalent:\n\nFor each \\(\\mathbf{b} \\in \\mathcal{R}^n\\), the equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has at least one solution\nEach \\(\\mathbf{b} \\in \\mathcal{R}^n\\) is a linear combination of the columns of \\(\\mathbf{A}\\)\nThe columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^n\\)\n\\(\\mathbf{A}\\) has \\(n\\) pivot columns. (\\(\\mathbf{A}\\) has a pivot in every row)"
  },
  {
    "objectID": "05-matrix-equations.html#matrix-vector-multiplication",
    "href": "05-matrix-equations.html#matrix-vector-multiplication",
    "title": "5  Matrix equations",
    "section": "5.3 Matrix-vector multiplication",
    "text": "5.3 Matrix-vector multiplication\nTo calculate \\(\\mathbf{A} \\mathbf{x}\\), we need to define matrix multiplication. The equivalence between the linear systems of equations \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\) and the matrix equation \\(\\mathbf{A} \\mathbf{x}\\) gives a hint in how to do this. First, recall the definition of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\)\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix}\na_{11} & a_{12} & \\ldots & a_{1K} \\\\\na_{21} & a_{22} & \\ldots & a_{2K} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\ldots & a_{nK} \\\\\n\\end{pmatrix} && \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_K \\end{pmatrix}\n\\end{aligned}\n\\] The matrix product \\(\\mathbf{A}\\mathbf{x}\\) is the linear system of equations \\[\n\\begin{aligned}\n\\mathbf{A}  \\mathbf{x} & = \\begin{pmatrix}\na_{11} & a_{12} & \\ldots & a_{1K} \\\\\na_{21} & a_{22} & \\ldots & a_{2K} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\ldots & a_{nK} \\\\\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\\\\n& = x_1\\begin{pmatrix}\na_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{n1}\n\\end{pmatrix} +\nx_2 \\begin{pmatrix}\na_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{n2}\n\\end{pmatrix} +\n\\cdots + x_K \\begin{pmatrix}\na_{1K} \\\\ a_{nK} \\\\ \\vdots \\\\ a_{nK} \\end{pmatrix} \\\\\n& =  \\begin{pmatrix}\na_{11} x_1 + a_{12} x_2 + \\ldots + a_{1K} x_K \\\\\na_{21} x_1 + a_{22} x_2 + \\ldots + a_{2K} x_K \\\\\n\\vdots \\\\\na_{n1} x_1 + a_{n2} x_2 + \\ldots + a_{nK} x_K \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\] Notice that the first row of the last matrix above has the sum first row of the matrix \\(\\mathbf{A}\\) multiplied by the corresponding elements in \\(\\mathbf{x}\\) (i.e., first element \\(a_{11}\\) of the first row of \\(\\mathbf{A}\\) times the first element \\(x_1\\) of \\(\\mathbf{x}\\) plus the second, third, fourth, etc.). Likewise, this pattern holds for the second row, and all the other rows. This gives an algorithm for evaluating the product \\(\\mathbf{A} \\mathbf{x}\\).\n\nDefinition 5.1 The product \\(\\mathbf{A}\\mathbf{x}\\) of a \\(n \\times K\\) matrix \\(\\mathbf{A}\\) with a \\(K\\)-vector \\(\\mathbf{x}\\) is a \\(n\\)-vector where the \\(i\\)th element of \\(\\mathbf{A}\\mathbf{x}\\) is the sum of the \\(i\\)th row of \\(\\mathbf{A}\\) times the corresponding elements of the vector \\(\\mathbf{x}\\)\n\n\nExample: in class\nExample: in class\nExample: in R using loops\nExample: in R using %*%"
  },
  {
    "objectID": "05-matrix-equations.html#properties-of-matrix-vector-multiplication",
    "href": "05-matrix-equations.html#properties-of-matrix-vector-multiplication",
    "title": "5  Matrix equations",
    "section": "5.4 Properties of matrix-vector multiplication",
    "text": "5.4 Properties of matrix-vector multiplication\nIf \\(\\mathbf{A}\\) is a \\(n \\times K\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^K\\) and \\(c\\) is a scalar, then\n\n\\(\\mathbf{A} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{A} \\mathbf{u} + \\mathbf{A} \\mathbf{v}\\)\n\\(\\mathbf{A} (c \\mathbf{u}) = (c \\mathbf{A}) \\mathbf{u}\\)\n\n\nProof in class"
  },
  {
    "objectID": "05-matrix-equations.html#solutions-of-linear-systems",
    "href": "05-matrix-equations.html#solutions-of-linear-systems",
    "title": "5  Matrix equations",
    "section": "5.5 Solutions of linear systems",
    "text": "5.5 Solutions of linear systems\n\n5.5.1 Homogeneous linear systems of equations\n\nDefinition 5.2 The matrix equation \\[\n\\begin{aligned}\n\\mathbf{A}\\mathbf{x} = \\mathbf{0}\n\\end{aligned}\n\\tag{5.2}\\] is called a homogeneous system of equations. The vector \\(\\mathbf{0}\\) is a vector of length \\(N\\) composed of all zeros. The trivial solution of the homogeneous equation is when \\(\\mathbf{x} = \\mathbf{0}\\) and is not a very useful solution. Typically one is interested in nontrivial solutions where \\(\\mathbf{x} \\neq \\mathbf{0}\\).\n\nThe homogeneous linear system of equations can be written in augmented matrix form \\[\n\\begin{aligned}\n\\begin{pmatrix} \\mathbf{a}_1 & \\ldots & \\mathbf{a}_K & \\mathbf{0} \\end{pmatrix}\n\\end{aligned}\n\\] which implies that a non-trivial solution only exists if there is a free variable. Another way of saying this is that at least one column \\(\\mathbf{A}\\) must not be a pivot column (Note: the last column of the augmented matrix will not be a pivot column because it will be a column of zeros). If every column of \\(\\mathbf{A}\\) were a pivot column, the reduced row echelon form of the augmented matrix would be \\[\n\\begin{aligned}\n\\begin{pmatrix}\n1 & 0 & \\ldots & 0 & 0 \\\\\n0 & 1 & \\ldots & 0 & 0 \\\\\n0 & 0 & \\ldots & 1 & 0\n\\end{pmatrix}\n\\end{aligned}\n\\] which implies the only solution is the trivial solution \\(\\mathbf{0}\\).\n\nExample: in class\n\n\\[\n\\begin{aligned}\n3 x_1 - 2 x_2 + 4 x_3 = 0 \\\\\n- 2 x_1 + 4 x_2 - 2 x_3 = 0 \\\\\n5 x_1 - 6 x_2 + 6 x_3 = 0\n\\end{aligned}\n\\] * Example: in class\nConsider the equation\n\\[\n\\begin{aligned}\n2x_1 + 4 x_2 - x_3 = 0.\n\\end{aligned}\n\\] we can write this as \\[\n\\begin{aligned}\nx_1 = -2 x_2 + \\frac{1}{2} x_3\n\\end{aligned}\n\\] where \\(x_2\\) and \\(x_3\\) are free variables. Writing this as a solution \\(\\mathbf{x}\\) gives \\[\n\\begin{aligned}\n\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} -2 x_2 + \\frac{1}{2} x_3 \\\\ x_2 \\\\ x_3 \\end{pmatrix} =\nx_2 \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} +\nx_3 \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\] which is a linear combination of the vectors \\(\\mathbf{u} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix}\\). This implies that we can write the solution \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) for scalars \\(a\\) and \\(b\\). Therefore, the solution set \\(\\mathbf{x}\\) is contained in the \\(\\mbox{span}\\{\\mathbf{u}, \\mathbf{v}\\}\\). Because the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are linearly independent (they don’t point in the same direction), the set of all linear combinations of \\(c \\mathbf{u} + d \\mathbf{v}\\) defines a plane.\n\nDefinition 5.3 A solution set of the form \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) is called a parametric vector solution."
  },
  {
    "objectID": "05-matrix-equations.html#solutions-to-nonhomogeneous-systems",
    "href": "05-matrix-equations.html#solutions-to-nonhomogeneous-systems",
    "title": "5  Matrix equations",
    "section": "5.6 Solutions to nonhomogeneous systems",
    "text": "5.6 Solutions to nonhomogeneous systems\nRecall the simple linear equation \\[\ny = mx + b\n\\] where \\(m\\) is the slope and \\(b\\) is the y-intercept. Setting \\(b = 0\\) gives a simple homogenous linear equation where the y-intercept goes through the origin (0, 0). When \\(b\\) is nonzero, the line keeps the same slope but is shifted upward/downward by \\(b\\).\n\nggplot(data = data.frame(x = 0, y = 0), aes(x, y)) +\n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    geom_abline(slope = 2, intercept = 0, color = \"red\") +\n    geom_abline(slope = 2, intercept = 2, color = \"blue\") +\n    coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4))  +\n    geom_text(\n        data = data.frame(x = c(0, 0), y = c(0, 2), text = c(\"homogeneous\\nsolution\", \"inhomogeneous\\nsolution\")),\n        aes(x = x + c(1.75, -1), y = y + 0.5, label = text), size = 5, inherit.aes = FALSE,\n        color = c(\"red\", \"blue\")) +\n    geom_segment(\n        aes(x = 0, xend = 0, y = 0, yend = 2),\n        arrow = arrow(length = unit(0.1, \"inches\")), \n        size = 1.5, color = \"orange\") +\n    geom_text(\n        data = data.frame(x = 0, y = 2, text = \"b\"),\n        aes(x = x + 0.5, y = y, label = text), \n        size = 8, inherit.aes = FALSE,\n        color = \"orange\") \n\n\n\n\nThis shift in location (but not in slope) is called a translation\n\nExample: Show this shift for a system of linear equations where the solution set defines a plane. From example above, \\[\n\\begin{aligned}\n2x_1 + 4 x_2 - x_3 = 0.\n\\end{aligned}\n\\] has the parametric solution \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) with \\[\n\\begin{aligned}\n\\mathbf{u} & = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} &\n\\mathbf{v} & = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\n\nNow, if we change the system of linear equations so that we have the inhomogeneous equation \\[\n\\begin{aligned}\n2x_1 + 4 x_2 - x_3 = 20.\n\\end{aligned}\n\\] we get the homogeneous solution set \\(x_1 = -2 x_2 + \\frac{1}{2} x_3 + 10\\) which can be written in parametric form as \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v} + \\mathbf{p}\\) with \\[\n\\begin{aligned}\n\\mathbf{u} & = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} &\n\\mathbf{v} & = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} &\n\\mathbf{p} & = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nFor plotting, we will solve these equations for \\(x_3\\), letting \\(x_1\\) and \\(x_2\\) be free variables (this is just for the requirements of the plotting function). Thus, the homogeneous equation has the solution \\(x_3 = 2x_1 + 4x_2\\) and the inhomogenous equation has the solution \\(x_3 = 2x_1 + 4x_2 - 20\\).\n\n# uses gg3D library\nn <- 60\nx1 <- x2 <- seq(-10, 10, length = n)\nregion <- expand.grid(x1 = x1, x2 = x2)\ndf <- data.frame(\n    x1 = region$x1,\n    x2 = region$x2,\n    x3 = c(\n        2 * region$x1 + 4 * region$x2,\n        2 * region$x1 + 4 * region$x2 - 20),\n    equation = rep(c(\"inhomogeneous\", \"homogeneous\"), each = n^2))\n\n# theta and phi set up the \"perspective/viewing angle\" of the 3D plot\ntheta <- 45\nphi <- 20\nggplot(df, aes(x = x1, y = x2, z = x3, color = equation)) +\n    axes_3D(theta = theta, phi = phi) +\n    stat_wireframe(\n        alpha = 0.75,\n        theta = theta, phi = phi) +\n    scale_color_manual(values = c(\"inhomogeneous\" = \"blue\", \"homogeneous\" = \"red\")) +\n    theme_void() +\n    labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), \n            angle=c(0, 0, 90), theta = theta, phi = phi) \n\nWarning: Removed 4 row(s) containing missing values (geom_path).\n\n\n\n\n\n\nExample: in class Let’s revisit the example from before\n\n\\[\n\\begin{aligned}\n3 x_1 - 2 x_2 + 4 x_3 = 0 \\\\\n- 2 x_1 + 4 x_2 - 2 x_3 = 0 \\\\\n5 x_1 - 6 x_2 + 6 x_3 = 0\n\\end{aligned}\n\\] but change this so that \\(\\mathbf{b} = \\begin{pmatrix} 2 \\\\ -6 \\\\ 8 \\end{pmatrix}\\)\n\nWrite this as a parametric solution with a mean shift\n\n\nA <- matrix(c(3, -2, 5, -2, 4, -6, 4, -2, 6, 2, -6, 8), 3, 4)\nfractions(rref(A))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0  3/2 -1/2\n[2,]    0    1  1/4 -7/4\n[3,]    0    0    0    0\n\n\n\\[\n\\begin{aligned}\nx_1 = - \\frac{3}{2} x_3  - \\frac{1}{2}\\\\\nx_2 = - \\frac{1}{4} x_3  - \\frac{7}{4} \\\\\n\\end{aligned}\n\\] which was the same solution set as the homoegenous solution \\(\\begin{pmatrix} -\\frac{3}{2} \\\\ - \\frac{1}{4} \\\\ 1 \\end{pmatrix}\\) plus the additional vector \\(\\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{7}{4} \\\\ 0 \\end{pmatrix}\\). Thus, the inhomogenous solution is now \\(\\mathbf{x} = c \\mathbf{u} + \\mathbf{p}\\) where \\[\n\\begin{aligned}\n\\mathbf{u} &= \\begin{pmatrix} -\\frac{3}{2} \\\\ - \\frac{1}{4} \\\\ 1 \\end{pmatrix} &\n\\mathbf{p} &= \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{7}{4} \\\\ 0 \\end{pmatrix}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "05-matrix-equations.html#finding-solutions",
    "href": "05-matrix-equations.html#finding-solutions",
    "title": "5  Matrix equations",
    "section": "5.7 Finding solutions",
    "text": "5.7 Finding solutions\nThe following algorithm describes how to solve a linear system of equations.\n\nPut the system of equations in an augmented matrix form\nReduce the augmented matrix to reduced row echelon form\nExpress each determined variable as a function of the free variables.\nWrite the solution in a general form where the determined variables are a function of the independent variables\nDecompose the solution \\(\\mathbf{x}\\) into a linear combination of free variables as parameters"
  },
  {
    "objectID": "06-linear-independence.html",
    "href": "06-linear-independence.html",
    "title": "6  Linear independence",
    "section": "",
    "text": "Definition 6.1 The set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) are called linearly independent if the only solution to the vector equation \\(\\sum_{k=1}^K x_k \\mathbf{a}_k = \\mathbf{0}\\) is the trivial solution. The set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) are called linearly dependent if there are coefficients \\(x_1, \\ldots, x_K\\) that are not all zero.\n\n\nExample 6.1 In class\n\nWhat does it mean for a set of vectors to be linearly dependent? This means that there is at least one vector \\(\\mathbf{a}_k\\) that can be written as a sum of the other vectors with coefficients \\(x_k\\): \\[\n\\begin{aligned}\n\\mathbf{a}_k = \\sum_{j \\neq k} x_{j} \\mathbf{a}_{j}\n\\end{aligned}\n\\] Note: linear dependence does not imply that all vectors \\(\\mathbf{a}_{k}\\) can be written as a linear combination of other vectors, just that there is at least one such vector in the set.\n\nExample 6.2 Example: in class – determine if the vectors are linearly independent and solve the dependence relation\n\n\nTheorem 6.1 The matrix \\(\\mathbf{A}\\) has linearly independent columns if and only if the matrix equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution.\n\n\nExample 6.3 Example: in class A set of a single vector\n\n\nExample 6.4 Example: in class A set of two vectors\n\nlinearly independent if:\nlinearly dependent if one vector is a scalar multiple of the other:\n\n\n\nTheorem 6.2 If an \\(n \\times K\\) matrix \\(\\mathbf{A}\\) has \\(K > n\\), then the columns of \\(\\mathbf{A}\\) are linearly dependent. In other words, if a set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) contains more vectors than entries within vectors, the set of vectors is linearly dependent.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf \\(K>n\\), there are more variables (\\(K\\)) than equations (\\(n\\)). Therefore, there is at least one free variable and this implies that the homogeneous equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\) has a non-trivial solution Equation 5.2\n\n\n\n\nTheorem 6.3 If a set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) contains the \\(\\mathbf{0}\\) vector, then the the set of vectors is linearly dependent.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nin class\n\n\n\n\nExample 6.5 In class: Determine whether the following sets of vectors are linearly dependent"
  },
  {
    "objectID": "07-linear-transformations-matrix.html",
    "href": "07-linear-transformations-matrix.html",
    "title": "7  Linear Transformations",
    "section": "",
    "text": "3 Blue 1 Brown – Linear transformations\n3 Blue 1 Brown – 3D transformations\nIt is often useful to think of \\(\\mathbf{A}\\mathbf{x}\\) as a linear transformation defined by the matrix \\(\\mathbf{A}\\) applied to the vector \\(\\mathbf{x}\\).\nA linear transformation is mathematically defined as a function/mapping \\(T(\\cdot)\\) (\\(T\\) for transformation) from a domain in \\(\\mathcal{R}^n\\) (function input) to a codomain in \\(\\mathcal{R}^m\\) (function output). In shorthand, this is written as \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) which is read a “\\(T\\) maps inputs from the domain \\(\\mathcal{R}^n\\) to the codomain \\(\\mathcal{R}^m\\).” For each \\(\\mathbf{x} \\in \\mathcal{R}^n\\) (in the domain), \\(T(\\mathbf{x}) \\in \\mathcal{R}^m\\) is known as the image of \\(\\mathbf{x}\\). The set of all \\(T(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathcal{R}^n\\) is known as the range of \\(T(\\mathbf{x})\\). Note that it is possible that the range of \\(T(\\mathbf{x})\\) is not required to be the entire space \\(\\mathcal{R}^m\\) (i.e., the range of the transformation \\(T\\) might be a subset of \\(\\mathcal{R}^m\\))\nDraw figure\nIn the case of matrix transformations (linear transformations), the function \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x}\\) where \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix and \\(\\mathbf{x} \\in \\mathcal{R}^n\\) is a \\(n\\)-vector.\nUsing the matrix transformation notation, the domain of the transformation \\(T\\) is \\(\\mathcal{R}^n\\), the codomain of \\(\\mathcal{T}\\) \\(\\mathcal{R}^m\\). The range of the transformation \\(T\\) is the set of all linear combinations of the columns of \\(\\mathbf{A}\\) (the \\(\\mbox{span}\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}\\)) because the transformation \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x}\\) is a linear combination \\(\\sum_{i=1}^n x_i \\mathbf{a}_i\\) of the columns \\(\\{\\mathbf{a}_i\\}_{i=1}^n\\) of \\(\\mathbf{A}\\) with coefficients \\(x_1, \\ldots, x_n\\)"
  },
  {
    "objectID": "07-linear-transformations-matrix.html#linear-transformations",
    "href": "07-linear-transformations-matrix.html#linear-transformations",
    "title": "7  Linear Transformations",
    "section": "7.1 Linear Transformations",
    "text": "7.1 Linear Transformations\n\nDefinition 7.1 A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is linear if\n\n\\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) for all \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in the domain of \\(T\\)\n\\(T(c \\mathbf{u}) = c T(\\mathbf{u})\\) for all scalars \\(c\\) and all vectors \\(\\mathbf{u}\\) in the domain of \\(T\\)\n\n\nNote: Because a linear transformation is equivalent to a matrix transformation, the definition above is equivalent to the following matrix-vector multiplication properties\nIf \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^m\\) and \\(c\\) is a scalar, then\n\n\\(\\mathbf{A} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{A} \\mathbf{u} + \\mathbf{A} \\mathbf{v}\\)\n\\(\\mathbf{A} (c \\mathbf{u}) = (c \\mathbf{A}) \\mathbf{u}\\)\n\nAs a consequence of the previous definition, the following properties hold for scalars \\(c\\) and \\(d\\) and vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v} \\in \\mathcal{R}^m\\)\n\n\\(T(\\mathbf{0}) = \\mathbf{0}\\)\n\\(T(c \\mathbf{u} + d \\mathbf{v}) = c T(\\mathbf{u}) + d T(\\mathbf{v})\\)\n\n\nShow why in class\n\nThese properties give rise to the following statement for scalars \\(c_1, \\ldots, c_m\\) and vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_m \\in \\mathcal{R}^n\\)\n\n\\(T(c_1 \\mathbf{u}_1 + \\ldots + c_m \\mathbf{u}_m) = c_1 T(\\mathbf{u}_1) + \\ldots + c_m T(\\mathbf{u}_m)\\)\n\nThe statements above for linear transformations are equivalent to the matrix statements where \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^m\\) and \\(c\\) is a scalar:\n\n\\(\\mathbf{A} \\mathbf{0} = \\mathbf{0}\\)\n\\(\\mathbf{A}(c \\mathbf{u} + d \\mathbf{v}) = c \\mathbf{A} \\mathbf{u} + d \\mathbf{A} \\mathbf{v}\\)\n\nAnd for a \\(m \\times n\\) matrix \\(\\mathbf{A}\\), scalars \\(c_1, \\ldots, c_m\\), and vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_m \\in \\mathcal{R}^n\\)\n\n\\(\\mathbf{A}(c_1 \\mathbf{u}_1 + \\ldots + c_m \\mathbf{u}_m) = c_1 \\mathbf{A}\\mathbf{u}_1 + \\ldots + c_m \\mathbf{A} \\mathbf{u}_m\\)"
  },
  {
    "objectID": "07-linear-transformations-matrix.html#types-of-matrix-transformations",
    "href": "07-linear-transformations-matrix.html#types-of-matrix-transformations",
    "title": "7  Linear Transformations",
    "section": "7.2 Types of matrix transformations",
    "text": "7.2 Types of matrix transformations\nThe basic types of matrix transformations include\n\ncontractions/expansions\nrotations\nreflections\nshears\nprojections\n\nFor the following examples, we will consider the unit vectors \\(\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\) and apply different linear transformations using the matrix \\(\\mathbf{A}\\).\nTo build the matrix transformations, we use the dasc2594 package and build matrix transformations based on code from https://www.bryanshalloway.com/2020/02/20/visualizing-matrix-transformations-with-gganimate/.\n\n7.2.1 Contractions/Expansions\n\nHorizonal Expansion\nThe matrix below gives a horizontal expansion when \\(x > 1\\)\n\\[\n\\mathbf{A} = \\begin{pmatrix}\nx & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\]\n\nIn the example below, we set \\(x = 2\\) and generate the transformation.\n\n\ntransformation_matrix <- tribble(\n  ~ x, ~ y,\n  2, 0,\n  0, 1) %>% \n  as.matrix()\n\np <- plot_transformation(transformation_matrix)\n\n\n\n\n\n\n\n\n\n\n\n\n\nHorizonal Contraction\nThe matrix below gives a horizontal contraction when \\(x < 1\\) * Horizontal contraction when \\(x < 1\\)\n\\[\n\\mathbf{A} = \\begin{pmatrix}\nx & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\]\n\nIn the example below, we set \\(x = 0.5\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVertical Expansion\nThe matrix below gives a vertical expansion when \\(x > 1\\)\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & x\n\\end{pmatrix}\n\\] * In the example below, we set \\(x = 2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVertical Contraction\nThe matrix below gives a vertical contraction when \\(x < 1\\)\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & x\n\\end{pmatrix}\n\\]\n\nIn the example below, we set \\(x = 0.5\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Rotations\n\nRotation by 90 degrees\nRotations in 2D of an angle \\(\\theta \\in [0, 2\\pi]\\) take the form of \\[\n\\mathbf{A} = \\begin{pmatrix}\n\\cos(\\theta) & -\\sin(\\theta) \\\\\n\\sin(\\theta) & \\cos(\\theta)\n\\end{pmatrix}\n\\] For example, a rotation of 90 degrees counter-clockwise (\\(\\theta = \\frac{\\pi}{2}\\)) is given by the transformation matrix \\[\n\\mathbf{A} = \\begin{pmatrix}\n\\cos(\\frac{\\pi}{2}) & -\\sin(\\frac{\\pi}{2}) \\\\\n\\sin(\\frac{\\pi}{2}) & \\cos(\\frac{\\pi}{2})\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother example is for a rotation of 45 degrees clockwise (\\(\\theta = -\\frac{\\pi}{4}\\)) is given by the transformation matrix \\[\n= \\begin{pmatrix}\n\\cos(\\frac{\\pi}{4}) & -\\sin(\\frac{\\pi}{4}) \\\\\n\\sin(\\frac{\\pi}{4}) & \\cos(\\frac{\\pi}{4})\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\\\\n\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Reflections\n\nReflection across the x-axis\nThe matrix below gives a reflection about the x-axis\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection across the y-axis\nThe matrix below gives a reflection about the y-axis\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n-1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection across the line y = x\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix}\n\\]\n\nIn the example below, we set \\(x = 0.5\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection across the line y = - x\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n0 & -1 \\\\\n-1 & 0\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection across the origin (0, 0)\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n-1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.4 Shears\nA shear transformation is like stretching play-dough if it was possible to stretch all parts of the dough uniformly (rather than some sections getting stretched more than others).\n\nHorizontal Shear\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & x \\\\\n0 & 1\n\\end{pmatrix}\n\\] For the example below, we plot a horizontal shear with \\(x = 2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVertical Shear\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & 0 \\\\\nx & 1\n\\end{pmatrix}\n\\] For the example below, we plot a horizontal shear with \\(x = 2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.5 Projections\nA projection is a mapping \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) from one space (\\(\\mathbf{R}^n\\)) to itself (\\(\\mathbf{R}^n\\)) such that \\(T^2 = T\\)\n\nProject onto the x-axis\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n\\] For the example below, we plot a projection onto the x-axis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject onto the y-axis\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\] For the example below, we plot a projection onto the y-axis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.6 Identity\nThe identity transformation is the transformation that leaves the vector input unchanged. The identity matrix is typically written as \\(\\mathbf{I}\\)\n\\[\n\\mathbf{I} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "07-linear-transformations-matrix.html#properties-of-matrix-transformations",
    "href": "07-linear-transformations-matrix.html#properties-of-matrix-transformations",
    "title": "7  Linear Transformations",
    "section": "7.3 Properties of matrix transformations",
    "text": "7.3 Properties of matrix transformations\n\n7.3.1 One-to-one transformations\n\nDefinition 7.2 A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is called one-to-one if every vector \\(\\mathbf{b}\\) in the image \\(\\mathcal{R}^m\\), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at most one solution in \\(\\mathcal{R}^n\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following statements are equivalent was of saying \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is one-to-one:\n\nFor every \\(\\mathbf{b} \\in \\mathcal{R}^m\\) (for every vector in the image), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has either zero or one solution\nEvery different input into the function \\(T(\\cdot)\\) has a different output\nIf \\(T(\\mathbf{u}) = T(\\mathbf{v})\\) then \\(\\mathbf{u} = \\mathbf{v}\\)\n\nThe following statements are equivalent was of saying \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is not one-to-one:\n\nThere exists as least one \\(\\mathbf{b} \\in \\mathcal{R}^m\\) such that the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has more than one solution\nThere are at least two different inputs into the function \\(T(\\cdot)\\) that have the same output\nThere exist vectors \\(\\mathbf{u} \\neq \\mathbf{v} \\in \\mathcal{R}^n\\) such that \\(T(\\mathbf{u}) = T(\\mathbf{v})\\)\n\n\nTheorem 7.1 Let \\(\\mathbf{A}\\mathbf{x}\\) be the matrix representation of the linear transformation \\(T(\\mathbf{x})\\) for the \\(m \\times n\\) matrix \\(\\mathbf{A}\\). Then the following statements are equivalent:\n\n\\(T\\) is one-to-one.\nFor every \\(\\mathbf{b} \\in \\mathcal{R}^m\\), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at most one solution.\nFor every \\(\\mathbf{b} \\in \\mathcal{R}^m\\), the equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) has a unique solution or is inconsistent.\nThe equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only a trivial solution.\nThe columns of \\(\\mathbf{A}\\) are linearly independent.\n\\(\\mathbf{A}\\) has a pivot in every column.\nThe range of \\(\\mathbf{A}\\) has dimension \\(n\\)\n\n\n\nExample: is the following matrix one-to-one?\n\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\]\n\nExample: is the following matrix one-to-one?\n\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n1 & 1 & 0\n\\end{pmatrix}\n\\]\nNote: Matrices that are wider than they are tall are not one-to-one transformations. (This does not mean that all tall matrices are one-to-one)\n\n\n7.3.2 Onto transformations\n\nDefinition 7.3 A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is called onto if, for every vector \\(\\mathbf{b} \\in \\mathcal{R}^m\\), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at least one solution \\(\\mathbf{x} \\in \\mathcal{R}^n\\)\n\n\n\n\n\n\n\n\n\n\n\nThe following are equivalent ways of saying that \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is onto:\n\nThe range of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is equal to the codomain of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\)\nEvery vector in the codomain is the output of some input vector\n\nThe following are equivalent ways of saying that \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is not onto:\n\nThe range of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is smaller than the codomain of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\).\nThere exists a vector \\(\\mathbf{b} \\in \\mathcal{R}^m\\) such that the equation \\(T(\\mathbf{x})\\) does not have a solution.\nThere is a vector in the codomain that is not the output of any input vector.\n\n\nTheorem 7.2 Let \\(\\mathbf{A}\\mathbf{x}\\) be the matrix representation of the linear transformation \\(T(\\mathbf{x})\\) for the \\(m \\times n\\) matrix \\(\\mathbf{A}\\). Then the following statements are equivalent:\n\n\\(T\\) is onto\n\\(T(\\mathbf{x}) = \\mathbf{b}\\) has at least one solution for every \\(\\mathbf{b} \\in \\mathcal{R}^m\\).\nThe equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) is consistent for every \\(\\mathbf{b} \\in \\mathcal{R}^m\\).\nThe columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^m\\)\n\\(\\mathbf{A}\\) has a pivot in every row\nThe range of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) has dimension \\(m\\)\n\n\n\nExample:\nExample: is the following matrix onto?\n\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\]\n\nExample: is the following matrix one-to-one?\n\n\\[\n\\mathbf{A} = \\begin{pmatrix}\n1 & 0  \\\\\n0 & 1  \\\\\n1 & 0\n\\end{pmatrix}\n\\]\nNote: Matrices that are taller than they are wide are not onto transformations. (This does not mean that all wide matrices are onto)"
  },
  {
    "objectID": "02-block-matrices.html",
    "href": "02-block-matrices.html",
    "title": "8  Block Matrices",
    "section": "",
    "text": "Another way to represent matrices is using a block (or partitioned) form. A block-representation of a matrix arises when the \\(n \\times p\\) matrix \\(\\mathbf{A}\\) is represented using smaller blocks as follows:\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\begin{pmatrix} \\mathbf{A}_{11} & \\mathbf{A}_{12} & \\cdots & \\mathbf{A}_{1K} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22} &  \\cdots & \\mathbf{A}_{2K} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{A}_{J1} & \\mathbf{A}_{J2} & \\cdots & \\mathbf{A}_{JK} \\\\\n\\end{pmatrix} \\\\\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{A}_{ij}\\) is a \\(n_j \\times p_k\\) matrix where \\(\\sum_{j=1}^J n_j = n\\) and \\(\\sum_{k=1}^K p_k = p\\).\nFor example, the matrix\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\begin{pmatrix} 5 & 7 & 1 \\\\\n5 & -22  & 2 \\\\\n-14 & 5 & 99 \\\\\n42 & -3 & 0\\end{pmatrix},\n\\end{aligned}\n\\]\ncan be written in block matrix form with\n\\[\n\\begin{aligned}\n\\mathbf{A} & =\n\\begin{pmatrix} \\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22} \\end{pmatrix} \\\\\n& = \\begin{pmatrix} \\begin{bmatrix} 5 & 7 \\\\\n5 & -22 \\end{bmatrix} &\n\\begin{bmatrix} 1 \\\\\n2 \\end{bmatrix} \\\\\n\\begin{bmatrix}\n-14 & 5 \\\\\n42 & -3\n\\end{bmatrix} &\n\\begin{bmatrix} 99 \\\\ 0 \\end{bmatrix}\n\\end{pmatrix},\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{A}_{11} = \\begin{bmatrix} 5 & 7 \\\\ 5 & -22 \\end{bmatrix}\\) is a \\(2 \\times 2\\) matrix, \\(\\mathbf{A}_{12} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) is a \\(2 \\times 1\\) matrix, etc."
  },
  {
    "objectID": "02-block-matrices.html#block-matrix-addition",
    "href": "02-block-matrices.html#block-matrix-addition",
    "title": "8  Block Matrices",
    "section": "8.1 Block Matrix Addition",
    "text": "8.1 Block Matrix Addition\nIf \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m \\times n\\) block matrices with blocks in \\(r\\) rows and \\(c\\) columns where\n\\[\n\\begin{aligned}\n\\mathbf{A} & =\n\\begin{pmatrix} \\mathbf{A}_{11} & \\mathbf{A}_{12} & \\cdots & \\mathbf{A}_{1c}\\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22} &\\cdots & \\mathbf{A}_{2c} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{A}_{r1} & \\mathbf{A}_{r2} &\\cdots & \\mathbf{A}_{rc} \\\\\n\\end{pmatrix} &\n\\mathbf{B} & =\n\\begin{pmatrix} \\mathbf{B}_{11} & \\mathbf{B}_{12} & \\cdots & \\mathbf{B}_{1c}\\\\\n\\mathbf{B}_{21} & \\mathbf{B}_{22} &\\cdots & \\mathbf{B}_{2c} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{B}_{r1} & \\mathbf{B}_{r2} &\\cdots & \\mathbf{B}_{rc} \\\\\n\\end{pmatrix} \\\\\n\\end{aligned}\n\\]\nand each block \\(\\mathbf{A}_{ij}\\) and \\(\\mathbf{B}_{ij}\\) have the same dimension, then\n\\[\n\\begin{aligned}\n\\mathbf{A} + \\mathbf{B} & =\n\\begin{pmatrix} \\mathbf{A}_{11} + \\mathbf{B}_{11} & \\mathbf{A}_{12} + \\mathbf{B}_{12} & \\cdots & \\mathbf{A}_{1c} + \\mathbf{B}_{1c}\\\\\n\\mathbf{A}_{21} + \\mathbf{B}_{21} & \\mathbf{A}_{22} + \\mathbf{B}_{22} & \\cdots & \\mathbf{A}_{2c} + \\mathbf{B}_{2c} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{A}_{r1} + \\mathbf{B}_{r1} & \\mathbf{A}_{r2} + \\mathbf{B}_{r2} & \\cdots & \\mathbf{A}_{rc} + \\mathbf{B}_{rc} \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\tag{8.1}\\]\nwhich is a matrix where each block is the sum of the other blocks. Notice that if each block was a scalar rather than a block matrix, this would be the usual definition of matrix addition (compare Equation 8.1) above to Equation 2.1). The one requirement is that each of the blocks \\(\\mathbf{A}_{ij}\\) and \\(\\mathbf{B}_{ij}\\) have the same dimension. When this is true, we say that \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are conformable for block matrix addition."
  },
  {
    "objectID": "02-block-matrices.html#block-matrix-multiplication",
    "href": "02-block-matrices.html#block-matrix-multiplication",
    "title": "8  Block Matrices",
    "section": "8.2 Block Matrix Multiplication",
    "text": "8.2 Block Matrix Multiplication\nIf \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m \\times n\\) block matrices with blocks in \\(r\\) rows and \\(c\\) columns (same as above) where\n\\[\n\\begin{aligned}\n\\mathbf{A} & =\n\\begin{pmatrix} \\mathbf{A}_{11} & \\mathbf{A}_{12} & \\cdots & \\mathbf{A}_{1c}\\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22} &\\cdots & \\mathbf{A}_{2c} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{A}_{r1} & \\mathbf{A}_{r2} &\\cdots & \\mathbf{A}_{rc} \\\\\n\\end{pmatrix} &\n\\mathbf{B} & =\n\\begin{pmatrix} \\mathbf{B}_{11} & \\mathbf{B}_{12} & \\cdots & \\mathbf{B}_{1c}\\\\\n\\mathbf{B}_{21} & \\mathbf{B}_{22} &\\cdots & \\mathbf{B}_{2c} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{B}_{r1} & \\mathbf{B}_{r2} &\\cdots & \\mathbf{B}_{rc} \\\\\n\\end{pmatrix} \\\\\n\\end{aligned}\n\\]\nand each row of blocks \\(\\mathbf{A}_{ij}\\) has the same number of columns as the block \\(\\mathbf{B}_{ij}\\) has rows, then the block matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are said to be conformable for block matrix multiplication. A consequence of this is that \\(r = c\\). When this is the case, the matrix products is\n\\[\n\\begin{aligned}\n\\mathbf{A} \\mathbf{B} & =\n\\begin{pmatrix} \\sum_{j = 1}^c \\mathbf{A}_{1j} \\mathbf{B}_{j1} &  \\sum_{j = 1}^c \\mathbf{A}_{1j} \\mathbf{B}_{j2} & \\cdots &  \\sum_{j = 1}^c \\mathbf{A}_{1j} \\mathbf{B}_{jc} \\\\\n\\sum_{j = 1}^c \\mathbf{A}_{2j} \\mathbf{B}_{j1} &  \\sum_{j = 1}^c \\mathbf{A}_{2j} \\mathbf{B}_{j2} & \\cdots &  \\sum_{j = 1}^c \\mathbf{A}_{2j} \\mathbf{B}_{jc} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sum_{j = 1}^c \\mathbf{A}_{rj} \\mathbf{B}_{j1} &  \\sum_{j = 1}^c \\mathbf{A}_{rj} \\mathbf{B}_{j2} & \\cdots &  \\sum_{j = 1}^c \\mathbf{A}_{rj} \\mathbf{B}_{jc}\n\\end{pmatrix}  \n\\end{aligned}\n\\tag{8.2}\\]\nwhich can be said in words as “each block-element (the \\(ij\\)th element \\((\\mathbf{A} \\mathbf{B})_{ij}\\)) of the block-matrix product \\(\\mathbf{A} \\mathbf{B}\\) is the sum of the \\(i\\)th block-row of \\(\\mathbf{A}\\) and the \\(j\\)th block column of \\(\\mathbf{B}\\). Notice that if each block was a scalar rather than a block matrix, this would be the usual definition of matrix multiplication (compare Equation 8.3 above to Equation 2.2).\n\nExample 8.1 in class\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA solution to the example problem"
  },
  {
    "objectID": "02-block-matrices.html#the-column-row-matrix-product",
    "href": "02-block-matrices.html#the-column-row-matrix-product",
    "title": "8  Block Matrices",
    "section": "8.3 The column-row matrix product",
    "text": "8.3 The column-row matrix product\n\nTheorem 8.1 The matrix product \\(\\mathbf{A}\\mathbf{B}\\) of an \\(m \\times n\\) matrix \\(\\mathbf{A} = \\begin{pmatrix} \\mathbf{a}_1 & \\mathbf{a}_2 & \\cdots & \\mathbf{a}_n \\end{pmatrix}\\) with columns \\(\\{\\mathbf{a}_i\\}_{i=1}^n\\) and an \\(n \\times p\\) matrix \\(\\mathbf{B} = \\begin{pmatrix} \\mathbf{b}_1' \\\\ \\mathbf{b}_2' \\\\ \\vdots \\\\ \\mathbf{b}_n' \\end{pmatrix}\\) with rows \\(\\{\\mathbf{b}_i'\\}_{i=1}^n\\) can be written as the column-row expansion below:\n\\[\n\\begin{aligned}\n\\mathbf{A} \\mathbf{B} & =\n\\begin{pmatrix} \\mathbf{a}_1 & \\mathbf{a}_2 & \\cdots & \\mathbf{a}_n \\end{pmatrix} \\begin{pmatrix} \\mathbf{b}_1' \\\\ \\mathbf{b}_2' \\\\ \\vdots \\\\ \\mathbf{b}_n' \\end{pmatrix} \\\\\n& = \\mathbf{a}_1 \\mathbf{b}_1' + \\mathbf{a}_2 \\mathbf{b}_2' + \\cdots + \\mathbf{a}_n \\mathbf{b}_n'\n\\end{aligned}\n\\tag{8.3}\\]\n\nRecall: The notation \\(\\mathbf{b}_i'\\) has a transpose because a vector is defined in the vertical orientation (column vector). Therefore, to formally define a row vector, we take a vertical vector of the values in the row and take its transpose to turn the column vector into a row vector.\n\nExample 8.2 in class"
  },
  {
    "objectID": "02-block-matrices.html#special-block-matrices",
    "href": "02-block-matrices.html#special-block-matrices",
    "title": "8  Block Matrices",
    "section": "8.4 Special Block Matrices",
    "text": "8.4 Special Block Matrices\nThere are many different forms of block matrices. Two that deserve special mention here include block diagonal matrices and block triangular matrices.\n\nDefinition 8.1 The matrix \\(\\mathbf{A}\\) is said to be block diagonal if\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix}\n\\mathbf{A}_1 & \\mathbf{0} & \\mathbf{0} & \\cdots & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{A}_2 & \\mathbf{0} & \\cdots & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{0} & \\mathbf{A}_3 & \\cdots & \\mathbf{0} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\cdots & \\mathbf{A}_n \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\]\n\n\nDefinition 8.2 The matrix \\(\\mathbf{A}\\) is said to be block (upper) triangular if\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix}\n\\mathbf{A}_{11} & \\mathbf{A}_{12} & \\mathbf{A}_{13} & \\cdots & \\mathbf{A}_{1n} \\\\\n\\mathbf{0} & \\mathbf{A}_{22} & \\mathbf{A}_{23} & \\cdots & \\mathbf{A}_{2n} \\\\\n\\mathbf{0} & \\mathbf{0} & \\mathbf{A}_{33} & \\cdots & \\mathbf{A}_{3n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\cdots & \\mathbf{A}_{nn} \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\]\n\\(\\mathbf{A}\\) is block (lower) triangular if\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix}\n\\mathbf{A}_{11} & \\mathbf{0} & \\mathbf{0} & \\cdots & \\mathbf{0} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22} & \\mathbf{0} & \\cdots & \\mathbf{0} \\\\\n\\mathbf{A}_{31} & \\mathbf{A}_{32} & \\mathbf{A}_{33} & \\cdots & \\mathbf{0} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{A}_{m1} & \\mathbf{A}_{m2} & \\mathbf{A}_{m3} & \\cdots & \\mathbf{A}_{mn} \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\]\n\n\nExample 8.3 Assume that \\(\\mathbf{A}\\), which has the form\n\\[\n\\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\ \\mathbf{0} & \\mathbf{A}_{22} \\end{pmatrix},  \n\\]\nis an invertible matrix where \\(\\mathbf{A}_{11}\\) a \\(p \\times p\\) invertible matrix, \\(\\mathbf{A}_{12}\\) a \\(p \\times q\\) matrix, and \\(\\mathbf{A}_{22}\\) is a \\(q \\times q\\) invertible matrix. Solve for \\(\\mathbf{A}^{-1}\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe inverse to \\(\\mathbf{A}\\) is a matrix \\(\\mathbf{B} = \\begin{pmatrix} \\mathbf{B}_{11} & \\mathbf{B}_{12} \\\\ \\mathbf{B}_{21} & \\mathbf{B}_{22} \\end{pmatrix}\\) with blocks \\(\\mathbf{B}_{11}\\), \\(\\mathbf{B}_{12}\\), \\(\\mathbf{B}_{21}\\), and \\(\\mathbf{B}_{22}\\) of appropriate size to be conformable with the blocks \\(\\mathbf{A}_{11}\\), \\(\\mathbf{A}_{12}\\), and\\(\\mathbf{A}_{22}\\) that make up the matrix \\(\\mathbf{A}\\).\nFor \\(\\mathbf{B}\\) to be the inverse of \\(\\mathbf{A}\\), \\(\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A} = \\mathbf{I} = \\begin{pmatrix} \\mathbf{I} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{I} \\end{pmatrix}\\) where \\(\\mathbf{0}\\) is a matrix of zeros of the appropriate size. Writing this out in blocks gives\n\\[\n\\begin{aligned}\n\\mathbf{B}\\mathbf{A} & = \\begin{pmatrix} \\mathbf{B}_{11} & \\mathbf{B}_{12} \\\\ \\mathbf{B}_{21} & \\mathbf{B}_{22} \\end{pmatrix}  \\begin{pmatrix} \\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\ \\mathbf{0} & \\mathbf{A}_{22} \\end{pmatrix} \\\\\n& = \\begin{pmatrix} \\mathbf{B}_{11} \\mathbf{A}_{11} & \\mathbf{B}_{11} \\mathbf{A}_{12} + \\mathbf{B}_{12} \\mathbf{A}_{22} \\\\ \\mathbf{B}_{21} \\mathbf{A}_{11} & \\mathbf{B}_{21} \\mathbf{A}_{12} + \\mathbf{B}_{22} \\mathbf{A}_{22}\\end{pmatrix}\n\\end{aligned}\n\\]\nwhich gives that \\(\\mathbf{B}_{11} = \\mathbf{A}_{11}^{-1}\\) because \\(\\mathbf{B}_{11}^{-1}\\mathbf{A}_{11} = \\mathbf{I}\\) and \\(\\mathbf{A}_{11}\\) is invertible. The equation also give \\(\\mathbf{B}_{21} \\mathbf{A}_{11} = \\mathbf{0}\\) and because \\(\\mathbf{A}_{11}\\) is an invertible matrix, the homogeneous equation \\(\\mathbf{A}_{11}\\mathbf{b} = \\mathbf{0}\\) has only the trivial solution for each column \\(\\mathbf{b}\\) of the matrix \\(\\mathbf{B}_{21}\\) which implies that \\(\\mathbf{B}_{21} = \\mathbf{0}\\). Using these facts, we can rewrite the above equation as\n\\[\n\\begin{aligned}\n\\mathbf{B}\\mathbf{A} & = \\begin{pmatrix} \\mathbf{I} & \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} + \\mathbf{B}_{12} \\mathbf{A}_{22} \\\\ \\mathbf{0} & \\mathbf{B}_{22} \\mathbf{A}_{22}\\end{pmatrix}\n\\end{aligned}\n\\]\nBecause the lower right entry \\(\\mathbf{B}_{22} \\mathbf{A}_{22}\\) must equal \\(\\mathbf{I}\\), we have that \\(\\mathbf{B}_{22} = \\mathbf{A}_{22}^{-1}\\). Then, the equation becomes\n\\[\n\\begin{aligned}\n\\mathbf{B}\\mathbf{A} & = \\begin{pmatrix} \\mathbf{I} & \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} + \\mathbf{B}_{12} \\mathbf{A}_{22} \\\\ \\mathbf{0} & \\mathbf{I} \\end{pmatrix}\n\\end{aligned}\n\\]\nThe final component is the upper right block. Because we are finding the inverse, we know that \\(\\mathbf{0} = \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} + \\mathbf{B}_{12} \\mathbf{A}_{22}\\). Subtracting \\(\\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}\\) from both sides of the equation gives\n\\[\n\\begin{aligned}\n\\mathbf{B}_{12} \\mathbf{A}_{22} & = - \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\\\\n\\mathbf{B}_{12} \\mathbf{A}_{22} \\mathbf{A}_{22}^{-1} & = - \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1}\\\\\n\\mathbf{B}_{12} \\mathbf{I} & = - \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1}\\\\\n\\mathbf{B}_{12} & = - \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1}\n\\end{aligned}\n\\]\nThus, \\(\\mathbf{A}^{-1} = \\begin{pmatrix} \\mathbf{A}_{11}^{-1} & - \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\\\ \\mathbf{0} & \\mathbf{A}_{22}^{-1} \\end{pmatrix}\\)"
  },
  {
    "objectID": "08-matrix-inverses.html",
    "href": "08-matrix-inverses.html",
    "title": "9  Matrix Inverses",
    "section": "",
    "text": "3 Blue 1 Brown – Inverse Matrices, column space, and null space\nFor scalars, the multiplicative identity is \\[\na \\frac{1}{a} = a a^{-1} = a^{-1} a = 1\n\\] where \\(a^{-1}\\) is the inverse of \\(a\\).\nIn R, an identity matrix is easy to construct. An \\(n \\times n\\) identity matrix can be constructed using the diag() function\nA consequence of the above theorem is that a \\(2 \\times 2\\) matrix is invertible only if its determinant is nonzero."
  },
  {
    "objectID": "08-matrix-inverses.html#elementary-matrices",
    "href": "08-matrix-inverses.html#elementary-matrices",
    "title": "9  Matrix Inverses",
    "section": "9.1 Elementary matrices",
    "text": "9.1 Elementary matrices\n\nElementary matrices are matrices that perform basic row operations (i.e., we can write the reduced row echelon algorithm as a produce of elementary matrices).\n\nRecall the elementary row operations:\n\nswaps: swapping two rows.\nsums: replacing a row by the sum itself and a multiple of another row.\nscalar multiplication: replacing a row by a scalar multiple times itself.\n\n\nExample 9.4 Consider the \\(3 \\times 3\\) matrix\n\nA <- matrix(c(4, 5, 9, -2, -4, 1, 4, 6, -2), 3, 3)\n\n\\(\\mathbf{A} = \\begin{pmatrix} 4 & -2 & 4 \\\\ 5 & -4 & 6 \\\\ 9 & 1 & -2 \\end{pmatrix}\\)\n\nWhat is the elementary matrix (let’s call it \\(\\mathbf{E}_1\\) that swaps the first and second rows of \\(\\mathbf{A}\\)?\n\n\nE_1 <- matrix(c(0, 1, 0, 1, 0, 0,  0, 0, 1), 3, 3)\n\n\\(\\mathbf{E}_1 = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\)\n\nA\n\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]    5   -4    6\n[3,]    9    1   -2\n\n## left multiple A by E_1\nE_1 %*% A\n\n     [,1] [,2] [,3]\n[1,]    5   -4    6\n[2,]    4   -2    4\n[3,]    9    1   -2\n\n\nThus, the matrix \\(\\mathbf{E}_1 = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) is the matrix that swaps the first and second row.\n\nWhat is the elementary matrix (let’s call it \\(\\mathbf{E}_2\\) that adds two times the first of \\(\\mathbf{A}\\) to the third row of \\(\\mathbf{A}\\)?\n\n\nE_2 <- matrix(c(1, 0, 2, 0, 1, 0, 0, 0, 1), 3, 3)\n\n\\(\\mathbf{E}_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 1 \\end{pmatrix}\\)\n\nA\n\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]    5   -4    6\n[3,]    9    1   -2\n\n## left multiple A by E_2\nE_2 %*% A\n\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]    5   -4    6\n[3,]   17   -3    6\n\n\nThus, the matrix \\(\\mathbf{E}_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 1 \\end{pmatrix}\\) is the matrix that adds two times the first of \\(\\mathbf{A}\\) to the third row of \\(\\mathbf{A}\\)\n\nWhat is the elementary matrix (let’s call it \\(\\mathbf{E}_3\\) that mutliples the second row of \\(\\mathbf{A}\\) by 3?\n\n\nE_3 <- matrix(c(1, 0, 0, 0, 3, 0, 0, 0, 1), 3, 3)\n\n\\(\\mathbf{E}_3 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\)\n\nA\n\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]    5   -4    6\n[3,]    9    1   -2\n\n## left multiple A by E_3\nE_3 %*% A\n\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]   15  -12   18\n[3,]    9    1   -2\n\n\nThus, the matrix \\(\\mathbf{E}_3 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) is the matrix that multiples the second row of \\(\\mathbf{A}\\) by 3.\n\n\nQuestion: Do you see any patterns with how the example elementary matrices look?\n\n\\[\n\\begin{aligned}\n\\mathbf{E_1} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} && \\mathbf{E_2} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 1 \\end{pmatrix} && \\mathbf{E_3} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n\\end{aligned}\n\\]\n\nThe elementary matrices look like the identity matrix \\(\\mathbf{I}\\) with an elementary row operation applied to \\(\\mathbf{I}\\). In fact, this leads us to this general fact:\n\nFact: If an elementary row matrix is applied to the \\(m \\times n\\) matrix \\(\\mathbf{A}\\), the result of this elementary row operation applied to \\(\\mathbf{A}\\) can be written as \\(\\mathbf{E} \\mathbf{A}\\) where \\(\\mathbf{E}\\) is the \\(m \\times m\\) identity matrix \\(\\mathbf{I}\\) with the respective elementary row operation applied to \\(\\mathbf{I}\\).\nFact: Each elementary matrix \\(\\mathbf{E}\\) is invertible\n\nExample 9.5 In class\n\nThe next theorem is quite important as the result gives an algorithm for calculating the inverse of a \\(n \\times n\\) matrix \\(\\mathbf{A}\\) which also makes it possible to solve matrix equations \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\)\n\nTheorem 9.4 If an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is invertible, then \\(\\mathbf{A}\\) is row-equivalent to \\(\\mathbf{I}\\) (\\(\\mathbf{A} \\sim \\mathbf{I}\\); row-equivalent means \\(\\mathbf{A}\\) can be reduced to \\(\\mathbf{I}\\) using elementary row operations). The row-equivalency implies that there is a series of elementary row operations (e.g., elementary matrices \\(\\mathbf{E}_1, \\ldots, \\mathbf{E}_k\\)) that converts \\(\\mathbf{A}\\) to \\(\\mathbf{I}\\). In addition, the application of these row matrices to \\(\\mathbf{I}\\) transforms \\(\\mathbf{I}\\) to the matrix inverse \\(\\mathbf{A}^{-1}\\).\n\n\nProof: in class"
  },
  {
    "objectID": "08-matrix-inverses.html#finding-the-inverse-of-mathbfa",
    "href": "08-matrix-inverses.html#finding-the-inverse-of-mathbfa",
    "title": "9  Matrix Inverses",
    "section": "9.2 Finding the inverse of \\(\\mathbf{A}\\)",
    "text": "9.2 Finding the inverse of \\(\\mathbf{A}\\)\nThe previous theorem states that for a \\(n \\times n\\) invertible matrix \\(\\mathbf{A}\\), the elementary row operations that covert \\(\\mathbf{A}\\) to \\(\\mathbf{I}\\) also convert \\(\\mathbf{I}\\) to \\(\\mathbf{A}^{-1}\\). This suggests an algorithm for finding the inverse \\(\\mathbf{A}^{-1}\\) of \\(\\mathbf{A}\\):\nCreate the augmented matrix \\(\\begin{pmatrix} \\mathbf{A} & \\mathbf{I} \\end{pmatrix}\\) and row reduce the augmented matrix. If the row-reduced augmented matrix is of the form \\(\\begin{pmatrix} \\mathbf{I} & \\mathbf{A}^{-1} \\end{pmatrix}\\) then \\(\\mathbf{A}^{-1}\\) is the inverse of \\(\\mathbf{A}\\). If the leading matrix in the augmented matrix is not the identity matrix \\(\\mathbf{I}\\), then \\(\\mathbf{A}\\) is not row equivalent to \\(\\mathbf{I}\\) and is therefore not invertible.\n\nExample 9.6 Let \\(\\mathbf{A} = \\begin{pmatrix} -3 & -3 & -4 \\\\ -4 & 2 & -4 \\\\ 4 & -4 & 4 \\end{pmatrix}\\). Does \\(\\mathbf{A}\\) have an inverse, and if so, what is it?\nUsing R"
  },
  {
    "objectID": "08-matrix-inverses.html#the-invertible-matrix-theorem",
    "href": "08-matrix-inverses.html#the-invertible-matrix-theorem",
    "title": "9  Matrix Inverses",
    "section": "9.3 The Invertible Matrix Theorem",
    "text": "9.3 The Invertible Matrix Theorem\n\nTheorem 9.5 (The Invertible Matrix Theorem) Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix. Then the following statements are equivalent (i.e., they are all either simultaneously true or false).\n\n\\(\\mathbf{A}\\) is an invertible matrix.\n\\(\\mathbf{A}\\) is row equivalent to the \\(n \\times n\\) identity matrix \\(\\mathbf{I}\\) (\\(\\mathbf{A} \\sim \\mathbf{I}\\)).\n\\(\\mathbf{A}\\) has \\(n\\) pivot columns.\nThe homogeneous matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\).\nThe columns of \\(\\mathbf{A}\\) are linearly independent.\nThe linear transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) given by the matrix transformation \\(\\mathbf{x} \\rightarrow \\mathbf{A}\\mathbf{x}\\) is one-to-one.\nThe inhomogeneous matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a unique solution for all \\(\\mathbf{b} \\in \\mathcal{R}^n\\).\nThe columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^n\\).\nThe linear transformation \\(\\mathbf{x} \\rightarrow \\mathbf{A} \\mathbf{x}\\) maps \\(\\mathcal{R}^n\\) onto \\(\\mathcal{R}^n\\).\nThere is an \\(n \\times n\\) matrix \\(\\mathbf{C}\\) such that \\(\\mathbf{C}\\mathbf{A} = \\mathbf{I}\\).\nThere is an \\(n \\times n\\) matrix \\(\\mathbf{D}\\) such that \\(\\mathbf{A}\\mathbf{D} = \\mathbf{I}\\).\n\\(\\mathbf{A}'\\) is an invertible matrix.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn class\n\n\n\nA result of the invertible matrix theorem is that if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are \\(n \\times n\\) matrices with \\(\\mathbf{A} \\mathbf{B} = \\mathbf{I}\\) then \\(\\mathbf{A} = \\mathbf{B}^{-1}\\) and \\(\\mathbf{B} = \\mathbf{A}^{-1}\\)."
  },
  {
    "objectID": "08-matrix-inverses.html#invertible-linear-transformations",
    "href": "08-matrix-inverses.html#invertible-linear-transformations",
    "title": "9  Matrix Inverses",
    "section": "9.4 Invertible Linear Transformations",
    "text": "9.4 Invertible Linear Transformations\n\nDefinition 9.3 A linear transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) is said to be invertible if there exists a transformation \\(S:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) such that\n\\[\n\\begin{aligned}\nS(T(\\mathbf{x})) = \\mathbf{x} && \\mbox{for all } \\mathbf{x} \\in \\mathcal{R}^n\nT(S(\\mathbf{x})) = \\mathbf{x} && \\mbox{for all } \\mathbf{x} \\in \\mathcal{R}^n \\\\\n\\end{aligned}\n\\]\n\n\nDraw figure in class\n\n\nTheorem 9.6 Let \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) be a linear transformation and let \\(\\mathbf{A}\\) be the matrix representing the transformation \\(T\\). Then the transformation \\(T\\) is invertible if and only if the matrix \\(\\mathbf{A}\\) is invertible. Therefore, the matrix that represents \\(S:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\), the inverse transformation of \\(T\\), is unique and is represented by the matrix \\(\\mathbf{A}^{-1}\\)."
  },
  {
    "objectID": "09-matrix-factorizations.html",
    "href": "09-matrix-factorizations.html",
    "title": "10  Matrix Factorizations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dasc2594)\nlibrary(mvnfast)\nlibrary(MASS)\nIn scalar mathematics, a factorization is an expression that writes a scalar \\(a\\) as a product of two or more scalars. For example, the scalar 2 has a square-root factorization of \\(2 =\\sqrt{2} * \\sqrt{2}\\) and 15 has a prime factorization of \\(15 = 3 * 5\\). A matrix factorization is a similar concept where a matrix \\(\\mathbf{A}\\) can be represented by a product or two or more matrices (e.g., \\(\\mathbf{A} = \\mathbf{B} \\mathbf{C}\\)). In data science, matrix factorizations are fundamental to working with data."
  },
  {
    "objectID": "09-matrix-factorizations.html#the-lu-factorization",
    "href": "09-matrix-factorizations.html#the-lu-factorization",
    "title": "10  Matrix Factorizations",
    "section": "10.1 The LU factorization",
    "text": "10.1 The LU factorization\nFirst, we define lower and upper triangular matrices.\n\nDefinition 10.1 The matrix \\(\\mathbf{A}\\) is said to be lower triangular if \\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\\na_{21} & a_{22} & 0 & \\cdots & 0 \\\\\na_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn} \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\]\nSimilarly, the matrix \\(\\mathbf{A}\\) is said to be upper triangular if \\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\n0 & a_{22} & a_{23} & \\cdots & a_{2n} \\\\\n0 & 0 & a_{33} & \\cdots & a_{3n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & a_{nn} \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\]\n\nThe LU factorization of a matrix \\(\\mathbf{A}\\) reduces the matrix \\(\\mathbf{A}\\) into two components. The first component \\(\\mathbf{L}\\) is a lower-triangular matrix and the second component \\(\\mathbf{U}\\) is an upper triangular matrix.\nUsing the LU factorization, the matrix factorization \\(\\mathbf{A} = \\mathbf{L} \\mathbf{U}\\) can be used in the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{L} \\mathbf{U}\\mathbf{x} = \\mathbf{b}\\) by first solving the sub-equation \\(\\mathbf{L} \\mathbf{y} = \\mathbf{b}\\) and then solving the second sub-equation \\(\\mathbf{U} \\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\). Thus, the matrix factorization applied to the matrix equation gives the pair of equations\n\\[\n\\begin{aligned}\n\\mathbf{L} \\mathbf{y} & = \\mathbf{b} \\\\\n\\mathbf{U} \\mathbf{x} & = \\mathbf{y}\n\\end{aligned}\n\\tag{10.1}\\]\nAt first glance, this seems like we are trading the challenge of solving one system of equations \\(\\mathbf{A}\\mathbf{x}\\) Equation 5.1 for the two equations in Equation 10.1. However, the computational benefits arise due to the fact that \\(\\mathbf{L}\\) and \\(\\mathbf{U}\\) are triangular matrices and solving matrix equations with triangular matrices is much faster.\n\nExample 10.1 Let \\(\\mathbf{A} = \\begin{pmatrix} 1 & 0 & 2 & -2 \\\\ -2 & -2 & -4 & 1 \\\\ -1 & -4 & -8 & 5 \\\\ -2 & -6 & -4 & 4 \\end{pmatrix}\\) which has the LU decomposition\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} 1 & 0 & 2 & -2 \\\\ -2 & -2 & -4 & 1 \\\\ -1 & -4 & -8 & 5 \\\\ -2 & -6 & -4 & 4 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ -2 & -1 & 0 & 0 \\\\ -1 & -2 & -3 & 0 \\\\ -2 & -3 & 0 & -3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 2 & -2 \\\\ 0 & 2 & 0 & 3 \\\\ 0 & 0 & 2 & -3 \\\\ 0 & 0 & 0 & -3 \\end{pmatrix}\n\\end{aligned}\n\\] and consider the system of equations defined by the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) where \\(\\mathbf{b} = \\begin{pmatrix} -5 \\\\ -7 \\\\ -2 \\\\ -14 \\end{pmatrix}\\).\n\nsolve \\(\\mathbf{L} \\mathbf{y} = \\mathbf{b}\\) using an augmented matrix and RREF.\nsolve \\(\\mathbf{U} \\mathbf{x} = \\mathbf{y}\\) using an augmented matrix and RREF.\ncompare to the solution \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) using an augmented matrix and RREF.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nFor the example, we will show how to solve a system of equations using the LU decomposition for the equation defined above.\n\nSolve \\(\\mathbf{L} \\mathbf{y} = \\mathbf{b}\\) using augmented matrix\n\n\\[\n\\begin{aligned}\n& & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ -2 & -1 & 0 & 0 & -7 \\\\ -1 & -2 & -3 & 0 & -2 \\\\ -2 & -3 & 0 & -3 & -14 \\end{pmatrix} & \\stackrel{r_2 \\leftarrow -\\frac{1}{2} r_2 - r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1/2 & 0 & 0 & 17/2 \\\\ -1 & -2 & -3 & 0 & -2 \\\\ -2 & -3 & 0 & -3 & -14 \\end{pmatrix} \\\\\n& \\stackrel{r_3 \\leftarrow - r_3 - r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1/2 & 0 & 0 & 17/2 \\\\ 0 & 2 & 3 & 0 & 7 \\\\ -2 & -3 & 0 & -3 & -14 \\end{pmatrix} & \\stackrel{r_4 \\leftarrow - \\frac{1}{2} r_4 - r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1/2 & 0 & 0 & 17/2 \\\\ 0 & 2 & 3 & 0 & 7 \\\\ 0 & 3/2 & 0 & 3/2 & 12 \\end{pmatrix} \\\\\n& \\stackrel{r_2 \\leftarrow 2 r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1 & 0 & 0 & 17 \\\\ 0 & 2 & 3 & 0 & 7 \\\\ 0 & 3/2 & 0 & 3/2 & 12 \\end{pmatrix} & \\stackrel{r_3 \\leftarrow \\frac{1}{2} r_3 - r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1 & 0 & 0 & 17 \\\\ 0 & 0 & 3/2 & 0 & -27/2 \\\\ 0 & 3/2 & 0 & 3/2 & 12 \\end{pmatrix} \\\\\n& \\stackrel{r_4 \\leftarrow \\frac{2}{3} r_4 - r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1 & 0 & 0 & 17 \\\\ 0 & 0 & 3/2 & 0 & -27/2 \\\\ 0 & 0 & 0 & 1 & -9 \\end{pmatrix} & \\stackrel{r_3 \\leftarrow \\frac{2}{3} r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1 & 0 & 0 & 17 \\\\ 0 & 0 & 1 & 0 & -9 \\\\ 0 & 0 & 0 & 1 & -9 \\end{pmatrix}\n\\end{aligned}\n\\]\n\n\n\n\nsolve \\(\\mathbf{U} \\mathbf{x} = \\mathbf{y}\\) using an augmented matrix and RREF.\n\n\\[\n\\begin{aligned}\n& & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 2 & 0 & 3 & 17 \\\\ 0 & 0 & 2 & -3 & -9 \\\\ 0 & 0 & 0 & -3 & -9 \\end{pmatrix} & \\stackrel{r_2 \\leftarrow \\frac{1}{2} r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 2 & -3 & -9 \\\\ 0 & 0 & 0 & -3 & -9 \\end{pmatrix} \\\\\n& \\stackrel{r_3 \\leftarrow \\frac{1}{2} r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & -3 & -9 \\end{pmatrix} & \\stackrel{r_4 \\leftarrow - \\frac{1}{3} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} \\\\\n& \\stackrel{r_1 \\leftarrow r_1 -2  r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 1 & 4 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} & \\stackrel{r_1 \\leftarrow r_1 - 2 r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} \\\\\n& \\stackrel{r_2 \\leftarrow r_2 - \\frac{3}{2} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 4 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} &\n\\stackrel{r_3 \\leftarrow r_3 + \\frac{3}{2} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 4 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix}\n\\end{aligned}\n\\]\n\ncompare to the solution \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) using an augmented matrix and RREF.\n\n\n\n\n\\[\n\\begin{aligned}\n& & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ -2 & -2 & -4 & 1 & -7 \\\\ -1 & -4 & -8 & 5 & -2 \\\\ -2 & -6 & -4 & 4 & -14 \\end{pmatrix} & \\stackrel{r_2 \\leftarrow r_2 + 2 r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & -2 & 0 & -3 & -17 \\\\ -1 & -4 & -8 & 5 & -2 \\\\ -2 & -6 & -4 & 4 & -14 \\end{pmatrix} \\\\\n& \\stackrel{r_3 \\leftarrow r_3 + r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & -2 & 0 & -3 & -17 \\\\ 0 & -4 & -6 & 3 & -7 \\\\ -2 & -6 & -4 & 4 & -14 \\end{pmatrix} & \\stackrel{r_4 \\leftarrow r_4 + 2 r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & -2 & 0 & -3 & -17 \\\\ 0 & -4 & -6 & 3 & -7 \\\\ 0 & -6 & 0 & 0 & -24 \\end{pmatrix} \\\\\n& \\stackrel{r_2 \\leftarrow - \\frac{1}{2} r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & -4 & -6 & 3 & -7 \\\\ 0 & -6 & 0 & 0 & -24 \\end{pmatrix} & \\stackrel{r_3 \\leftarrow r_3 + 4 r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & -6 & 9 & 27 \\\\ 0 & -6 & 0 & 0 & -24 \\end{pmatrix} \\\\\n& \\stackrel{r_4 \\leftarrow r_4 + 6 r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & -6 & 9 & 27 \\\\ 0 & 0 & 0 & 9 & 27 \\end{pmatrix} &\n\\stackrel{r_3 \\leftarrow - \\frac{1}{6} r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 9 & 27 \\end{pmatrix}\\\\\n& \\stackrel{r_4 \\leftarrow \\frac{1}{9} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} & \\stackrel{r_1 \\leftarrow r_1 - 2 r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 1 & 4 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} \\\\\n& \\stackrel{r_1 \\leftarrow r_1 - r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} &\n\\stackrel{r_2 \\leftarrow r_2 - \\frac{3}{2} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 4 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} \\\\\n& \\stackrel{r_3 \\leftarrow r_3 + \\frac{3}{2} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 4 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix}  \n\\end{aligned}\n\\]\nWhile it might not be completely obvious, once one has calculated the LU decomposition, it can often be much faster to solve systems of equations with the LU decomposition.\n\n\n\n\nin lab: Solve some large systems of equations by brute force which shows how the LU decomposition is faster.\n\n\n10.1.1 Geometric interpretation of the LU factorization\n\nDraw image in class – composition of transformations \\(T_A(\\cdot) = T_L(T_U(\\cdot))\\)"
  },
  {
    "objectID": "09-matrix-factorizations.html#obtaining-the-lu-factorization",
    "href": "09-matrix-factorizations.html#obtaining-the-lu-factorization",
    "title": "10  Matrix Factorizations",
    "section": "10.2 Obtaining the LU factorization",
    "text": "10.2 Obtaining the LU factorization\nNotice that the upper-triangular matrix \\(\\mathbf{U}\\) is in echelon form. Congratulations! you know how to construct a matrix \\(\\mathbf{U}\\) by reducing the matrix \\(\\mathbf{A}\\) to an echelon form \\(\\mathbf{U}\\) using elementary matrices \\(\\mathbf{E}_1, \\ldots \\mathbf{E}_k\\). Now, we only need to find the lower triangular matrix \\(\\mathbf{L}\\).\nCombining the LU factorization and the fact that we can find an upper triangular matrix \\(\\mathbf{U}\\) using elementary row matrices, we have\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\mathbf{L} \\mathbf{U} \\\\\n\\mathbf{E}_k \\cdots \\mathbf{E}_1 \\mathbf{A} & = \\mathbf{U}.\n\\end{aligned}\n\\tag{10.2}\\] We also know that each of the elementary row matrices \\(\\mathbf{E}_j\\) are invertible (you can always re-swap rows, subtract instead of add rows, etc.) which says that each inverse \\(\\mathbf{E}_j^{-1}\\) exists. Thus, the product \\(\\mathbf{E}_k \\cdots \\mathbf{E}_1\\) must have an inverse which is \\[\n\\begin{aligned}\n(\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1} & = \\mathbf{E}_1^{-1} \\cdots \\mathbf{E}_k^{-1}.\n\\end{aligned}\n\\] Plugging this inverse into Equation 10.2 gives (left multiplying by \\((\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\) on both sides) \\[\n\\begin{aligned}\n(\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1} (\\mathbf{E}_k \\cdots \\mathbf{E}_1) \\mathbf{A} & = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\mathbf{U} \\\\\n\\mathbf{A} & = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\mathbf{U} \\\\\n& = \\mathbf{L} \\mathbf{U}\n\\end{aligned}\n\\] where \\(\\mathbf{L} = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\)\nAlgorithm for finding the LU decomposition\nGiven the matrix \\(\\mathbf{A}\\)\n\nFind elementary matrices \\(\\mathbf{E}_1, \\ldots, \\mathbf{E}_k\\) such that \\(\\mathbf{E}_k \\cdots \\mathbf{E}_1 \\mathbf{A}\\) is in row echelon form (if this is possible, otherwise an LU factorization does not exist). Call this matrix \\(\\mathbf{U}\\), the upper triangular component of the LU factorization.\nThe, the lower triangular \\(\\mathbf{L} = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\).\n\nNotice that the algorithm does not say to find a specific matrix \\(\\mathbf{U}\\). In general, any row echelon form matrix \\(\\mathbf{U}\\) will work."
  },
  {
    "objectID": "09-matrix-factorizations.html#the-cholesky-factor",
    "href": "09-matrix-factorizations.html#the-cholesky-factor",
    "title": "10  Matrix Factorizations",
    "section": "10.3 The Cholesky factor",
    "text": "10.3 The Cholesky factor\nA Cholesky decomposition is special type of LU decomposition. A Cholesky decomposition is an LU decomposition on a symmetric, positive-definite square matrix.\n\nDefinition 10.2 If a matrix \\(\\mathbf{A}\\) meets the following two conditions, the matrix \\(\\mathbf{A}\\) is said to be symmetric, positive-definite.\n\nA matrix \\(\\mathbf{A}\\) is said to by symmetric if \\(\\mathbf{A} = \\mathbf{A}'\\)\n\n\nA \\(n \\times n\\) matrix is said to be positive definite if for all \\(\\mathbf{x} \\in \\mathcal{R}^n\\), the quadratic form \\(\\mathbf{x}' \\mathbf{A }\\mathbf{x} \\geq 0\\)\n\n\nNote: the condition of positive definiteness is actually impossible to check. Can you show this is true for all vectors? Luckily, a \\(n \\times n\\) symmetric matrix is positive definite if and only if the matrix \\(\\mathbf{A}\\) is invertible (which we know about by the invertible matrix theorem Theorem 9.5).\n\nDefinition 10.3 Let \\(\\mathbf{A}\\) be a symmetric, positive definite matrix (by this, \\(\\mathbf{A}\\) is a \\(n \\times n\\) square matrix). Then \\[\n\\begin{aligned}\n\\mathbf{A} = \\mathbf{L} \\mathbf{L}'\n\\end{aligned}\n\\] is the Cholesky decomposition of \\(\\mathbf{A}\\) if \\(\\mathbf{L}\\) is a lower-triangular matrix. Also, the lower triangular Cholesky matrix \\(\\mathbf{L}\\) is unique.\n\nWhat makes the Cholesky factor special?\n\nThe decomposition \\(\\mathbf{A} = \\mathbf{L} \\mathbf{U}\\) has the property that \\(\\mathbf{U} = \\mathbf{L}'\\) so that the computer only has to store one of the matrix components (reduce memory demands). As about half of the elements of \\(\\mathbf{L}\\) are 0, matrix multiplication is much less computationally demanding as about half of the flops are not required to be evaluated (x * 0 = 0).\nThe Cholesky factor is unique. There is only one Cholesky factor for each symmetric positive definite matrix.\nThe Cholesky has properties related to multivariate normal distributions.\n\nLet \\(\\mathbf{y} \\sim \\operatorname{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})\\), and \\(\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}'\\). Then, if \\(\\mathbf{z} \\sim \\operatorname{N}(\\mathbf{0}, \\mathbf{I})\\), then \\(\\mathbf{L} \\mathbf{z} \\sim \\operatorname{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})\\). We say the \\(\\mathbf{y}\\) and \\(\\mathbf{L}\\mathbf{z}\\) are equal in distribution.\n\n# simulate N 2-dimensional random normal vectors \nN <- 5000\nmu    <- rep(0, 2)\nSigma <- matrix(c(2, 1.5, 1.5, 2), 2, 2)\ny <- rmvn(N, mu, Sigma)\n\n# calculate the Cholesky factor\nL <- t(chol(Sigma))    # R calculates the upper (right) Cholesky factor by default\nz <- rmvn(N, mu, diag(2))\nLz <- t(L %*% t(z))    # pay attention to the dimensions of L and z here...\n\ndata.frame(\n    observation = 1:N,\n    x1          = c(y[, 1], z[, 1], Lz[, 1]),\n    x2          = c(y[, 2], z[, 2], Lz[, 2]),\n    variable    = factor(rep(c(\"y\", \"z\", \"Lz\"), each = N), levels = c(\"y\", \"z\", \"Lz\"))\n) %>%\n    ggplot(aes(x = x1, y = x2, color = variable)) +\n    geom_point(alpha = 0.1) +\n    geom_density2d() +\n    facet_wrap(~ variable)"
  },
  {
    "objectID": "10-subspaces-of-Rn.html",
    "href": "10-subspaces-of-Rn.html",
    "title": "11  Subspaces of \\(\\mathcal{R}^n\\)",
    "section": "",
    "text": "3 Blue 1 Brown – Linear combinations, span, and basis vectors\n3 Blue 1 Brown – Inverse Matrices, column space, and null space\nFirst, let’s recall the definition of a subset. A set \\(A\\) is a subset of a set \\(B\\) if all elements of \\(A\\) are also members of \\(B\\). For example, the integers \\(\\mathcal{Z}\\) are a subset of the real numbers \\(\\mathbf{R}\\) (\\(\\mathcal{Z} \\subset \\mathcal{R}\\)) and the real numbers are a subset of the complex numbers \\(\\mathcal{C}\\) (\\(\\mathcal{R} \\subset \\mathcal{C}\\)).\nSubspaces are a generalization of the idea of subsets that are useful for understanding vector spaces."
  },
  {
    "objectID": "10-subspaces-of-Rn.html#special-subspaces-column-space-and-null-space",
    "href": "10-subspaces-of-Rn.html#special-subspaces-column-space-and-null-space",
    "title": "11  Subspaces of \\(\\mathcal{R}^n\\)",
    "section": "11.1 Special subspaces: column space and null space",
    "text": "11.1 Special subspaces: column space and null space\n\nDefinition 11.2 The column space, denoted \\(\\operatorname{col}(\\mathbf{A})\\), of a \\(m \\times n\\) matrix \\(\\mathbf{A}\\) which has columns \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_n \\in \\mathcal{R}^m\\) is the set of vectors that are linear combinations of the columns of \\(\\mathbf{A}\\) which is equivalent to the span\\(\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}\\).\n\n\n\nExample 11.3 \n\nGiven the matrix \\(\\mathbf{A} = \\begin{pmatrix} 1 & -8 & -9 \\\\ -3 & 3 & -5 \\\\ 1 & 0 & 6 \\\\ -2 & -1 & -1 \\end{pmatrix}\\) with columns \\(\\mathbf{a}_1\\), \\(\\mathbf{a}_2\\), and \\(\\mathbf{a}_3\\), what is the column space of \\(\\mathbf{A}\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGiven the matrix \\(\\mathbf{A} = \\begin{pmatrix} 1 & -8 & -9 \\\\ -3 & 3 & -5 \\\\ 1 & 0 & 6 \\\\ -2 & -1 & -1 \\end{pmatrix}\\) with columns \\(\\mathbf{a}_1\\), \\(\\mathbf{a}_2\\), and \\(\\mathbf{a}_3\\), the column space of \\(\\mathbf{A}\\) is the set of vectors \\(\\mathbf{b}\\) such that \\[\n\\begin{aligned}\n\\mathbf{b} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a}_2 + x_3 \\mathbf{a}_3.\n\\end{aligned}\n\\] Thus, the column space of \\(\\mathbf{A}\\) is the span of the vectors that makes up the columns of \\(\\mathbf{A}\\).\n\n\n\n\nDefinition 11.3 The null space, denoted \\(\\operatorname{null}(\\mathbf{A})\\), of a matrix \\(\\mathbf{A}\\) is the set of all solutions to the homogeneous matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\).\n\nWhile the idea of a null space might at first glance seem unclear, the null space is the set of all vectors which the matrix transformation defined by \\(\\mathbf{A}\\) maps to \\(\\mathbf{0}\\). In other words, the null space of \\(\\mathbf{A}\\) is the set of vectors \\(\\{ \\mathbf{x} : \\mathbf{A} \\mathbf{x} = \\mathbf{0} \\}\\).\n\n\nExample 11.4 \n\nGiven the matrix \\(\\mathbf{A} = \\begin{pmatrix} -3 & -3 & -4 & -5 & -2 \\\\ -4 & 2 & -4 & 5 & 3 \\\\ 4 & -4 & 4 & -3 & 5 \\end{pmatrix}\\) with columns \\(\\mathbf{a}_1\\), \\(\\mathbf{a}_2\\), \\(\\mathbf{a}_3\\), and \\(\\mathbf{a}_4\\), find vectors that span the null space of \\(\\mathbf{A}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGiven the matrix \\(\\mathbf{A} = \\begin{pmatrix} -3 & -3 & -4 & -5 & -2 \\\\ -4 & 2 & -4 & 5 & 3 \\\\ 4 & -4 & 4 & -3 & 5 \\end{pmatrix}\\) with columns \\(\\mathbf{a}_1\\), \\(\\mathbf{a}_2\\), \\(\\mathbf{a}_3\\), and \\(\\mathbf{a}_4\\), the null space of \\(\\mathbf{A}\\) is the solution set of the homogeneous system of equations \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\). Using an augmented matrix and transforming into reduced row echelon form gives the RREF form \\[\n\\begin{aligned}\n\\begin{pmatrix} -3 & -3 & -4 & -5 & -2 & 0 \\\\ -4 & 2 & -4 & 5 & 3 & 0 \\\\ 4 & -4 & 4 & -3 & 5 & 0 \\end{pmatrix} \\sim \\begin{pmatrix} 1 & 0 & 0 & -15 & -25 & 0 \\\\ 0 & 1 & 0 & -1 & -4 & 0 \\\\ 0 & 0 & 1 & 53/4 & 89/4 & 0 \\end{pmatrix}\n\\end{aligned}\n\\] which tells us that the solution to the system of equations is \\[\n\\begin{aligned}\nx_1 - 15 x_4 -25 x_5 & = 0 \\\\\nx_2 - x_4 - 4 x_5 & = 0 \\\\\nx_3 + \\frac{53}{4} x_4  + \\frac{89}{4} x_5 & = 0\\\\\nx_4 & = x_4 \\\\\nx_5 & = x_5\n\\end{aligned}\n\\] Writing this solution as a vector times \\(x_4\\) and a vector times \\(x_5\\) gives the vectors \\(\\begin{pmatrix} 15 \\\\ 1 \\\\ -\\frac{53}{4} \\\\ 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 25 \\\\ 4 \\\\ -\\frac{89}{4} \\\\ 0 \\\\ 1 \\end{pmatrix}\\).\nIn R, this is shown by starting with the matrix A\n\nA <- matrix(c(-3, -4, 4, -3, 2, -4, -4, -4, 4, -5, 5, -3, -2, 3, 5), 3, 5)\nA\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   -3   -3   -4   -5   -2\n[2,]   -4    2   -4    5    3\n[3,]    4   -4    4   -3    5\n\n\nLooking at the reduced row echelon form of \\(\\mathbf{A}\\) gives\n\nrref(A)\n\n     [,1] [,2] [,3]   [,4]   [,5]\n[1,]    1    0    0 -15.00 -25.00\n[2,]    0    1    0  -1.00  -4.00\n[3,]    0    0    1  13.25  22.25\n\n\nwhere the columns of interest are the non-pivot columns. For this matrix \\(\\mathbf{A}\\), the fourth and fifth columns of \\(\\mathbf{A}\\) are the non-pivot columns. The fourth column or the RREF form corresponds to the variable \\(x_4\\) and the fifth column corresponds to the variable \\(x_5\\). Thus, you can extract the vectors that form the null space from these fourth and fifth columns of the RREF form of \\(\\mathbf{A}\\) like so\n\n# the nullspace vector corresponding to x4 = 1 and x5 = 0\nc(-rref(A)[, 4], 1, 0)\n\n[1]  15.00   1.00 -13.25   1.00   0.00\n\n# the nullspace vector corresponding to x4 = 0 and x5 = 1\nc(-rref(A)[, 5], 0, 1)\n\n[1]  25.00   4.00 -22.25   0.00   1.00\n\n\nWe check that these vectors are in null(\\(\\mathbf{A}\\)) by using matrix multiplication and verifying that these are zero (at least up to numeric overflow/underflow)\n\nA %*% c(-rref(A)[, 4], 1, 0)\n\n              [,1]\n[1,] -7.105427e-15\n[2,] -7.105427e-15\n[3,]  7.105427e-15\n\nA %*% c(-rref(A)[, 5], 0, 1)\n\n              [,1]\n[1,] -1.421085e-14\n[2,] -1.421085e-14\n[3,]  1.421085e-14\n\n\n\n\n\n\nTheorem 11.1 The null space of a n \\(m \\times m\\) matrix \\(\\mathbf{A}\\) is a subspace of \\(\\mathcal{R}^n\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nDo in class Show that the three requirements of the definition of a subspace in Definition 11.1 are met.\n\n\n\n\n\n\n\n\n\nExample: give \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) and determine if \\(\\mathbf{x}\\) is in the null space of \\(\\mathbf{A}\\) using R"
  },
  {
    "objectID": "10-subspaces-of-Rn.html#the-basis-of-a-subspace",
    "href": "10-subspaces-of-Rn.html#the-basis-of-a-subspace",
    "title": "11  Subspaces of \\(\\mathcal{R}^n\\)",
    "section": "11.2 The basis of a subspace",
    "text": "11.2 The basis of a subspace\n\nDefinition 11.4 A basis for a subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) is\n\na linearly independent set in \\(\\mathcal{H}\\) that\nspans \\(\\mathcal{H}\\).\n\nEquivalently, a basis is a set of linearly independent vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\) such that span\\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\} = \\mathcal{H}\\).\n\nThe requirement that the vectors of a basis are linearly independent while spanning a subspace \\(\\mathcal{H}\\) means that a basis is a minimal spanning set for the subspace \\(\\mathcal{H}\\)\n\nExample 11.5 Is a basis for a vector space unique?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA basis for a vector is not unique. Just like you can represent the number 16 as \\(1.6 * 10^1\\) (base 10) or \\(2^4\\) (base 2), the basis for a vector space is also not unique.\n\n\n\n\nDefinition 11.5 The standard basis for \\(\\mathcal{R}^n\\) is the set of vectors \\(\\left\\{ \\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n \\right\\}\\) of length \\(n\\) where the vector \\(\\mathbf{e}_j\\) is a vector that is 0 in every value except for a 1 in the \\(j\\)th position. For example, \\[\n\\begin{aligned}\n\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}, && \\mathbf{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}, && \\mathbf{e}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 0\\end{pmatrix}, && \\ldots, && \\mathbf{e}_n = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{pmatrix}.\n\\end{aligned}\n\\] Notice that the matrix defined as having columns \\(\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\) is the identity matrix \\(\\mathbf{I}\\).\n\n\nExample 11.6 What is the standard basis for \\(\\mathcal{R}^3\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe standard basis for \\(\\mathcal{R}^3\\) are the x-, y-, and z-axes. These are written as vectors where the x-axis is \\(\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\), the y-axis is \\(\\mathbf{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\), and the z-axis is \\(\\mathbf{e}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1\\end{pmatrix}\\),\n\n\n\n\nExample 11.7 Do the following set of vectors form a basis for \\(\\mathcal{R}^3\\)?\n\n\n\n\\(\\mathbf{x} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 2 \\end{pmatrix}\\), \\(\\mathbf{y} = \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\), and \\(\\mathbf{z} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the set of vectors \\(\\mathbf{x} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 2 \\end{pmatrix}\\), \\(\\mathbf{y} = \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\), and \\(\\mathbf{z} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}\\) to form a basis for \\(\\mathcal{R}^3\\), we need to satisfy the two conditions in Definition 11.4. Both conditions of Definition 11.4 can be checked by combining the set of vectors \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), and \\(\\mathbf{z}\\) into a matrix and using RREF to determine the span\\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) and determine whether the set of vectors \\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) is linearly independent. The matrix of the vectors is \\[\n\\begin{pmatrix} 2 & -1 & 1 \\\\ 1 & 1 & 0 \\\\ 2 & 1 & 2 \\end{pmatrix}\n\\] which is row-equivalent to \\[\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n\\] Because the reduced row echelon form of the matrix has 3 pivot columns, the span\\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\} = \\mathcal{R}^3\\) which satisfies the first condition for being a basis. Because there is a pivot in every column, we know the set of vectors is linearly independent which satisfies the second condition for being a basis. Therefore, the set of vectors \\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) forms a basis for \\(\\mathcal{R}^3\\)\nUsing R, we can show this result first by creating the vectors x, y, and z and then joining these into a matrix A using cbind()\n\nx <- c(2, 1, 2)\ny <- c(-1, 1, 1)\nz <- c(1, 0, 2)\n\nA <- cbind(x, y, z)\n\nNext, we convert A to reduced row echelon form to get the row equivalent matrix\n\nrref(A)\n\n     x y z\n[1,] 1 0 0\n[2,] 0 1 0\n[3,] 0 0 1\n\n\nBecause this reduced row echelon form has a pivot in each column we know the columns of the matrix A are linearly independent. Because there are 3 pivot columns in total, we know the span of the columns of A is \\(\\mathcal{R}^3\\). Therefore, the vectors \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z}\\) forms a basis for \\(\\mathcal{R}^3\\).\n\n\n\n\nExample 11.8 Do the following set of vectors form a basis for \\(\\mathcal{R}^3\\)?\n\n\n\n\\(\\mathbf{w} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 2 \\end{pmatrix}\\), \\(\\mathbf{x} = \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\), \\(\\mathbf{y} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}\\), and \\(\\mathbf{z} = \\begin{pmatrix} 4 \\\\ 2 \\\\ -2 \\end{pmatrix}\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the set of vectors \\(\\mathbf{w} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 2 \\end{pmatrix}\\), \\(\\mathbf{x} = \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\), \\(\\mathbf{y} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}\\), and \\(\\mathbf{z} = \\begin{pmatrix} 4 \\\\ 2 \\\\ -2 \\end{pmatrix}\\) to form a basis for \\(\\mathcal{R}^3\\), we need to satisfy the two conditions in Definition 11.4. Both conditions of Definition 11.4 be checked by combining the set of vectors \\(\\mathbf{w}\\), \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), and \\(\\mathbf{z}\\) into a matrix and using RREF to determine the span\\(\\{ \\mathbf{w}, \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) and determine whether the set of vectors \\(\\{ \\mathbf{w}, \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) is linearly independent. The matrix of the vectors is \\[\n\\begin{pmatrix} 2 & -1 & 1 & 4 \\\\ 1 & 1 & 0 & 2 \\\\ 2 & 1 & 2 & -2 \\end{pmatrix}\n\\] which is row-equivalent to \\[\n\\begin{pmatrix} 1 & 0 & 0 & 16/5 \\\\ 0 & 1 & 0 & -6/5 \\\\ 0 & 0 & 1 & -18/5 \\end{pmatrix}\n\\] Because the reduced row echelon form of the matrix has 3 pivot columns, the span\\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\} = \\mathcal{R}^3\\) which satisfies the first condition for being a basis. However, there is not a pivot in every column which tells us that the set of vectors is linearly dependent which does not satisfy the second condition for being a basis. Therefore, the set of vectors \\(\\{ \\mathbf{w}, \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) do not form a basis for \\(\\mathcal{R}^3\\)\nUsing R, we can show this result first by creating the vectors w, x,y, andzand then joining these into a matrixAusingcbind()`\n\nw <- c(2, 1, 2)\nx <- c(-1, 1, 1)\ny <- c(1, 0, 2)\nz <- c(4, 2, -2)\n\nA <- cbind(w, x, y, z)\n\nNext, we convert A to reduced row echelon form to get the row equivalent matrix\n\nrref(A)\n\n     w x y    z\n[1,] 1 0 0  3.2\n[2,] 0 1 0 -1.2\n[3,] 0 0 1 -3.6\n\n\nBecause this reduced row echelon form does not have a pivot in each column, we know the columns of the matrix A are linearly dependent. Because there are 3 pivot columns in total, we know the span of the columns of A is \\(\\mathcal{R}^3\\). Because the set of vectors \\(\\{ \\mathbf{w}, \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) are linearly dependent, they do not form a basis for \\(\\mathcal{R}^3\\).\n\n\n\n\nExample 11.9 Do the following set of vectors form a basis for \\(\\mathcal{R}^3\\)?\n\n\n\n\\(\\mathbf{x} = \\begin{pmatrix} 4 \\\\ 3 \\\\ 2 \\end{pmatrix}\\), \\(\\mathbf{y} = \\begin{pmatrix} 3 \\\\ -3 \\\\ 4 \\end{pmatrix}\\), and \\(\\mathbf{z} = \\begin{pmatrix} 5 \\\\ 9 \\\\ 0 \\end{pmatrix}\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the set of vectors \\(\\mathbf{x} = \\begin{pmatrix} 4 \\\\ 3 \\\\ 2 \\end{pmatrix}\\), \\(\\mathbf{y} = \\begin{pmatrix} 3 \\\\ -3 \\\\ 4 \\end{pmatrix}\\), and \\(\\mathbf{z} = \\begin{pmatrix} 5 \\\\ 9 \\\\ 0 \\end{pmatrix}\\) to form a basis for \\(\\mathcal{R}^3\\), we need to satisfy the two conditions in Definition 11.4. Both conditions of Definition Definition 11.4 can be checked by combining the set of vectors \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), and \\(\\mathbf{z}\\) into a matrix and using RREF to determine the span\\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) and determine whether the set of vectors \\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) is linearly independent. The matrix of the vectors is \\[\n\\begin{pmatrix} 4 & 3 & 5 \\\\ 3 & -3 & 9 \\\\ 2 & 4 & 0 \\end{pmatrix}\n\\] which is row-equivalent to \\[\n\\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix}\n\\] Because the reduced row echelon form of the matrix has 2 pivot columns, the span\\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\} = \\mathcal{R}^2\\) which does not satisfy the first condition for being a basis for \\(\\mathcal{R}^3\\). In addition, there is not a pivot in every column which tells us that the set of vectors is linearly dependent which does not satisfy the second condition for being a basis. Therefore, the set of vectors \\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) do not form a basis for \\(\\mathcal{R}^3\\)\nUsing R, we can show this result first by creating the vectors x,y, andzand then joining these into a matrixAusingcbind()`\n\nx <- c(4, 3, 2)\ny <- c(3, -3, 4)\nz <- c(5, 9, 0)\n\nA <- cbind(x, y, z)\n\nNext, we convert A to reduced row echelon form to get the row equivalent matrix\n\nrref(A)\n\n     x y  z\n[1,] 1 0  2\n[2,] 0 1 -1\n[3,] 0 0  0\n\n\nBecause this reduced row echelon form does not have a pivot in each column, we know the columns of the matrix A are linearly dependent. Because there are 2 pivot columns in total, we know the span of the columns of A is \\(\\mathcal{R}^2\\) which is not \\(\\mathcal{R}^3\\). Because the set of vectors \\(\\{ \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\}\\) are linearly dependent and do not span \\(\\mathcal{R}^3\\), they do not form a basis for \\(\\mathcal{R}^3\\).\n\n\n\n\nExample 11.10 Using R, find a basis for the null space of the matrix \\[\n\\mathbf{A} = \\begin{pmatrix} 2 & 4 & 1 & 3 \\\\ -1 & -2 & 6 & 5 \\\\ 1 & 2 & -3 & 2 \\end{pmatrix}\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGiven the matrix \\(\\mathbf{A}\\), we look for non-trivial solutions to \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\)\n\nA <- matrix(c(2, -1, 1, 4, -2, 2, 1, 6, -3, 3, 5, 2), 3, 4)\nrref(cbind(A, 0))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    0    0    0\n[2,]    0    0    1    0    0\n[3,]    0    0    0    1    0\n\n\nwhich has solution \\(x_1 = -2 x_2\\), \\(x_2 = \\mbox{free}\\), \\(x_3 = 0\\) and \\(x_4 = 0\\). This can be represented as a vector \\(\\mathbf{v} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\). Thus, \\(\\left\\{ \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right\\}\\) is a basis for the null space of \\(\\mathbf{A}\\).\nWe can check this by showing that \\(\\mathbf{A} \\mathbf{v} = \\mathbf{0}\\)\n\nv <- c(-2, 1, 0, 0)\nA %*% v\n\n     [,1]\n[1,]    0\n[2,]    0\n[3,]    0\n\n\nIn addition, any linear combination of the basis is also in the null space\n\nA %*% (5*v)\n\n     [,1]\n[1,]    0\n[2,]    0\n[3,]    0\n\n\n\n\n\n\nTheorem 11.2 The pivot columns of a matrix \\(\\mathbf{A}\\) form a basis for the column space of \\(\\mathbf{A}\\).\n\nNote: Use the columns of \\(\\mathbf{A}\\), not the columns of the matrix in echelon form.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will provide just a sketch of the proof here. First, the column space (Definition 11.2) is the space defined by the linear combination of the columns of \\(\\mathbf{A}\\). Thus, the span of any subset of the vectors that make up the columns of \\(\\mathbf{A}\\) must, by definition, be in the column space of \\(\\mathbf{A}\\), because any linear combination of the subset of vectors is just a linear combination of the full set of vectors with the coefficients of the vectors in the subset with the same coefficients and the coefficients of the vectors not in the subset equal to 0. Thus, the span of the subset of the vectors is contained within the span of the columns of \\(\\mathbf{A}\\), which is defined as the column space.\nNow, choose the subset of columns of \\(\\mathbf{A}\\) that correspond to the pivot columns of the reduced row echelon form of \\(\\mathbf{A}\\). If the span of the columns of \\(\\mathbf{A} = \\mathcal{R}^p\\), then 1) there must be \\(p\\) pivot columns in the reduced row echelon form of \\(\\mathbf{A}\\) and 2) there are \\(p\\) vectors in the subset of vectors where the columns of \\(\\mathbf{A}\\) are pivot columns. Thus, the subset of vectors spans \\(\\mathcal{R}^p\\) (the column space) and, by definition, there are \\(p\\) pivot columns (therefore there is a pivot in each column) so the subset of vectors defined by the pivot columns of \\(\\mathbf{A}\\) are linearly independent.\n\n\n\n\nExample 11.11 Find a basis for the column space of the matrix \\[\n\\mathbf{A} = \\begin{pmatrix} 3 & 1 & 2 & -3 \\\\ 4 & 1 & -3 & -2 \\\\ 4 & -1 & -3 & 1 \\end{pmatrix}\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nGiven the matrix \\(\\mathbf{A}\\), we first find its reduced row echelon form \\[\n\\begin{pmatrix} 1 & 0 & 0 & -11/34 \\\\ 0 & 1 & 0 & -3/2 \\\\ 0 & 0 & 1 & -9/34 \\end{pmatrix}\n\\] which has pivots in the first three columns. Thus, the column space of \\(\\mathbf{A}\\), which is defined as the linear combination of the vectors of \\(\\mathbf{A}\\), spans \\(\\mathcal{R}^3\\) because there are three pivot columns. The first three columns of \\(\\mathbf{A}\\) are \\(\\begin{pmatrix} 3 \\\\ 4 \\\\ 4 \\end{pmatrix}\\), \\(\\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}\\), and \\(\\begin{pmatrix} 2 \\\\ -3 \\\\ -3 \\end{pmatrix}\\). Because these 3 vectors are linearly independent (they are each pivot columns in the reduced row echelon form of \\(\\mathbf{A}\\)) and they span \\(\\mathcal{R}^3\\), they form a basis for the column space of \\(\\mathbf{A}\\).\nThus, any vector in the columns space of \\(\\mathbf{A}\\) (for example, the fourth column of \\(\\mathbf{A}\\)) can be written as a linear combination of the basis vectors of the columns space of \\(\\mathbf{A}\\).\n\n\n\n\n\nExample 11.12 \n\nFind a basis for the column space of the matrix \\[\n\\begin{pmatrix} -4 & 1 \\\\ 8 & -2 \\\\ 6 & 3 \\\\ 9 & 7 \\end{pmatrix}\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nGiven the matrix \\(\\mathbf{A}\\), we first find its reduced row echelon form \\[\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\] which has pivots in the first two columns. Thus, the column space of \\(\\mathbf{A}\\), which is defined as the linear combination of the vectors of \\(\\mathbf{A}\\), spans \\(\\mathcal{R}^2\\) because there are two pivot columns. The first two columns of \\(\\mathbf{A}\\) are \\(\\begin{pmatrix} -4 \\\\ 8 \\\\ 6 \\\\ 9 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\\\ 7 \\end{pmatrix}\\). Because these 2 vectors are linearly independent (they are each pivot columns) and they span \\(\\mathcal{R}^2\\), they form a basis for the column space of \\(\\mathbf{A}\\).\nNote however, that the first two columns of the reduced row echelon form of \\(\\mathbf{A}\\) (the vectors \\(\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\)) do not form a basis for the column space of \\(\\mathbf{A}\\). This can be seen because The first two columns of \\(\\mathbf{A}\\) have non-zero entries in the 3rd and 4th elements."
  },
  {
    "objectID": "11-dimension-and-rank.html",
    "href": "11-dimension-and-rank.html",
    "title": "12  Dimension and Rank",
    "section": "",
    "text": "library(dasc2594)"
  },
  {
    "objectID": "11-dimension-and-rank.html#coordinate-systems",
    "href": "11-dimension-and-rank.html#coordinate-systems",
    "title": "12  Dimension and Rank",
    "section": "12.1 Coordinate systems",
    "text": "12.1 Coordinate systems\nRecall the idea of polynomials (e.g., a polynomial of order \\(p\\) is \\(a_1x^p + a_2x^{p-1} + \\ldots + a_p x^1 + a_{p+1} x^0\\)) where the polynomials \\(x^p, x^{p-1}, \\ldots, x^1, x^0\\) form a set of powers up to the power \\(p\\) of \\(x\\) from which the coefficients \\(a_p, \\ldots, a_{p+1}\\) can be used to make any polynomial of order \\(p\\). It can be said that the powers of \\(x\\) (\\(x^p, x^{p-1}, \\ldots, x^1, x^0\\)) form a basis for all polynomials of order \\(p\\).\nIn the previous section, we extended this analogy to vector spaces using the concept of a minimal spanning set. Consider the basis \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) for a subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) where span\\(\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\} = \\mathcal{H}\\). Because the set \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) is a basis, the set of vectors is linearly independent. Then, because the set \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) is a basis, we have the following result.\n\nTheorem 12.1 For each vector \\(\\mathbf{a}\\) in the subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\), and a basis \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\), there is a unique set of coefficients \\(x_1, \\ldots, x_k\\) such that \\[\n\\begin{aligned}\n\\mathbf{a} & = x_1 \\mathbf{b}_1 + \\cdots + x_k \\mathbf{b}_k\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn class: assume contradiction that there are two ways \\(x_1, \\ldots, x_k\\) and \\(y_1, \\ldots, y_k\\)… Show that this violates the assumption of linear dependence.\n\n\n\n\nDefinition 12.1 Let \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\}\\) be a basis for a subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\). Then, for each \\(\\mathbf{a} \\in \\mathcal{H}\\), the coordinates of \\(\\mathbf{a}\\) with respect to the basis \\(\\mathcal{B}\\) are the set of coefficients \\(\\{x_1, \\ldots, x_k\\}\\) where \\[\n\\begin{aligned}\n\\mathbf{a} & = x_1 \\mathbf{b}_1 + \\cdots + x_k \\mathbf{b}_k.\n\\end{aligned}\n\\]\n\n\n\n\n\nExample 12.1 Let \\(\\mathcal{B} = \\left\\{ \\mathbf{b}_1 = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\mathbf{b}_2 = \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix}, \\mathbf{b}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right\\}\\) and \\(\\mathbf{a} = \\begin{pmatrix} 5 \\\\ 6 \\\\ 1 \\end{pmatrix}\\). What are the coordinates of \\(\\mathbf{a}\\) with respect to the basis \\(\\mathcal{B}\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt can be seen that \\(\\mathbf{a} = 3 \\mathbf{b}_1 - 2 \\mathbf{b}_2 + 0 \\mathbf{b}_3\\) because \\(3 \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix} - 2 \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + 0 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 6 \\\\ 1 \\end{pmatrix}\\). Thus the coordinates of \\(\\mathbf{a}\\) with respect to \\(\\mathcal{B}\\) are \\(\\mathbf{x} = \\begin{pmatrix} 3 \\\\ -2 \\\\ 0 \\end{pmatrix}\\)\nNow, the question is how to find such a solution in general. What we know is that if we write the matrix \\(\\mathbf{B} = \\begin{pmatrix} \\mathbf{b}_1 & \\mathbf{b_2} & \\mathbf{b_3} \\end{pmatrix}\\), then the coefficients for the vector \\(\\mathbf{x}\\) are the solutions to the matrix equation \\[\n\\begin{aligned}\n\\mathbf{B} \\mathbf{x} = \\mathbf{a}\n\\end{aligned}\n\\] Notice that this is the same matrix equation as \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) but written in different notation that denotes that \\(\\mathbf{B}\\) is a basis. Because \\(\\mathbf{B}\\) is a basis, we know that there is a pivot in every column which tells us that as long as \\(\\mathbf{a}\\) is in the columnspace of \\(\\mathbf{B}\\), there will be a unique solution for the coordinates \\(\\mathbf{x}\\). Using an augmented matrix approach, you can solve for \\(\\mathbf{x}\\) using elementary row operations applied to the matrix \\[\n\\begin{aligned}\n\\begin{pmatrix} \\mathbf{B} & \\mathbf{a} \\end{pmatrix} & = \\begin{pmatrix} 3 & 2 & 0 & 5 \\\\ 0 & -3 & 0 & 6 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix} \\\\\n& \\stackrel{RREF}{\\sim} \\begin{pmatrix} 1 & 0 & 0 & 3 \\\\ 0 & 1 & 0 & -2 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix}\n\\end{aligned}\n\\] which gives the solution that \\(\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_ 2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -2 \\\\ 0\\end{pmatrix}\\)\nIn R, this can be done as\n\nb1 <- c(3, 0, 1)\nb2 <- c(2, -3, 1)\nb3 <- c(0, 0, 1)\na <- c(5, 6, 1)\n\nrref(cbind(b1, b2, b3, a))\n\n     b1 b2 b3  a\n[1,]  1  0  0  3\n[2,]  0  1  0 -2\n[3,]  0  0  1  0"
  },
  {
    "objectID": "11-dimension-and-rank.html#dimension-of-a-subspace",
    "href": "11-dimension-and-rank.html#dimension-of-a-subspace",
    "title": "12  Dimension and Rank",
    "section": "12.2 Dimension of a subspace",
    "text": "12.2 Dimension of a subspace\n\nDefinition 12.2 The dimension \\(\\operatorname{dim}(\\mathcal{H})\\) of a nonzero subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) is the number of (nonzero) vectors that make up a basis \\(\\mathcal{B}\\) for \\(\\mathcal{H}\\). The dimension of the subspace \\(\\mathcal{H} = \\{\\mathbf{0}\\}\\) that contains only the \\(\\mathbf{0}\\) vectors is defined as 0.\n\n\nExample 12.2 Note that under this definition, the basis \\(\\mathcal{B}\\) is not unique. For example, the following bases for the 3-dimensional subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^3\\) both have three linearly independent vectors.\n\\[\n\\begin{aligned}\n\\mathcal{B}_1 =  \\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right\\} && \\mathcal{B}_2 = \\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right\\}\n\\end{aligned}\n\\]\nLet \\(\\mathbf{x} = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix}\\). What are the coordinates of \\(\\mathbf{x}\\) with respect to \\(\\mathcal{B}_1\\) and \\(\\mathcal{B}_2\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUnder the basis \\(\\mathcal{B}_1\\), the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}_1\\) are \\(a_1 = 3\\), \\(a_2 = 4\\), and \\(a_3 = 0\\) because \\[\n\\begin{aligned}\n\\mathbf{x} = a_1 \\mathbf{b}_1 + a_2 \\mathbf{b}_2 + a_3 \\mathbf{b}_3 =  \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + 4 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + 0 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix},\n\\end{aligned}\n\\] which we write as \\(\\left[\\mathbf{x}\\right]_{B_1} = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix}\\) to denote that these are the coordinates of the vector \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}_1\\).\nThe coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}_2\\) are \\(a_1 = 3\\), \\(a_2 = 1\\), and \\(a_3 = 0\\) because \\[\n\\begin{aligned}\n\\mathbf{x} = a_1 \\mathbf{b}_1 + a_2 \\mathbf{b}_2 + a_3 \\mathbf{b}_3 = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + 0 \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n\\end{aligned}\n\\] which we write as \\(\\left[\\mathbf{x}\\right]_{B_2} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) to denote that these are the coordinates of the vector \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}_2\\)..\nWe can get these coordinates using R by creating augmented matrices and using row operations. For example, the coordinates of \\(\\mathbf{x}\\) with respect to \\(\\mathcal{B}_1\\) are\n\nB1 <- matrix(c(1, 0, 0, 0, 1, 0, 0, 0, 1), 3, 3)\nB1\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nx <- c(3, 4, 0)\nx\n\n[1] 3 4 0\n\n# augmented matrix\ncbind(B1, x)\n\n           x\n[1,] 1 0 0 3\n[2,] 0 1 0 4\n[3,] 0 0 1 0\n\n# rref of augmented matrix\nrref(cbind(B1, x))\n\n           x\n[1,] 1 0 0 3\n[2,] 0 1 0 4\n[3,] 0 0 1 0\n\n\nwhich gives the coordinates\n\nrref(cbind(B1, x))[, 4]\n\n[1] 3 4 0\n\n\nThe coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}_2\\) are\n\nB2 <- matrix(c(1, 1, 0, 0, 1, 0, 1, 0, 1), 3, 3)\nB2\n\n     [,1] [,2] [,3]\n[1,]    1    0    1\n[2,]    1    1    0\n[3,]    0    0    1\n\nx <- c(3, 4, 0)\nx\n\n[1] 3 4 0\n\n# augmented matrix\ncbind(B2, x)\n\n           x\n[1,] 1 0 1 3\n[2,] 1 1 0 4\n[3,] 0 0 1 0\n\n# rref of augmented matrix\nrref(cbind(B2, x))\n\n           x\n[1,] 1 0 0 3\n[2,] 0 1 0 1\n[3,] 0 0 1 0\n\n\nwhich gives the coordinates\n\nrref(cbind(B2, x))[, 4]\n\n[1] 3 1 0\n\n\n\n\n\n\nExample 12.3 Also note that if two subspaces \\(\\mathcal{H}_1\\) and \\(\\mathcal{H}_2\\) have the same dimension (i.e., dim(\\(\\mathcal{H}_1\\)) = dim(\\(\\mathcal{H}_2\\)) = \\(p\\)), this does not mean that these are the same subspaces. For example, Let \\(\\mathcal{H}_1\\) and \\(\\mathcal{H}_2\\) be subspaces of \\(\\mathcal{R}^3\\) of dimension 2 with respective bases \\[\n\\begin{aligned}\n\\mathcal{B}_1 = \\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\} && \\mathcal{B}_2 = \\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right\\}.\n\\end{aligned}\n\\]\nNote that the subspace defined by the span of the basis vectors in \\(\\mathcal{B}_1\\) is a plane in the x-y axes and the subspace defined by the span of the basis vectors in \\(\\mathcal{B}_2\\) is a plane in the x-z axes.\n\n\nWhat is the dimension of a basis for \\(\\mathcal{R}^n\\)?"
  },
  {
    "objectID": "11-dimension-and-rank.html#rank",
    "href": "11-dimension-and-rank.html#rank",
    "title": "12  Dimension and Rank",
    "section": "12.3 Rank",
    "text": "12.3 Rank\n\nDefinition 12.3 The rank of a matrix \\(\\mathbf{A}\\), denoted as \\(\\operatorname{rank}(\\mathbf{A})\\), is the dimension of the column space of \\(\\mathcal{A}\\).\n\nRecall that the pivot columns of \\(\\mathbf{A}\\) form a basis for the column space of \\(\\mathbf{A}\\). Hence, the number of pivot columns in the matrix \\(\\mathbf{A}\\) is the rank of the matrix \\(\\mathbf{A}\\).\n\nExample 12.4 Determine the rank of the following matrices\n\n\n\n\n\\(\\mathbf{A} = \\begin{pmatrix} -7 & 1 & -5 & 9 \\\\ 5 & -6 & 4 & 8 \\\\ -4 & -1 & -2 & 0 \\end{pmatrix}\\)\n\\(\\mathbf{B} = \\begin{pmatrix} 5 & -1 & 3 \\\\ -6 & 4 & -5 \\\\ 6 & 6 & 0 \\\\ -7 & -25 & 9 \\\\ 8 & 26 & -9 \\end{pmatrix}\\)\n\\(\\mathbf{C} = \\begin{pmatrix} 3 & -5 & 1 & -8 & -1 \\\\ -2 & -6 & -9 & 9 & -3 \\\\ -4 & 5 & 4 & 8 & 2 \\end{pmatrix}\\)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing Definition 12.3, the rank of \\(\\mathbf{A}\\) is equal to the dimension of the column space of \\(\\mathbf{A}\\) where the dimension can be found by counting the number of pivot columns.\n\n\\(\\begin{pmatrix} -7 & 1 & -5 & 9 \\\\ 5 & -6 & 4 & 8 \\\\ -4 & -1 & -2 & 0 \\end{pmatrix} \\stackrel{RREF}{\\sim} \\begin{pmatrix} 1 & 0 & 0 & 200/27 \\\\ 0 & 1 & 0 & -34/9 \\\\ 0 & 0 & 1 & -349/27 \\end{pmatrix}\\) which has 3 pivot columns. Thus, \\(\\operatorname{rank}(\\mathbf{A}) = 3\\)\n\\(\\begin{pmatrix} 5 & -1 & 3 \\\\ -6 & 4 & -5 \\\\ 6 & 6 & 0 \\\\ -7 & -25 & 9 \\\\ 8 & 26 & -9 \\end{pmatrix} \\stackrel{RREF}{\\sim} \\begin{pmatrix} 1 & 0 & 1/2 \\\\ 0 & 1 & -1/2 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\\) which has 2 pivot columns. Thus, \\(\\operatorname{rank}(\\mathbf{B}) = 2\\)\n\\(\\begin{pmatrix} 3 & -5 & 1 & -8 & -1 \\\\ -2 & -6 & -9 & 9 & -3 \\\\ -4 & 5 & 4 & 8 & 2 \\end{pmatrix} \\stackrel{RREF}{\\sim} \\begin{pmatrix} 1 & 0 & 0 & -465/191 & -6/191 \\\\ 0 & 1 & 0 & 8/191 & 42/191 \\\\ 0 & 0 & 1 & -93/191 & 37/191 \\end{pmatrix}\\) which has 3 pivot columns. Thus, \\(\\operatorname{rank}(\\mathbf{C}) = 3\\)\n\n\n\n\n\nTheorem 12.2 (The Rank Theorem) If a matrix \\(\\mathbf{A}\\) has \\(n\\) columns, then \\(\\operatorname{rank}(\\mathbf{A}) + \\operatorname{dim}(\\operatorname{null}(\\mathbf{A})) = n\\)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe rank(\\(\\mathbf{A}\\)) is number of linearly independent columns. The dimension for the null(\\(\\mathbf{A}\\)) is the number of linearly dependent columns of \\(\\mathbf{A}\\) (non-trivial solutions to \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\)).\n\n\n\nThe following theorem states that any \\(p\\) vectors in \\(\\mathcal{R}^p\\) that are linearly independent must span \\(\\mathcal{R}^p\\).\n\nTheorem 12.3 (The Basis Theorem) Let \\(\\mathcal{H}\\) be a p-dimensional subspace of \\(\\mathcal{R}^n\\).\n\nThen any linearly independent set of \\(p\\) elements in \\(\\mathcal{H}\\) is a basis for \\(\\mathcal{H}\\).\nEquivalently, any set of \\(p\\) elements of \\(\\mathcal{H}\\) that span \\(\\mathcal{H}\\) is a basis for \\(\\mathcal{H}\\)\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe consider the two statements in the theorem above.\n\nEach of the \\(p\\) vectors are in \\(\\mathcal{H}\\) and the set of vectors in \\(\\mathcal{H}\\) are linearly independent. Thus, the span of the set of vectors is \\(\\mathcal{R}^p\\). We have examples where two subspaces have the same dimension but are not equal, however, because each vector is in \\(\\mathcal{H}\\) and \\(\\mathcal{H}\\) is a subspace, all linear combinations of the vectors are in \\(\\mathcal{H}\\). Thus, the set of \\(p\\) vectors span \\(\\mathcal{H}\\). Thus, the set of vectors spans the subspace and are linearly independent which satisfies the conditions of Definition 11.4.\nThe set of \\(p\\) vectors span \\(\\mathcal{H}\\). Because \\(\\mathcal{H}\\) is a \\(p\\)-dimensional subspace of \\(\\mathcal{R}^n\\), each vector must be linearly independent. If the vectors were not linearly independent, the \\(p\\) vectors would not span a \\(p\\)-dimensional space. Thus, the set of \\(p\\) vectors span \\(\\mathcal{H}\\). Thus, the set of vectors spans the subspace and are linearly independent which satisfies the conditions of Definition 11.4.\n\n\n\n\n\nTheorem 12.4 (Invertible Matrix Theorem Yet Again) Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) matrix. Then, in addition to the current conditions from Theorem 9.5, the following statements are equivalent to \\(\\mathbf{A}\\) being an invertible matrix:\n\nThe columns of \\(\\mathbf{A}\\) form a basis for \\(\\mathcal{R}^n\\)\n\\(\\operatorname{col}(\\mathbf{A}) = \\mathcal{R}^n\\)\n\\(\\operatorname{dim}(\\operatorname{col}(\\mathbf{A})) = n\\)\n\\(\\operatorname{rank}(\\mathbf{A}) = n\\)\n\\(\\operatorname{null}(\\mathbf{A}) = \\{\\mathbf{0}\\}\\)\n\\(\\operatorname{dim}(\\operatorname{null}(\\mathbf{A})) = 0\\)"
  },
  {
    "objectID": "12-determinants.html",
    "href": "12-determinants.html",
    "title": "13  Determinants",
    "section": "",
    "text": "3 Blue 1 Brown – The determinant\nThe determinant is the unique function mapping square matrices to the real number line that satisfies the above definition."
  },
  {
    "objectID": "12-determinants.html#determinants-of-2-times-2-matrices",
    "href": "12-determinants.html#determinants-of-2-times-2-matrices",
    "title": "13  Determinants",
    "section": "13.1 Determinants of \\(2 \\times 2\\) matrices",
    "text": "13.1 Determinants of \\(2 \\times 2\\) matrices\n\nIf \\(\\mathbf{A} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\) is a \\(2 \\times 2\\) matrix, the determinant \\(\\operatorname{det}(\\mathbf{A}) = ad - bc\\)\n\n\n\n\n\nLet \\(\\mathbf{A} = \\begin{pmatrix} 5 & 3 \\\\ 1 & -3 \\end{pmatrix}\\). What is \\(\\det(\\mathbf{A})\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing ?def-det22, the determinant of \\(\\mathbf{A} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\) is \\(\\det(\\mathbf{A}) = ad - bc= (5 * -3) - (3 * 1) = -18\\)"
  },
  {
    "objectID": "12-determinants.html#determinants-of-n-times-n-matrices",
    "href": "12-determinants.html#determinants-of-n-times-n-matrices",
    "title": "13  Determinants",
    "section": "13.2 Determinants of \\(n \\times n\\) matrices",
    "text": "13.2 Determinants of \\(n \\times n\\) matrices\nTo better understand determinants of \\(n \\times n\\) matrices, we need to define the two concepts of a matrix minor and cofactor.\n\nFor an \\(n \\times n\\) matrix \\(\\mathbf{A}\\),\n\nThe (i, j) minor \\(\\mathbf{A}_{-i-j}\\) is the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)th row and the \\(j\\) column from \\(\\mathbf{A}\\)\nThe (i, j) cofactor \\(c_{ij}\\) is defined using the determinant of the minor where \\[\n\\begin{aligned}\nc_{ij} = \\mathbf(-1)^{i + j} \\det{\\mathbf{A}_{-i-j}}\n\\end{aligned}\n\\]\n\n\nNote: The cofactor of a scalar \\(a\\) (a \\(1 \\times 1\\) matrix) is defined as \\(\\mathbf{C}_{ij} = (-1)^{1 + 1} \\det(a) = a\\).\nNote: The leading term in the cofactor definition \\(\\mathbf(-1)^{i + j}\\) defines a checkerboard pattern shown below \\[\n\\begin{aligned}\n\\begin{pmatrix}\n+ & - & + & - & \\cdots \\\\\n- & + & - & + & \\cdots \\\\\n+ & - & + & - & \\cdots \\\\\n- & + & - & + & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots \\\\\n+ & - & + & - & \\cdots \\\\\n\\end{pmatrix}\n\\end{aligned}\n\\]\n\n\nExample 13.1 \n\nLet \\(\\mathbf{A} = \\begin{pmatrix} 1 & -3 & 1 \\\\ 4 & 2 & -3 \\\\ 7 & 4 & 7 \\end{pmatrix}\\).\n\nFind the minor \\(\\mathbf{A}_{-2-3}\\)\nFind the cofactor \\(c_{23}\\)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing ?def-minor-cofactor, the minor \\(\\mathbf{A}_{-2-3}\\) is the matrix \\(\\mathbf{A}\\) with the second row and third column removed.\n\n\n\nThus, the minor \\(\\mathbf{A}_{-2-3} = \\begin{pmatrix} 1 & -3 \\\\ 7 & 4 \\end{pmatrix}\\).\nIn R, this can be found by first defining the matrix A as\n\nA <- matrix(c(1, 4,7, -3, 2, 4, 1, -3, 7), 3,3)\n\nthen finding the minor \\(\\mathbf{A}_{-2-3}\\) as\n\nA_minor_23 <- A[-2, -3]\n\nThe matrix cofactor \\(c_{23} = (-1)^{2+3} \\det \\mathbf{A}_{-2-3} = (-1) * ((1)(4) - (-3) (7)) = -25\\)\nIn R, given the minor A_minor_23, the cofactor is\n\n(-1)^(2+3) * det(A_minor_23)\n\n[1] -25\n\n\n\n\n\nNote that in the cofactor definition of a \\(n \\times n\\) matrix it is assumed that you can calculate the determinant of the \\(n-1 \\times n-1\\) minor \\(\\mathbf{A}_{-i-j}\\). From this we see that each of the \\(n \\times n\\) cofactors of \\(\\mathbf{A}\\) are themselves the (signed) determinants of \\(n-1 \\times n-1\\) submatrices (the matrix minors). Thus, solving for all cofactors in general requires a recursive definition where smaller and smaller submatrices are evaluated.\n\nTheorem 13.1 (Cofactor exapansion) Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix with \\(ij\\)th elements \\(a_{ij}\\). Then\n\nThe cofactor expansion along the \\(i\\)th row (for any fixed row \\(i\\)) is \\[\n\\begin{aligned}\n\\det(\\mathbf{A}) = \\sum_{j=1}^n a_{ij} c_{ij} = a_{i1} c_{i1} + a_{i2} c_{i2} + \\cdots + a_{in} c_{in}\n\\end{aligned}\n\\]\nThe cofactor expansion along the \\(j\\)th column (for any fixed column \\(j\\)) is \\[\n\\begin{aligned}\n\\det(\\mathbf{A}) = \\sum_{i=1}^n a_{ij} c_{ij} = a_{1j} c_{1j} + a_{2j} c_{2j} + \\cdots + a_{nj} c_{nj}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis is quite complex. For those interested, an example is available here\n\n\n\nNote: The above theorem states that there are actually \\(2n\\) ways to calculate the determinant–one for each row and column of \\(\\mathbf{A}\\).\n\n\n\n\nUse the minor/cofactor definition to calculate the determinant of the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{pmatrix} 5 & 0 & 2 \\\\ 1 & 3 & 3 \\\\ 2 & -4 & 1 \\end{pmatrix}\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe determinant of \\(\\mathbf{A}\\) can be found by using Theorem 13.1 by expanding either down a row or a column. Because the first row contains a 0, we will use the cofactor expansion theorem. The cofactor expansion along the first row is \\[\n\\begin{aligned}\n\\det(\\mathbf{A}) = a_{11} c_{11} + a_{12} c_{12} + a_{13} c_{13}\n\\end{aligned}\n\\] where \\(a_{ij}\\) is the \\(ij\\)th element of \\(\\mathbf{A}\\). Thus, the cofactor expansion is \\[\n\\begin{aligned}\n\\det(\\mathbf{A}) = 5 c_{11} + 0 c_{12} + 2 c_{13}\n\\end{aligned}\n\\] This implies that we only need to find the cofactors \\(c_{11}\\) and \\(c_{13}\\) (but not \\(c_{12}\\) because it gets multiplied by 0). Thus, the cofactor expansion along the first row only requires finding 2 cofactors \\(c_{11}\\) and \\(c_{13}\\).\n\n\n\nThe minor \\(\\mathbf{A}_{-1-1}\\) is the matrix \\(\\mathbf{A}\\) with the first row and first column removed and is \\(\\mathbf{A}_{-1-1} = \\begin{pmatrix} 3 & 3 \\\\ -4 & 1 \\end{pmatrix}\\). The minor \\(\\mathbf{A}_{-1-3}\\) is the matrix \\(\\mathbf{A}\\) with the first row and third column removed and is \\(\\mathbf{A}_{-1-3} = \\begin{pmatrix} 1 & 3 \\\\ 2 & -4 \\end{pmatrix}\\). The cofactor \\(c_{11}\\) is given by \\[\n\\begin{aligned}\nc_{11} = (-1)^{1+1}((3)(1) - (3) (-4)) = 15\n\\end{aligned}\n\\] and the cofactor \\(c_{13}\\) is given by \\[\n\\begin{aligned}\nc_{13} = (-1)^{1+3}((1)(-4) - (3) (2)) = -10\n\\end{aligned}\n\\]\nCombining these, the determinant of \\(\\mathbf{A}\\) using the cofactor expansion is \\[\n\\begin{aligned}\n\\det(\\mathbf{A}) & = a_{11} c_{11} + a_{12} c_{12} + a_{13} c_{13} \\\\\n& = (5) (15) + (0) (c_{12}) + (2) (-10) \\\\\n& = 55\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nUse the minor/cofactor definition to calculate the determinant of the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{pmatrix} 2 & 4 & -1 \\\\ -3 & 0 & 2 \\\\ 2 & 0 & 4 \\end{pmatrix}\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe determinant of \\(\\mathbf{A}\\) can be found by using Theorem 13.1 by expanding either down a row or a column. Because the second row contains multiple zeros, we will use the cofactor expansion theorem. The cofactor expansion along the second column is \\[\n\\begin{aligned}\n\\det(\\mathbf{A}) & = a_{12} c_{12} + a_{22} c_{22} + a_{32} c_{32} \\\\\n& = 4 c_{12} + 0 c_{22} + 0 c_{32}\n\\end{aligned}\n\\] This implies that we only need to find the cofactor \\(c_{12}\\) (but not \\(c_{22}\\) and \\(c_{32}\\) because these get multiplied by 0). Thus, the cofactor expansion along the second column only requires finding the single cofactor \\(c_{12}\\).\n\n\n\nThe minor \\(\\mathbf{A}_{-1-2}\\) is the matrix \\(\\mathbf{A}\\) with the first row and second column removed and is \\(\\mathbf{A}_{-1-2} = \\begin{pmatrix} -3 & 2 \\\\ 2 & 4 \\end{pmatrix}\\). The cofactor \\(c_{12}\\) is given by \\[\n\\begin{aligned}\nc_{12} = (-1)^{1+2}((-3)(4) - (2, 4) (2)) = -16.\n\\end{aligned}\n\\]\nThus, the determinant of \\(\\mathbf{A}\\) using the cofactor expansion is \\[\n\\begin{aligned}\n\\det(\\mathbf{A}) & = a_{12} c_{12} + a_{22} c_{22} + a_{32} c_{32} \\\\\n& = (4) (16) + (0) (c_{22}) + (0) (c_{32}) \\\\\n& = 64\n\\end{aligned}\n\\]\n\n\n\n\nLet \\(\\mathbf{A}\\) be a \\(n \\times n\\) matrix that has all zero entries for the \\(j\\)th row. Find \\(\\det(\\mathbf{A})\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing the cofactor expansion theorem (Theorem 13.1), expand the determinant along the \\(j\\)th row. Then, because all entries of the \\(j\\)th row of \\(\\mathbf{A}\\) are zero, this gives \\[\n\\begin{aligned}\n\\det(\\mathbf{A}) & = a_{j1} c_{j1} + a_{j2} c_{j2} + \\cdots + a_{jn} c_{jn} \\\\\n& = 0 c_{j1} + 0 c_{j2} + \\cdots + 0 c_{jn} \\\\\n& = 0\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\nTheorem 13.2 The determinant of a matrix \\(\\mathbf{A}\\) is equal to the determinant of its transpose \\(\\mathbf{A}'\\). In other words, \\(\\det(\\mathbf{A}) = \\det(\\mathbf{A}')\\)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFollows directly from cofactor expansion theorem. The expansion along a given row/column of \\(\\mathbf{A}\\) is equivalent to expansion along the corresponding column/row of \\(\\mathbf{A}'\\) (notice the row/column for \\(\\mathbf{A}\\) got swapped to column/row for \\(\\mathbf{A}'\\))."
  },
  {
    "objectID": "12-determinants.html#properties-of-determinants",
    "href": "12-determinants.html#properties-of-determinants",
    "title": "13  Determinants",
    "section": "13.3 Properties of determinants",
    "text": "13.3 Properties of determinants\n\nTheorem 13.3 A \\(n \\times n\\) square matrix \\(\\mathbf{A}\\) is invertible if and only if \\(\\det(\\mathbf{A}) \\neq 0\\)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the invertible matrix theorem (Theorem 9.5), we know that the matrix \\(\\mathbf{A}\\) is invertible if and only if every column of \\(\\mathbf{A}\\) is a pivot column. Therefore, each column is linearly independent from the other columns. Based on the example above, if the rows were not linearly independent, the determinant would be equal to 0 (as row operations could create a row/column of all zeros). Thus, if the determinant is not 0, the columns of \\(\\mathbf{A}\\) are linearly independent and the matrix is invertible.\n\n\n\n\nTheorem 13.4 If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are \\(n \\times n\\) matrices, \\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix, and \\(c\\) is a scalar, we have\n\n\\(\\det(\\mathbf{I}) = 1\\)\n\\(\\det(\\mathbf{A}) = \\det(\\mathbf{A}')\\)\n\\(\\det(\\mathbf{A}^{-1}) = 1 / \\det(\\mathbf{A})\\) if \\(\\det(\\mathbf{A}) \\neq 0\\) (\\(\\mathbf{A}\\) is invertible)\n\\(\\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B})\\)\n\\(\\det(c\\mathbf{A}) = c^n \\det(\\mathbf{A})\\)"
  },
  {
    "objectID": "12-determinants.html#cramers-rule-and-determinants",
    "href": "12-determinants.html#cramers-rule-and-determinants",
    "title": "13  Determinants",
    "section": "13.4 Cramer’s Rule and Determinants",
    "text": "13.4 Cramer’s Rule and Determinants\n\n3 Blue 1 Brown – Cramer’s rule\n\nWhile commonly used for theoretical results, Cramer’s rule is not commonly used in applied linear algebra. As such, we will mention Cramer’s rule but not focus on it.\n\nTheorem 13.5 (Cramer’s Rule) Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) invertible matrix. Define \\(\\mathbf{A}_i(\\mathbf{b})\\) as the matrix \\(\\mathbf{A}\\) with the \\(i\\)th column replace by the vector \\(\\mathbf{b}\\). For example, \\(\\mathbf{A}_i(\\mathbf{b}) = \\begin{pmatrix} \\mathbf{a}_1 & \\cdots & \\mathbf{a}_{i-1} & \\mathbf{b} & \\mathbf{a}_{i+1} & \\cdots & \\mathbf{a}_n \\end{pmatrix}\\). Then, for any \\(\\mathbf{b} \\in \\mathcal{R}^n\\), the unique solution to \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has entries given by \\[\n\\begin{aligned}\nx_i = \\frac{\\det(\\mathbf{A}_i(\\mathbf{b}))}{\\det(\\mathbf{A})} & \\mbox{ for } i = 1, 2, \\ldots, n\n\\end{aligned}\n\\]\n\n\nExample 13.2 In Cramer’s rule, why do we know\n\nthe solution is unique for any \\(\\mathbf{b}\\)?\nthe determinant \\(\\det(\\mathbf{A}) \\neq 0\\)?"
  },
  {
    "objectID": "13-determinants-and-volumes.html",
    "href": "13-determinants-and-volumes.html",
    "title": "14  Determinants and volumes",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dasc2594)\nThe determinant is a function that takes the vectors \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\) that make up the columns of \\(\\mathbf{A}\\) and returns the volume of the parallelpiped \\(\\mathcal{P}\\) from Definition 14.1.\nThe Shiny app below allows you to plot the vector for any \\((x, y)\\) pair of your choosing.\nNote: Because \\(\\det(\\mathbf{A}) = \\det(\\mathbf{A}')\\), the absolute value of the determinant is equal to the volume of the parallelpiped defined by the columns of \\(\\mathbf{A}\\) (we could just have easily done all the calculations on the columns of \\(\\mathbf{A}\\) as the rows of \\(\\mathbf{A}\\))."
  },
  {
    "objectID": "13-determinants-and-volumes.html#volumes-of-parallelpipeds",
    "href": "13-determinants-and-volumes.html#volumes-of-parallelpipeds",
    "title": "14  Determinants and volumes",
    "section": "14.1 Volumes of Parallelpipeds",
    "text": "14.1 Volumes of Parallelpipeds\n\nLet \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\) be nonzero vectors. Then, for any scalar \\(c\\), the area of the parallelpiped defined by \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\) is the same as the area of the parallelpiped defined by the vectors \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2 + c \\mathbf{a}_1\\) (an elementary column operation).\n\n\nDraw a parallelpiped in class. Recall that areas of a parallelpiped are defined (in 2 dimensions) as the length of the base times the height perpendicular to the base. In 3 dimensions, the volume of a parallelpiped is the base times the width (the area of the base) times the height."
  },
  {
    "objectID": "13-determinants-and-volumes.html#volumes-of-linear-transformations",
    "href": "13-determinants-and-volumes.html#volumes-of-linear-transformations",
    "title": "14  Determinants and volumes",
    "section": "14.2 Volumes of Linear Transformations",
    "text": "14.2 Volumes of Linear Transformations\nRecall linear transformations \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) (Section Chapter 7) where for any \\(\\mathbf{x} \\in \\mathcal{R}^n\\) (the domain), \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} \\in \\mathcal{R}^n\\) (the codomain).\n\nTheorem 14.2 Let \\(\\mathcal{S}\\) be a set in the domain that has a volume \\(vol(\\mathcal{S})\\). Then, the volume of the image of the set under the transformation \\(T(\\mathcal{S})\\) is \\(vol(T(\\mathcal{S})) = |\\det(\\mathbf{A})|vol(\\mathcal{S})\\)"
  },
  {
    "objectID": "14-vector-spaces-and-subspaces.html",
    "href": "14-vector-spaces-and-subspaces.html",
    "title": "15  Vector Spaces and Subspaces",
    "section": "",
    "text": "3 Blue 1 Brown – Abstract vector spaces\nRecall the definition of a subspace:\nA consequence of this definition is that a subspace \\(\\mathcal{H}\\) is closed under linear combinations."
  },
  {
    "objectID": "14-vector-spaces-and-subspaces.html#null-space-and-column-space",
    "href": "14-vector-spaces-and-subspaces.html#null-space-and-column-space",
    "title": "15  Vector Spaces and Subspaces",
    "section": "15.1 Null space and column space",
    "text": "15.1 Null space and column space\nAlso, recall the special subspaces of the column space and the null space.\n\n15.1.1 Null space\n\nThe null space null(\\(\\mathbf{A}\\)) of an \\(m \\times n\\) \\(\\mathbf{A}\\) is the set of all solutions of the homogeneous equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\)\nAnother way to write null(\\(\\mathbf{A}\\)) is\n\\[\n\\begin{aligned}\n\\mbox{null}(\\mathbf{A}) = \\{\\mathbf{x} : \\mathbf{x} \\in \\mathcal{R}^n \\mbox{ and } \\mathbf{A} \\mathbf{x} = \\mathbf{0} \\}\n\\end{aligned}\n\\]\n\n\nTheorem 15.1 The null space of an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) is a subspace of \\(\\mathcal{R}^n\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTo show that the null space of \\(\\mathbf{A}\\), denoted null(\\(\\mathbf{A})\\), is a subspace we need to show the following\n\nThe zero vector \\(\\mathbf{0}\\) is in the null space of \\(\\mathbf{A}\\)\nThe null space of \\(\\mathbf{A}\\) is closed under addition\nThe null space of \\(\\mathbf{A}\\) is closed under scalar multiplication\n\nThe null space of \\(\\mathbf{A}\\) is defined as the set of vectors \\(\\mathbf{x}\\) such that \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\).\n\nFirst, we show that the zero vector is in the subspace by setting \\(\\mathbf{x} = \\mathbf{0}\\). Thus, because \\(\\mathbf{A} \\mathbf{0} = \\mathbf{0}\\), the zero vector \\(\\mathbf{0}\\) is in the null space of \\(\\mathbf{A}\\) .\nNext, let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors in the null space of \\(\\mathbf{A}\\). Thus, by the definition of the null space we have \\(\\mathbf{A} \\mathbf{u} = \\mathbf{0}\\) and \\(\\mathbf{A} \\mathbf{v} = \\mathbf{0}\\). Consider the vector \\(\\mathbf{u} + \\mathbf{v}\\) and consider \\(\\mathbf{A} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{A} \\mathbf{u} + \\mathbf{A} \\mathbf{v} = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}\\). Thus \\(\\mathbf{u} + \\mathbf{v}\\) is in the null space of \\(\\mathbf{A}\\).\nFinally, let \\(\\mathbf{u}\\) be a vector in the null space of \\(\\mathbf{A}\\) and let \\(c\\) be a scalar. Thus, by the definition of the null space we have \\(\\mathbf{A} \\mathbf{u} = \\mathbf{0}\\). Then, consider \\(\\mathbf{A} (c \\mathbf{u}) = c \\mathbf{A} \\mathbf{u} = c \\mathbf{0} = \\mathbf{0}\\)\n\nBecause the three requirements for a subspace are met, this gives us that the null space of \\(\\mathbf{A}\\) is a subspace.\n\n\n\nAs a consequence, there will exist a set of vectors that span the null space null(\\(\\mathbf{A}\\)). However, the null space of \\(\\mathbf{A}\\) is defined implicitly. This means that the null space of \\(\\mathbf{A}\\) is not obvious given the vectors of \\(\\mathbf{A}\\) and must be checked/calculated.\n\n\nExample 15.1 \nIn class\n\nFind a spanning set for null(\\(\\mathbf{A}\\)) where\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} 7 & -2 & 7 & -4 & 5 \\\\ 2 & 0 & 3 & 3 & 9 \\\\ -5 & 2 & -5 & 7 & -2 \\end{pmatrix}\n\\end{aligned}\n\\]\n\nFind solution to system of homogeneous system of equations \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\)\n\n\nA <- matrix(c(7, 2, -5, -2, 0, 2, 7, 3, -5, -4, 3, 7, 5, 9, -2), 3, 5)\nrref(cbind(A, 0))\n\n     [,1] [,2] [,3] [,4]  [,5] [,6]\n[1,]    1    0    0 1.50 -4.50    0\n[2,]    0    1    0 7.25  2.75    0\n[3,]    0    0    1 0.00  6.00    0\n\n\n\nTake the general solution and write as a linear combination of vectors where the coefficients are the free variables.\ngeneral solution \\(x_1 = -1.5 x_4 + 4.5 x_5\\), \\(x_2 = -7.25 x_4 - 2.75 x_5\\), \\(x_3 = -6 x_5\\) and both \\(x_4\\) and \\(x_5\\) are free. Write out the general solution in vector form.\n\n\\[\n\\begin{aligned}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{pmatrix} = \\begin{pmatrix} -1.5 x_4 + 4.5 x_5\\\\ -7.25 x_4 - 2.75 x_5 \\\\ -6 x_5 \\\\ x_4 \\\\ x_5 \\end{pmatrix} = x_4 \\begin{pmatrix} -1.5 \\\\ -7.25 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + x_5 \\begin{pmatrix} 4.5 \\\\ 2.75 \\\\ -6 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\n\nFrom above, the free variables \\(x_4\\) and \\(x_5\\) are multiplied by the vectors \\(\\mathbf{u} = \\begin{pmatrix} -1.5 \\\\ -7.25 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 4.5 \\\\ 2.75 \\\\ -6 \\\\ 0 \\\\ 1 \\end{pmatrix}\\) where \\(\\{ \\mathbf{u}, \\mathbf{v} \\}\\) are a spanning set for the null(\\(\\mathbf{A}\\))\n\n\n\n\nExample 15.2 \nIn class – do another Find a spanning set for null(\\(\\mathbf{A}\\)) where\n\n\n\n\n15.1.2 Column space\n\nThe columns space col(\\(\\mathbf{A}\\)) of an \\(m \\times n\\) \\(\\mathbf{A}\\) is the set of all linear combinations of the columns of \\(\\mathbf{A}\\).\nIf \\(\\{ \\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}\\) are the columns of \\(\\mathbf{A}\\), then\n\\[\n\\begin{aligned}\n\\mbox{col}(\\mathbf{A}) = \\mbox{span}(\\mathbf{A})\n\\end{aligned}\n\\]\nthis can be written in set notation as\n\\[\n\\begin{aligned}\n\\mbox{col}(\\mathbf{A}) = \\{ \\mathbf{b} : \\mathbf{A} \\mathbf{x} = \\mathbf{b} \\mbox{ for some } \\mathbf{x} \\in \\mathcal{R}^n \\}\n\\end{aligned}\n\\]\n\n\nTheorem 15.2 The column space of an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) is a subspace of \\(\\mathcal{R}^n\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nDo in class\n\n\\(\\mathbf{0}\\) vector\nsum of vectors\nscalar multiplication\n\n\n\n\nCompared to the null space, the column space is defined explicitly–it is the span of the columns of \\(\\mathbf{A}\\). The definition of the column space results in the fact that col(\\(\\mathbf{A}\\)) is the range of the linear transformation \\(\\mathbf{x} \\rightarrow \\mathbf{A} \\mathbf{x}\\).\n\n\nExample 15.3 \nIn class\n\nFind a spanning set for col(\\(\\mathbf{A}\\)) where\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} 6 & 0 & 4 \\\\ 5 & -1 & -9 \\\\ -4 & 7 & 4 \\\\ 6 & 2 & 9 \\end{pmatrix}\n\\end{aligned}\n\\]\n\n\n\n15.1.3 Understanding the differerneces between the column space and the null space"
  },
  {
    "objectID": "15-linearly-independent-sets-bases.html",
    "href": "15-linearly-independent-sets-bases.html",
    "title": "16  Linearly independent sets and bases",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dasc2594)\nRecall that a set of vectors \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) is linearly independent if the only solution to the system of equations\n\\[\n\\begin{aligned}\nx_1 \\mathbf{v}_1 + \\cdots + x_n \\mathbf{v}_n = \\mathbf{0}\n\\end{aligned}\n\\]\nis the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\). In other words, it is not possible to write any of the vectors in the set \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) as a linear combination of the other vectors."
  },
  {
    "objectID": "15-linearly-independent-sets-bases.html#bases-for-nullmathbfa-and-colmathbfa",
    "href": "15-linearly-independent-sets-bases.html#bases-for-nullmathbfa-and-colmathbfa",
    "title": "16  Linearly independent sets and bases",
    "section": "16.1 Bases for null(\\(\\mathbf{A}\\)) and col(\\(\\mathbf{A}\\))",
    "text": "16.1 Bases for null(\\(\\mathbf{A}\\)) and col(\\(\\mathbf{A}\\))\n\nExample 16.3 Find a basis for the col(\\(\\mathbf{A}\\)) where\n\nset.seed(2021)\nA <- matrix(sample(-9:9, 15, replace = TRUE), 5, 3)\n\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} -3 & -4 & 5 \\\\ -4 & -4 & -3 \\\\ 4 & -4 & -1 \\\\ -3 & 4 & 2 \\\\ 2 & -5 & 9 \\end{pmatrix}\n\\end{aligned}\n\\]\n\nCalculate row echelon form and identify the pivot columns. The vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\) that make up the columns of \\(\\mathbf{A}\\) that are in the pivot columns form a basis for \\(\\mathbf{A}\\)\nWhy is this? Think about the relationship between the columns of \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\) in \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) that result in a consistent solution.\n\n\n\nExample 16.4 Find a basis for the null(\\(\\mathbf{A}\\)) where\n\nset.seed(2021)\nA <- matrix(sample(-9:9, 15, replace = TRUE), 5, 3)\n\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} -3 & -4 & 5 \\\\ -4 & -4 & -3 \\\\ 4 & -4 & -1 \\\\ -3 & 4 & 2 \\\\ 2 & -5 & 9 \\end{pmatrix}\n\\end{aligned}\n\\]\n\nCalculate solutions to homogeneous system of equations, write solution in vector equation form. Vectors form a basis for null(\\(\\mathbf{A}\\))\n\n\n\nnote: Facts about the basis for the null space null(\\(\\mathbf{A}\\))\n\n\nThe spanning set produced using the method above produces a linearly independent set because the free variables are weights on the spanning vectors.\nWhen null(\\(\\mathbf{A}\\)) contains nonzero vectors, the number of vectors in the spanning set for null(\\(\\mathbf{A}\\)) is the number of free variables in the solution of \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\).\n\n\nExample 16.5 The matrix \\(4 \\times 5\\) \\(\\mathbf{A}\\) has columns given by the vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_5\\) and is row equivalent to the matrix\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} 1 & 0 & 3 & -2 & 1 \\\\ 0 & 0 & 3 & -2 & 5 \\\\ 0 & 0 & 0 & -1 & -2 \\\\ 0 & 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nWhat is a basis for col(\\(\\mathbf{A}\\)) in terms of the vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_5\\)\n\n\nNote that two matrices that are row equivalent have the same linear dependence relationsihps between their vectors (but the basis for their column space is different)\n\n\nExample 16.6 The matrix \\(\\mathbf{A}\\) is row equivalent to the matrix \\(\\mathbf{B}\\)\n\nA <- matrix(c(1, 3, 2, 5, 4, 12 , 8, 20, 0, 1, 1, 2, 2, 5, 3, 8, -1, 5, 2, 8), 4, 5)\nB <- rref(A)\n\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} 1 & 4 & 0 & 2 & -1 \\\\ 3 & 12 & 1 & 5 & 5 \\\\ 2 & 8 & 1 & 3 & 2 \\\\ 5 & 20 & 2 & 8 & 8 \\end{pmatrix} & \\mathbf{B} = \\begin{pmatrix} 1 & 4 & 0 & 2 & 0 \\\\ 0 & 0 & 1 & -1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 0 \\end{pmatrix} \\\\\n\\end{aligned}\n\\]\n\nWhat is a basis for col(\\(\\mathbf{A}\\))?\nWhat is a basis for col(\\(\\mathbf{B}\\))?\nWhat is span(\\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_5\\))?\nWhat is span(\\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_5\\))?\nAre the spaces spanned by the columns of \\(\\mathbf{A}\\) and the columns of \\(\\mathbf{B}\\) the same space?\n\n\n\nTheorem 16.2 The pivot columns of a matrix \\(\\mathbf{A}\\) for a basis for col(\\(\\mathbf{A}\\))\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nsketch: \\(\\mathbf{B}\\) rref of \\(\\mathbf{A}\\), linearly independent columns of \\(\\mathbf{B}\\) are same as linearly independent columns in \\(\\mathbf{A}\\). Other (non-pviot) columns are linearly dependent. By spanning set theorem, non-pivot columns can be removed from the spanning set without changing the span, leaving only the pivot columns of \\(\\mathbf{A}\\) as a basis for col($)"
  },
  {
    "objectID": "16-coordinate-systems.html",
    "href": "16-coordinate-systems.html",
    "title": "17  Coordinate Systems and Dimension",
    "section": "",
    "text": "3 Blue 1 Brown – Change of basis\nWe already know about the cartesian coordinate system (x, y, z) which has the set of basis vectors\n\\[\n\\begin{aligned}\n\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} &&\n\\mathbf{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} &&\n\\mathbf{e}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\nHowever, using the concept of a basis for a subspace \\(\\mathcal{H}\\) of some vector space \\(\\mathcal{V}\\), we might want to use a different basis. Luckily, we have learned how to construct bases for col(\\(\\mathbf{A}\\)) and null(\\(\\mathbf{A}\\)).\nYou might be wondering why we want to create different bases. The usual cartesian basis has been good enough for me so far (unless you have used polar coordinates). In data science, the data often live in a high dimensional space (i.e., there are a number of data variables). However, while the data might have many variables, some of these variables are partially dependent and thus the space in which the data are embedded might be well approximated using a subspace of the original variables which can increase computation speed (less computation with fewer variables – recall from lab how the inverse of \\(\\mathbf{X}'\\mathbf{X}\\) took much much longer with larger numbers of variables). Thus, understanding different coordinate systems and how to change coordinate systems can lead to more efficient data representation and model fitting."
  },
  {
    "objectID": "16-coordinate-systems.html#coordinates-in-mathcalrn",
    "href": "16-coordinate-systems.html#coordinates-in-mathcalrn",
    "title": "17  Coordinate Systems and Dimension",
    "section": "17.1 Coordinates in \\(\\mathcal{R}^n\\)",
    "text": "17.1 Coordinates in \\(\\mathcal{R}^n\\)\nLet \\(\\mathbf{x}\\) be defined with the standard coordinates. Let \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n \\}\\) be a basis in \\(\\mathcal{R}^n\\). Define \\(\\mathbf{B} = \\begin{pmatrix} \\mathbf{b}_1 & \\cdots & \\mathbf{b}_n \\end{pmatrix}\\) as the matrix with columns the vectors of the basis. Then, the coordinates \\(\\left[\\mathbf{x}\\right]_B = \\begin{pmatrix} [x_1]_B \\\\ \\vdots \\\\ [x_n]_B \\end{pmatrix}\\) of \\(\\mathbf{x}\\)\nwith respect to the basis \\(\\mathcal{B}\\) can be found by solving the matrix equation\n\\[\n\\begin{aligned}\n\\mathbf{B} \\left[\\mathbf{x}\\right]_B = \\mathbf{x}\n\\end{aligned}\n\\]\nThe matrix \\(\\mathbf{B}\\) is called the change-of-coordinates matrix from \\(\\mathcal{B}\\) to the standard basis in \\(\\mathcal{R}^n\\). The solution set (the coefficients) \\(\\left[\\mathbf{x}\\right]_B\\) can be found using row operations or by using the fact that because the columns of \\(\\mathbf{B}\\) spans \\(\\mathcal{R}^n\\) the matrix \\(\\mathbf{B}\\) is invertible. Then, the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\) is\n\\[\n\\begin{aligned}\n\\left[\\mathbf{x}\\right]_B = \\mathbf{B}^{-1} \\mathbf{x}\n\\end{aligned}\n\\]\n\nTheorem 17.2 Let \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) be a basis for the vector space \\(\\mathcal{V}\\). Then, the coordinate mapping \\(\\mathbf{x} \\rightarrow \\mathbf{B}^{-1} \\mathbf{x}\\) is a one-to-one and onto transformation from \\(\\mathcal{V}\\) to \\(\\mathcal{R}^n\\)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst we want to show that multiplication by \\(\\mathbf{B}^{-1}\\) defines a linear transformation. First, take two vectors\n\\[\n\\begin{aligned}\n\\mathbf{u} &  = c_1 \\mathbf{b}_1 + \\ldots + c_n \\mathbf{b}_n\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\mathbf{v} = d_1 \\mathbf{b}_1 + \\ldots + d_n \\mathbf{b}_n\n\\end{aligned}\n\\]\n\nFirst, we show the mapping preserves vector addition\n\n\\[\n\\begin{aligned}\n\\mathbf{u} + \\mathbf{v} \\rightarrow \\mathbf{B}^{-1} (\\mathbf{u} + \\mathbf{v}) =  \\mathbf{B}^{-1} \\mathbf{u} + \\mathbf{B}^{-1} \\mathbf{v}\n\\end{aligned}\n\\]\nwhich preserves vector addition\n\nNext, we show the mapping preserves scalar multiplication. Given scalar \\(a\\),\n\n\\[\n\\begin{aligned}\na\\mathbf{u} \\rightarrow \\mathbf{B}^{-1} (a \\mathbf{u}) =  a \\mathbf{B}^{-1} \\mathbf{u}\n\\end{aligned}\n\\]\nwhich preserves scalar multiplication.\n\nTherefore, this is a linear transformation. one-to-one and onto come from fact that \\(\\mathbf{B}\\) is and \\(n \\times n\\) matrix with \\(n\\) pivot columns (\\(n\\) linearly independent vectors because it is a basis for \\(\\mathcal{R}^n\\))\n\n\n\n\n\nExample 17.3 in class–Give basis in \\(\\mathcal{R}^4\\), find coefficients with respect to this basis for the vector \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "16-coordinate-systems.html#dimension-of-a-vector-space",
    "href": "16-coordinate-systems.html#dimension-of-a-vector-space",
    "title": "17  Coordinate Systems and Dimension",
    "section": "17.2 Dimension of a vector space",
    "text": "17.2 Dimension of a vector space\nIn some sense, we already know about the dimension of a vector space through the concept of a span. The span of a set of vectors defines the dimension of the vector space.\n\nTheorem 17.3 In a vector space \\(\\mathcal{V}\\) with basis \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\), any set in \\(\\mathcal{V}\\) containing more than \\(n\\) vectors must be linearly dependent.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_p \\}\\) be a set of vectors in \\(\\mathcal{V}\\) with \\(p > n\\). The coordinate vectors \\(\\{\\mathbf{B} \\mathbf{u}_1, \\ldots, \\mathbf{B} \\mathbf{u}_p\\}\\) form a linearly dependent set in \\(\\mathcal{R}^n\\) because there are more vectors (\\(p\\)) than entries (\\(n\\)) in each vector. Thus, there exist scalars \\(c_1, \\ldots, c_p\\), some nonzero, such that\n\\[\n\\begin{aligned}\nc_1 \\mathbf{B} \\mathbf{u}_1 + \\ldots + c_p \\mathbf{B} \\mathbf{u}_p = \\mathbf{0}.\n\\end{aligned}\n\\]\nwhich by linearity implies\n\\[\n\\begin{aligned}\n\\mathbf{B} (c_1 \\mathbf{u}_1 + \\ldots + c_p \\mathbf{u}_p) = \\mathbf{0}\n\\end{aligned}\n\\]\n\n\n\nBecause the matrix \\(\\mathbf{B}\\) is a \\(n \\times n\\) matrix with n linearly independent columns, the only way the equation above can equal \\(\\mathbf{0}\\) is if the vector \\(c_1 \\mathbf{u}_1 + \\ldots + c_p \\mathbf{u}_p = \\mathbf{0}\\) (by the invertible matrix theorem). Therefore, the set of vectors \\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_p \\}\\) is linearly dependent because there are coefficients that allow the vectors to sum to \\(\\mathbf{0}\\). Thus, we know that for a vector space \\(\\mathcal{V}\\) that has a basis \\({\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n \\}}\\) that consists on \\(n\\) vectors, then every linearly independent set of vectors in \\(\\mathcal{V}\\) contains at most \\(n\\) vectors.\n\nTheorem 17.4 If a vector space \\({\\mathcal{V}}\\) has a basis with \\(n\\) vectors, then every other basis of \\({\\mathcal{V}}\\) must also contain exactly \\(n\\) vectors.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(\\mathcal{B}_1\\) be a basis of \\({\\mathcal{V}}\\) containing \\(n\\) vectors and let \\(\\mathcal{B}_2\\) be any other basis of \\({\\mathcal{V}}\\). Because \\(\\mathcal{B}_1\\) and \\(\\mathcal{B}_2\\) are both bases, they both contain sets of linearly independent vectors. As such, the previous theorem states that each of these bases contain at most \\(n\\) vectors (otherwise the sets wouldn’t be linearly independent). Because \\(\\mathcal{B}_2\\) is a basis and the basis \\(\\mathcal{B}_1\\) contains \\(n\\) vectors, \\(\\mathcal{B}_2\\) must contain at least \\(n\\) vectors. These results combined are only satisfied when \\(\\mathcal{B}_2\\) contains \\(n\\) vectors.\n\n\n\nLike the span defined by the columns of a matrix \\(\\mathbf{A}\\), there is an abstract concept called dimension which measures the “size” of a vector space.\n\nDefinition 17.2 If \\({\\mathcal{V}}\\) is spanned by a finite set of vectors, then \\({\\mathcal{V}}\\) is said to be finite dimensional. If \\({\\mathcal{V}}\\) is not spanned by a finite set of vectors, \\({\\mathcal{V}}\\) is said to be infinite dimensional. The smallest set of vectors that spans \\({\\mathcal{V}}\\) is a basis for \\({\\mathcal{V}}\\) and the number of vectors in this basis is called the dimension of \\({\\mathcal{V}}\\) and written as dim(\\({\\mathcal{V}}\\)). If \\({\\mathcal{V}}= \\{\\mathbf{0}\\}\\), then dim(\\({\\mathcal{V}}\\)) is said to be 0.\n\n\nExample 17.4 in class - span of 2 or 3 linearly independent vectors\n\nspan, dim, and geometry\n\n\n\nExample 17.5 in class - span of 2 or 3 linearly dependent vectors\n\nspan, dim, and geometry"
  },
  {
    "objectID": "16-coordinate-systems.html#subspaces-of-finite-dimension",
    "href": "16-coordinate-systems.html#subspaces-of-finite-dimension",
    "title": "17  Coordinate Systems and Dimension",
    "section": "17.3 Subspaces of finite dimension",
    "text": "17.3 Subspaces of finite dimension\n\nTheorem 17.5 Let \\(\\mathcal{H}\\) be a subspace of a finite-dimensional vector space \\({\\mathcal{V}}\\). Then, any linearly independent set in \\(\\mathcal{H}\\) can be expanded, if necessary to form a basis for \\(\\mathcal{H}\\). As \\(\\mathcal{H}\\) is a subspace of the finite-dimensional vector space \\({\\mathcal{V}}\\), \\(\\mathcal{H}\\) is a finite-dimensional vector space with\n\\[\n\\begin{aligned}\n\\mbox{dim}(\\mathcal{H}) \\leq \\mbox{dim}(\\mathcal{V})\n\\end{aligned}\n\\]\n\nFor a vector space of known dimension \\(p\\), finding a basis can be simplified by finding a linearly independent set of size \\(p\\).\n\nTheorem 17.6 (The Basis Theorem) Let \\(\\mathcal{V}\\) be a \\(p\\) dimensional vector space with \\(p \\geq 1\\). Any linearly independent subset of \\(p\\) vectors is a basis for \\(\\mathcal{V}\\). Equivalently, any set of \\(p\\) vectors that span \\({\\mathcal{V}}\\) is automatically a basis for \\(\\mathcal{V}\\)."
  },
  {
    "objectID": "16-coordinate-systems.html#dimensions-of-nullmathbfa-and-colmathbfa",
    "href": "16-coordinate-systems.html#dimensions-of-nullmathbfa-and-colmathbfa",
    "title": "17  Coordinate Systems and Dimension",
    "section": "17.4 Dimensions of null(\\(\\mathbf{A}\\)) and col(\\(\\mathbf{A}\\))",
    "text": "17.4 Dimensions of null(\\(\\mathbf{A}\\)) and col(\\(\\mathbf{A}\\))\nThe dimension of null(\\(\\mathbf{A}\\)) are the number of free variables in \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) and the dimension of col(\\(\\mathbf{A}\\)) is the number of pivot columns of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "17-rank.html",
    "href": "17-rank.html",
    "title": "18  Rank",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dasc2594)\nNote: the row space of \\(\\mathbf{A}\\) is the column space of the transposed matrix \\(\\mathcal{A}'\\).\n\\[\n\\begin{aligned}\nrow(\\mathbf{A}) = col(\\mathbf{A}')\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "17-rank.html#rank",
    "href": "17-rank.html#rank",
    "title": "18  Rank",
    "section": "18.1 Rank",
    "text": "18.1 Rank\n\nDefinition 18.2 The rank of a matrix \\(\\mathbf{A}\\), rank(\\(\\mathbf{A}\\)) is the dimension of the column space col(\\(\\mathbf{A}\\))\n\n\nTheorem 18.1 (The Rank Theorem) Let \\(\\mathbf{A}\\) be an \\(m \\times n\\) matrix. Then the dimension of are equal. The rank of \\(\\mathbf{A}\\) equals the number of pivot columns of \\(\\mathbf{A}\\) and\n\\[\n\\begin{aligned}\nrank (\\mathbf{A}) + dim(null(\\mathbf{A})) = n\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe rank(\\(\\mathbf{A}\\)) is the number of pivot columns and dim(null(\\(\\mathbf{A}\\))) is the number of non-pivot columns. The number of pivot columns (rank(\\(\\mathbf{A}\\))) + the number of non-pivot columns (dim(null(\\(\\mathbf{A}\\)))) are the number of columns.\n\n\n\n\nExample 18.3 in class\n\\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix with dim(null(\\(\\mathbf{A}\\))) = p. What is rank(\\(\\mathbf{A}\\))\n\n\nExample 18.4 \\(\\mathbf{A}\\) is a 6x9 matrix. Is it possible for null(\\(\\mathbf{A}\\)) = 2?\n\n\nTheorem 18.2 (Invertible Matrix Theorm + Rank) This is an extension of the prior statement of the invertible matrix theorem Theorem 9.5 Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix. Then the following statements are equivalent (i.e., they are all either simultaneously true or false).\n13) The columns of \\(\\mathbf{A}\\) form a basis of \\(\\mathcal{R}^n\\)\n14) col(\\(\\mathbf{A}\\)) = \\(\\mathcal{R}^n\\)\n15) dim(col(\\(\\mathbf{A}\\))) = \\(n\\)\n16) rank(\\(\\mathbf{A}\\)) = \\(n\\)\n17) null(\\(\\mathbf{A}\\)) = \\(\\{\\mathbf{0}\\}\\)\n18) dim(null(\\(\\mathbf{A}\\))) = 0"
  },
  {
    "objectID": "18-change-of-basis.html",
    "href": "18-change-of-basis.html",
    "title": "19  Change of basis",
    "section": "",
    "text": "3 Blue 1 Brown – Change of basis\nConsider two bases \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n \\}\\) and \\(\\mathcal{C} = \\{ \\mathbf{c}_1, \\ldots, \\mathbf{c}_n \\}\\) for a vector space \\(\\mathcal{V}\\). If we have a vector \\(\\left[\\mathbf{x}\\right]_B\\) with coordinates in \\(\\mathcal{B}\\), what are the coordinates of \\(\\left[\\mathbf{x}\\right]_C\\) with respect to \\(\\mathcal{C}\\)?\nNotice that change of coordinates is a linear transformation from \\(\\mathcal{B}\\) to \\(\\mathcal{C}\\) with transformation matrix \\(\\mathbf{A}\\). Despite the more complex notation, this is just another linear transformation [link].\n::: {#exm-} Show the change of basis from the basis \\(\\mathcal{B} = \\left\\{ \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\\right\\}\\) to the basis \\(\\mathcal{C} = \\left\\{ \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ -\\frac{1}{2} \\end{pmatrix}\\right\\}\\). To do this, represent the columns that make up the basis \\(\\mathcal{B}\\) as the matrix \\(\\mathbf{B} = \\begin{pmatrix} \\frac{1}{2} & -1 \\\\ 1 & 0 \\end{pmatrix}\\) and represent the columns that make up the basis \\(\\mathcal{C}\\) as the matrix \\(\\mathbf{C} = \\begin{pmatrix} 0 & \\frac{1}{2} \\\\ 1 & -\\frac{1}{2} \\end{pmatrix}\\). Then, the change of basis can be represented as\nwhich can be represented with the static images\nThe change of basis represents a linear transformation. When previously discussing linear transformations in Chapter 7, we considered a linear transformation from the standard basis \\(\\mathcal{I}\\) defined by the basis vectors \\(\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right\\}\\) with the vectors represented as the columns of the identity matrix \\(\\mathbf{I}\\). We can consider a change of basis as two consecutive linear transformations. First, a linear transformation from the basis \\(\\mathcal{B}\\) to the standard basis \\(\\mathcal{I}\\) and then a linear transformation from the standard basis \\(\\mathcal{I}\\) to the basis \\(\\mathcal{C}\\). This can be represented using the following example code:"
  },
  {
    "objectID": "18-change-of-basis.html#changing-coordinates-between-different-bases",
    "href": "18-change-of-basis.html#changing-coordinates-between-different-bases",
    "title": "19  Change of basis",
    "section": "19.1 Changing coordinates between different bases",
    "text": "19.1 Changing coordinates between different bases\nNow, we can combine these ideas. Given a vector \\(\\left[\\mathbf{x}\\right]_B\\) written with coordinates with respect to the basis \\(\\mathcal{B}\\), we can find the coordinates of \\(\\left[\\mathbf{x}\\right]_C\\) with respect to the basis \\(\\mathcal{C}\\). First, we find the coordinates of the vector \\(\\mathbf{x}\\) with respect to the standard basis then find the coordinates of \\(\\left[\\mathbf{x}\\right]_C\\) with respect to the basis \\(\\mathcal{C}\\). Let \\(\\mathbf{B} = \\begin{pmatrix} \\mathbf{b}_1 & \\ldots & \\mathbf{b}_n \\end{pmatrix}\\) and \\(\\mathbf{C} = \\begin{pmatrix} \\mathbf{c}_1 & \\ldots & \\mathbf{c}_n \\end{pmatrix}\\), then given a vector \\(\\left[\\mathbf{x}\\right]_B\\) with coordinates with respect to the basis \\(\\mathcal{B}\\), the coordinates \\(\\left[\\mathbf{x}\\right]_C\\) of this vector with respect to the basis \\(\\mathcal{C}\\) is \\[\n\\begin{aligned}\n\\left[\\mathbf{x}\\right]_C = \\mathbf{C}^{-1} \\mathbf{B} \\left[\\mathbf{x}\\right]_B.\n\\end{aligned}\n\\]\nDraw diagram\n::: {.callout-note icon=false collapse=“true” appearance=“simple”} ## Solution\n\n\nExample 19.1 \n\nWorking with the same bases \\(\\mathcal{B} = \\left\\{ \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\\right\\}\\) and \\(\\mathcal{C} = \\left\\{ \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ -\\frac{1}{2} \\end{pmatrix}\\right\\}\\) from the previous example, Let \\(\\left[\\mathbf{x}\\right]_B = \\begin{pmatrix} -3/2 \\\\ 1/2 \\end{pmatrix}\\) be the coordinates of the vector \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B} = \\left\\{ \\begin{pmatrix} 1/2 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\\right\\}\\). Find\n\nthe coordinates of \\(\\mathbf{x}\\) with respect to the standard basis and\nthe coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{C}\\).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere we solve the two questions from the example above.\n\nRecall that the coordinates \\(\\left[\\mathbf{x}\\right]_B\\) of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\) mean that the vector \\(\\mathbf{x}\\) can be written as a linear combination of the basis vectors \\(\\mathbf{b}_1\\) and \\(\\mathbf{b}_2\\) with coefficients given by the elements in \\(\\left[\\mathbf{x}\\right]_B = \\begin{pmatrix} \\left[x_1\\right]_B \\\\ \\left[x_2\\right]_B \\end{pmatrix}\\). This results in the equation\n\n\\[\n\\begin{aligned}\n\\mathbf{x} & = \\left[x_1\\right]_B \\mathbf{b}_1 + \\left[x_2\\right]_B \\mathbf{b}_2\n\\end{aligned}\n\\] Plugging the values from the example gives \\[\n\\begin{aligned}\n\\mathbf{x} & = \\left[x_1\\right]_B \\mathbf{b}_1 + \\left[x_2\\right]_B \\mathbf{b}_2 \\\\\n& = -1.5 \\begin{pmatrix} 1/2 \\\\ 1 \\end{pmatrix} + 0.5 \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} \\\\\n& = \\begin{pmatrix} -3/4 \\\\ -3/2 \\end{pmatrix} + \\begin{pmatrix} -1/2 \\\\ 0 \\end{pmatrix} \\\\\n& = \\begin{pmatrix} -5/4 \\\\ -3/2 \\end{pmatrix}\n\\end{aligned}\n\\] 2) Now, recall the coordinates \\(\\left[\\mathbf{x}\\right]_C\\) of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{C}\\) mean that the vector \\(\\mathbf{x}\\) can be written as a linear combination of the basis vectors \\(\\mathbf{c}_1\\) and \\(\\mathbf{c}_2\\) with coefficients given by the elements in \\(\\left[\\mathbf{x}\\right]_C = \\begin{pmatrix} \\left[x_1\\right]_C \\\\ \\left[x_2\\right]_C \\end{pmatrix}\\). This results in the equation\n\\[\n\\begin{aligned}\n\\mathbf{x} & = \\left[x_1\\right]_C \\mathbf{c}_1 + \\left[x_2\\right]_C \\mathbf{c}_2\n\\end{aligned}\n\\] However, unlike part (1), we do not know the coefficients \\(\\left[\\mathbf{x}\\right]\\) but need to solve for them. Rewriting the above equation in the form of \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) gives \\[\n\\begin{aligned}\n\\mathbf{C} \\left[ \\mathbf{x} \\right]_C & = \\mathbf{x}\n\\end{aligned}\n\\] Because the matrix of basis vectors \\(\\mathbf{C}\\) is an invertible matrix (a basis is a linearly independent spanning set), the coefficients \\(\\left[\\mathbf{x}\\right]_C\\) can be solved using the equation \\[\n\\begin{aligned}\n\\mathbf{C} \\left[ \\mathbf{x} \\right]_C & = \\mathbf{x} \\\\\n\\mathbf{C}^{-1} \\mathbf{C}\\left[ \\mathbf{x} \\right]_C & = \\mathbf{C}^{-1}\\mathbf{x} \\\\\n\\left[ \\mathbf{x} \\right]_C & = \\mathbf{C}^{-1} \\mathbf{x}\n\\end{aligned}\n\\] The matrix inverse \\(\\mathbf{C}^{-1}\\) can be found using Theorem 9.1 to get \\(\\mathbf{C}^{-1} = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix}\\). Then, plugging in the values from the example gives\n\\[\n\\begin{aligned}\n\\left[ \\mathbf{x} \\right]_C & = \\mathbf{C}^{-1} \\mathbf{x} \\\\\n& = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} -5/4 \\\\ -3/2 \\end{pmatrix} \\\\\n& = \\begin{pmatrix} -17/4 \\\\ -5/4 \\end{pmatrix}\n\\end{aligned}\n\\] Another way to change coordinates is to change directly from basis \\(\\mathcal{B}\\) to \\(\\mathcal{C}\\) without going through the intermediate transformation to the standard coordinates. Combining the results from (1) and (2) gives \\[\n\\begin{aligned}\n\\left[ \\mathbf{x} \\right]_C & = \\mathbf{C}^{-1} \\mathbf{x} \\\\\n& = \\mathbf{C}^{-1} \\mathbf{B} \\left[\\mathbf{x}\\right]_B\n\\end{aligned}\n\\] so that one can change coordinates from the basis \\(\\mathcal{B}\\) to the basis \\(\\mathcal{C}\\) using the linear transformation defined by the matrix multiplication \\(\\mathbf{C}^{-1} \\mathbf{B}\\).\nIn R, first define the basis matrices B and C and the coordinates x_b of the vector \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\).\n\nB <- matrix(c(1/2, 1, -1, 0), 2, 2)\nC <- matrix(c(0, 1/2, 1, -1/2), 2, 2)\nx_b <- c(-3/2, 1/2)\n\n1) The coordinates x with respect to the standard basis is\n\nx <- B %*% x_b\nx\n\n      [,1]\n[1,] -1.25\n[2,] -1.50\n\n\n2) The coordinates x_c with respect to the basis \\(\\mathcal{C}\\) can be found by calculating the matrix inverse C_inv and then using the matrix inverse to calculate the coordinates with respect to the basis \\(\\mathcal{C}\\) as\n\nC_inv <- solve(C)\nx_c <- C_inv %*% x\nx_c\n\n      [,1]\n[1,] -4.25\n[2,] -1.25\n\n\nDone as a single transformation, the linear transformation is defined as\n\nB %*% C_inv\n\n     [,1] [,2]\n[1,] -0.5    1\n[2,]  1.0    2\n\n\nwhich gives the coordinates\n\nB %*% C_inv %*% x_b\n\n      [,1]\n[1,]  1.25\n[2,] -0.50\n\n\n\n\n\n\nExample 19.2 3-d change of basis"
  },
  {
    "objectID": "19-eigenvectors-and-eigenvalues.html",
    "href": "19-eigenvectors-and-eigenvalues.html",
    "title": "20  Eigenvectors and Eigenvalues",
    "section": "",
    "text": "3 Blue 1 Brown – Eigenvalues\nWe have just learned about change of basis in an abstract sense. Now, we will learn about a special change of basis that is “data-driven” called an eigenvector. Eigenvectors and the corresponding eigenvalues are a vital tool in data science for data compression and modeling.\nThus, we end up with the understanding that nn eigenvector is a (nonzero) vector \\(\\mathbf{x}\\) that gets mapped to a scalar multiple of itself \\(\\lambda \\mathbf{x}\\) by the matrix transformation defined by \\(T: \\mathbf{x} \\rightarrow \\mathbf{A}\\mathbf{x} = \\mathbf{x}\\). As such, when \\(\\mathbf{x}\\) is an eigenvector of \\(\\mathbf{A}\\) we say that \\(\\mathbf{x}\\) and \\(\\mathbf{A} \\mathbf{x}\\) are collinear with the origin (\\(\\mathbf{0}\\)) and each other in the sense that these points lie on the same line that goes through the origin.\nNote: The matrix \\(\\mathbf{A}\\) must be an \\(n \\times n\\) square matrix. A similar decomposition (called the singular value decomposition) can be used for rectangular matrices."
  },
  {
    "objectID": "19-eigenvectors-and-eigenvalues.html#eigenspaces",
    "href": "19-eigenvectors-and-eigenvalues.html#eigenspaces",
    "title": "20  Eigenvectors and Eigenvalues",
    "section": "20.1 Eigenspaces",
    "text": "20.1 Eigenspaces\nGiven a square \\(n \\times n\\) matrix \\(\\mathbf{A}\\), we know how to check if a given vector \\(\\mathbf{x}\\) is an eigenvector and then how to find the eigenvalue associated with that eigenvector. Next, we want to check if a given number is an eigenvalue of \\(\\mathbf{A}\\) and to find all the eigenvectors corresponding to that eigenvalue.\nGiven a square \\(n \\times n\\) matrix \\(\\mathbf{A}\\) and a scalar \\(\\lambda\\), the eigenvectors of \\(\\mathbf{A}\\) associated with the scalar \\(\\lambda\\) (if there are eigenvectors associated with \\(\\lambda\\)) are the nonzero solutoins to the equation \\(\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}\\). This can be written as\n\\[\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x} & = \\lambda \\mathbf{x} \\\\\n\\mathbf{A} \\mathbf{x} -\\lambda \\mathbf{x} & = \\mathbf{0} \\\\\n\\mathbf{A} \\mathbf{x} -\\lambda \\mathbf{I} \\mathbf{x} & = \\mathbf{0} \\\\\n\\left( \\mathbf{A} -\\lambda \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}. \\\\\n\\end{aligned}\n\\]\nTherefore, the eigenvectors of \\(\\mathbf{A}\\) associated with \\(\\lambda\\), if there are any, are the nontrivial solutions of the homogeneous matrix equation \\(\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}\\). In other words, the eigenvectors are the nonzero vectors in the null space null\\(\\left( \\mathbf{A} -\\lambda \\mathbf{I} \\right)\\). If there is not a nontrivial solution (solution \\(\\mathbf{x} \\neq \\mathbf{0}\\)), then \\(\\lambda\\) is not an eigenvalue of \\(\\mathbf{A}\\).\nHey, we know how to find solutions to homogeneous systems of equations! Thus, we know how to find the eigenvectors of \\(\\mathbf{A}\\). All we have to do is solve the system of linear equations \\(\\left( \\mathbf{A} -\\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}\\) for a given \\(\\lambda\\) (actually, for all \\(\\lambda\\)s, which we can’t do). If only there was some way to find eigenvalues \\(\\lambda\\) (hint: there is and it is coming next chapter).\n\n\nExample 20.4 \n\nLet \\(\\mathbf{A} = \\begin{pmatrix} 3 & 6 & -8 \\\\ 0 & 0 & 6 \\\\ 0 & 0 & 2 \\end{pmatrix}\\). Then an eigenvector with eigenvector \\(\\lambda\\) is a nontrival solution to\n\\[\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n3  - \\lambda & 6 & -8 \\\\\n0 & 0 - \\lambda & 6 \\\\\n0 & 0 & 2 - \\lambda\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich can be solved for a given \\(\\lambda\\) using an augmented matrix form and row operations to reduce to reduced row echelon form.\nLetting \\(\\lambda = 3\\), we have\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n3  - 3 & 6 & -8 \\\\\n0 & 0 - 3 & 6 \\\\\n0 & 0 & 2 - 3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich can be written as the matrix equation\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 0 & 6 & -8 \\\\ 0 & -3 & 6 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nNote that the columns of the matrix above are not linearly independent. Thus, we can solve a non-unique solution (the solution set is a line going through the origin) by finding the reduce row echelon form of an augmented matrix\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 0 & 6 & -8 & 0 \\\\ 0 & -3 & 6 & 0 \\\\ 0 & 0 & -1 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich has solution\n\\[\n\\begin{aligned}\nx_1 & = x_1 \\\\\nx_2 & = 0 \\\\\nx_3 & = 0\n\\end{aligned}\n\\]\nFixing \\(x_1 = 1\\) gives the eigenvector associated with \\(\\lambda = 3\\) of \\(\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\). We can verify that this is an eigenvector with matrix multiplication\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 3 & 6 & -8 \\\\ 0 & 0 & 6 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} & = \\begin{pmatrix} 3 \\\\ 0 \\\\ 0 \\end{pmatrix}  = 3 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nUsing R, this can be done as\n\n\n\n\nlambda <- 3\n# apply rref to the augmented matrix\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    1    0    0\n[2,]    0    0    1    0\n[3,]    0    0    0    0\n\n\nwhere the solution set is determined from the RREF form of the augmented matrix of the equation \\(\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}\\)\n\n\n\nExample 20.5 \n\nLet \\(\\mathbf{A} = \\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix}\\). Find the eigenvectors associated with the eigenvalues (a) \\(\\lambda_1 = -4\\), (b) \\(\\lambda_2 = 3\\), and (c) \\(\\lambda_3 = -1\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGiven the matrix \\(\\mathbf{A} = \\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix}\\), we can find the eigenvectors associated with the given eigenvalues\n\nThe eigenvalues associated with the first eigenvector \\(\\lambda_1 = -4\\) by solving\n\n\\[\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda_1 \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n-21/5  - \\lambda_1 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - \\lambda_1 & 3/5 \\\\\n-4 & -10 & 5 - \\lambda_1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nand can be solved for \\(\\lambda_1\\) using an augmented matrix form and row operations to reduce to reduced row echelon form.\nLetting \\(\\lambda_1 = -4\\), we have\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n-21/5  - -4 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - -4 & 3/5 \\\\\n-4 & -10 & 5 - -4\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich results in the augmented matrix\n\\[\n\\begin{aligned}\n\\begin{pmatrix} -1/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & 6/5 & 3/5 & 0 \\\\ -4 & -10 & 9 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nReducing the augmented matrix to reduced row echelon form gives\n\\[\n\\begin{aligned}\n\\begin{pmatrix} -1/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & 6/5 & 3/5 & 0 \\\\ -4 & -10 & 9 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & -1 & 0 \\\\ 0 & 1 & -1/2 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich has solution\n\\[\n\\begin{aligned}\nx_1 - x_3 & = 0 \\\\\nx_2 - \\frac{1}{2} x_3 & = 0 \\\\\nx_3 & = x_3\n\\end{aligned}\n\\]\nFixing \\(x_3 = 1\\) gives the eigenvector associated with \\(\\lambda_1 = -4\\) of \\(\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}\\). We can verify that this is an eigenvector with matrix multiplication to show \\(\\mathbf{A} \\mathbf{x}_1 = \\lambda_1 \\mathbf{x}_1\\)\n\\[\n\\begin{aligned}\n\\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} -4 \\\\ -2 \\\\ -4 \\end{pmatrix}  = -4 \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\nUsing R, this is\n\n\n\n\nA <- matrix(c(-21/5, -6/5, -4, -34/5, -14/5, -10, 18/5,  3/5, 5), 3, 3)\nlambda_1 <- -4\nrref(cbind(A - lambda_1 * diag(nrow(A)), 0))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0 -1.0    0\n[2,]    0    1 -0.5    0\n[3,]    0    0  0.0    0\n\n\nVerifying that the eigenvalue \\(\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}\\) is an eigenvector is\n\nx_1 <- c(1, 1/2, 1)\nall.equal(drop(A %*% x_1), lambda_1 * x_1) # drop() makes a matrix with one column a vector\n\n[1] TRUE\n\n\n\nThe eigenvalues associated with the second eigenvector \\(\\lambda_2 = 3\\) by solving\n\n\\[\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda_2 \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n-21/5  - \\lambda_2 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - \\lambda_2 & 3/5 \\\\\n-4 & -10 & 5 - \\lambda_2\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nand can be solved for \\(\\lambda_2\\) using an augmented matrix form and row operations to reduce to reduced row echelon form.\nLetting \\(\\lambda_2 = 3\\), we have\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n-21/5  - 3 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - 3 & 3/5 \\\\\n-4 & -10 & 5 - 3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich results in the augmented matrix\n\\[\n\\begin{aligned}\n\\begin{pmatrix} -36/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & -29/5 & 3/5 & 0 \\\\ -4 & -10 & 2 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nReducing the augmented matrix to reduced row echelon form gives\n\\[\n\\begin{aligned}\n\\begin{pmatrix} -36/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & -29/5 & 3/5 & 0 \\\\ -4 & -10 & 2 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & -1/2 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich has solution\n\\[\n\\begin{aligned}\nx_1 - \\frac{1}{2} x_3 & = 0 \\\\\nx_2  & = 0 \\\\\nx_3 & = x_3\n\\end{aligned}\n\\]\nFixing \\(x_3 = 1\\) gives the eigenvector associated with \\(\\lambda_2 = 3\\) of \\(\\mathbf{x}_2 = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}\\). We can verify that this is an eigenvector with matrix multiplication to show \\(\\mathbf{A} \\mathbf{x}_2 = \\lambda_2 \\mathbf{x}_2\\)\n\\[\n\\begin{aligned}\n\\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} 3/2 \\\\ 0 \\\\ 3 \\end{pmatrix}  = 3 \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\nUsing R, this is\n\n\n\n\nA <- matrix(c(-21/5, -6/5, -4, -34/5, -14/5, -10, 18/5,  3/5, 5), 3, 3)\nlambda_2 <- 3\nrref(cbind(A - lambda_2 * diag(nrow(A)), 0))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0 -0.5    0\n[2,]    0    1  0.0    0\n[3,]    0    0  0.0    0\n\n\nVerifying that the eigenvalue \\(\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}\\) is an eigenvector is\n\nx_2 <- c(1/2, 0, 1)\nall.equal(drop(A %*% x_2), lambda_2 * x_2) # drop() makes a matrix with one column a vector\n\n[1] TRUE\n\n\n\nThe eigenvalues associated with the third eigenvector \\(\\lambda_3 = -1\\) by solving\n\n\\[\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda_3 \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n-21/5  - \\lambda_3 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - \\lambda_3 & 3/5 \\\\\n-4 & -10 & 5 - \\lambda_3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nand can be solved for \\(\\lambda_3\\) using an augmented matrix form and row operations to reduce to reduced row echelon form.\nLetting \\(\\lambda_3 = -1\\), we have\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n-21/5  - -1 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - -1 & 3/5 \\\\\n-4 & -10 & 5 - -1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich results in the augmented matrix\n\\[\n\\begin{aligned}\n\\begin{pmatrix} -16/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & -9/5 & 3/5 & 0 \\\\ -4 & -10 & 6 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nReducing the augmented matrix to reduced row echelon form gives\n\\[\n\\begin{aligned}\n\\begin{pmatrix} -16/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & -9/5 & 3/5 & 0 \\\\ -4 & -10 & 6 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich has solution\n\\[\n\\begin{aligned}\nx_1 + x_3 & = 0 \\\\\nx_2  - x_3 & = 0 \\\\\nx_3 & = x_3\n\\end{aligned}\n\\]\nFixing \\(x_3 = 1\\) gives the eigenvector associated with \\(\\lambda_3 = -1\\) of \\(\\mathbf{x}_3 = \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\). We can verify that this is an eigenvector with matrix multiplication to show \\(\\mathbf{A} \\mathbf{x}_2 = \\lambda_2 \\mathbf{x}_2\\)\n\\[\n\\begin{aligned}\n\\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\end{pmatrix}  = -1 \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\nUsing R, this is\n\n\n\n\nA <- matrix(c(-21/5, -6/5, -4, -34/5, -14/5, -10, 18/5,  3/5, 5), 3, 3)\nlambda_3 <- -1\nrref(cbind(A - lambda_3 * diag(nrow(A)), 0))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    1    0\n[2,]    0    1   -1    0\n[3,]    0    0    0    0\n\n\nVerifying that the eigenvalue \\(\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}\\) is an eigenvector is\n\nx_3 <- c(-1, 1, 1)\nall.equal(drop(A %*% x_3), lambda_3 * x_3) # drop() makes a matrix with one column a vector\n\n[1] TRUE\n\n\nNow, let’s compare the output of the eigen() function in R to these eigenvectors calculated “by hand.” The eigen() function returns the two objects named $values that contain the eigenvalues of \\(\\mathbf{A}\\) and the object $vectors that contains a matrix of eigenvectors as the columns fo the matrix. Each eigenvalue corresponds to the respective column of the eigenvector matrix.\n\neigen(A)\n\neigen() decomposition\n$values\n[1] -4  3 -1\n\n$vectors\n          [,1]          [,2]       [,3]\n[1,] 0.6666667 -4.472136e-01 -0.5773503\n[2,] 0.3333333 -3.041122e-16  0.5773503\n[3,] 0.6666667 -8.944272e-01  0.5773503\n\n\nNote that the eigenvectors returned by the eigen() function are the same as those in the example, but the vectors are different. However, the vectors from eigen() point in the same direction as those found “by hand” and only differ in the length of the vector. For example, we found the eigenvector associated with the eigenvalue -4 to be \\(\\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}\\) which points in the same direction as the vector from eigen() of \\(\\begin{pmatrix} 2/3 \\\\ 1/3 \\\\ 2/3 \\end{pmatrix}\\) which is just a scalar multiple of the vector found “by hand.” Recall that when we found a solution using RREF and the augmented matrix, the solution set was infinite (a line) and we just set the free variable equal to 1. Another equally valid solution would be to set the free variable so that the total length of the vector is 1, and this is what the eigen() function does.\n\n\n\n\nDefinition 20.2 Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix and let \\(\\lambda\\) be an eigenvalue of \\(\\mathbf{A}\\). Then, the \\(\\lambda\\)-eigenspace of \\(\\mathbf{A}\\) is the solution set of the matrix equation \\(\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}\\) which is the subspace null(\\(\\mathbf{A} - \\lambda \\mathbf{I}\\)).\n\nTherefore, the \\(\\lambda\\)-eigenspace is a subspace (the null space of any matrix is a subspace) that contains the zero vector \\(\\mathbf{0}\\) and all the eigenvectors of \\(\\mathbf{A}\\) with corresponding eigenvalue \\(\\lambda\\).\n\n\nExample 20.6 \n\nFor \\(\\lambda\\) = (a) -2, (b) 1, and (c) 3, decide if \\(\\lambda\\) is a eigenvalue of the matrix \\(\\mathbf{A} = \\begin{pmatrix} 3 & 0 \\\\ -3 & 2 \\end{pmatrix}\\) and if so, compute a basis for the \\(\\lambda\\)-eigenspace.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGiven the matrix \\(\\mathbf{A}\\) defined in the example, we will check if any of the values of \\(\\lambda\\) are eigenvalues.\n\nA <-  matrix(c(3, -3, 0, 2), 2, 2)\n\n\nFirst, we check if \\(\\lambda = -2\\) is an eigenvalue of \\(\\mathbf{A}\\). If \\(\\lambda = -2\\) is an eigenvalue of \\(\\mathbf{A}\\), then there is a non-trivial solution to\n\n\\[\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda\\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n\\]\nThe homogeneous system of equations can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n3  - \\lambda & 0 \\\\\n-3 & 2 - \\lambda\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nand can be solved for \\(\\lambda = -2\\) using an augmented matrix form and row operations to reduce to reduced row echelon form where\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n3  - -2 & 0 \\\\\n-3 & 2 - -2\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich results in the augmented matrix\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 5 & 0 & 0 \\\\ -3 & 4 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nReducing the augmented matrix to reduced row echelon form gives\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 5 & 0 & 0 \\\\ -3 & 4 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich has solution\n\\[\n\\begin{aligned}\nx_1  & = 0 \\\\\nx_2  & = 0\n\\end{aligned}\n\\]\nwhich is the trivial solution. Thus, \\(\\lambda = -2\\) is not an eigenvalue of \\(\\mathbf{A}\\).\nUsing R, this is\n\n\n\n\nA <-  matrix(c(3, -3, 0, 2), 2, 2)\nlambda <- -2\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n\n\nBecause there is only the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\), \\(\\lambda = -2\\) is not an eigenvalue of \\(\\mathbf{A}\\).\n\nlambda <- 1\n\n\nNext, we check if \\(\\lambda = 1\\) is an eigenvalue of \\(\\mathbf{A}\\). If \\(\\lambda = 1\\) is an eigenvalue of \\(\\mathbf{A}\\), then there is a non-trivial solution to\n\n\\[\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda\\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n\\]\nThe homogeneous system of equations can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n3  - \\lambda & 0 \\\\\n-3 & 2 - \\lambda\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nand can be solved for \\(\\lambda = 1\\) using an augmented matrix form and row operations to reduce to reduced row echelon form where\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n3  - 1 & 0 \\\\\n-3 & 2 - 1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich results in the augmented matrix\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 2 & 0 & 0 \\\\ -3 & 1 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nReducing the augmented matrix to reduced row echelon form gives\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 2 & 0 & 0 \\\\ -3 & 1 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich has solution\n\\[\n\\begin{aligned}\nx_1  & = 0 \\\\\nx_2  & = 0\n\\end{aligned}\n\\]\nwhich is the trivial solution. Thus, \\(\\lambda = 1\\) is not an eigenvalue of \\(\\mathbf{A}\\).\nFixing \\(x_3 = 1\\) gives the eigenvector associated with \\(\\lambda_3 = NA\\) of \\(\\mathbf{x}_3 = \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\). We can verify that this is an eigenvector with matrix multiplication to show \\(\\mathbf{A} \\mathbf{x}_2 = \\lambda_2 \\mathbf{x}_2\\)\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 3 & 0 \\\\ -3 & 2 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} -3 \\\\ 5 \\end{pmatrix}  = 1 \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\nUsing R, this is\n\n\n\n\nA <-  matrix(c(3, -3, 0, 2), 2, 2)\nlambda <- 1\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n\n\nBecause there is only the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\), \\(\\lambda = 1\\) is not an eigenvalue of \\(\\mathbf{A}\\).\n\nlambda <- 3\n\n\nNext, we check if \\(\\lambda = 3\\) is an eigenvalue of \\(\\mathbf{A}\\). If \\(\\lambda = 3\\) is an eigenvalue of \\(\\mathbf{A}\\), then there is a non-trivial solution to\n\n\\[\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda\\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n\\]\nThe homogeneous system of equations can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n3  - \\lambda & 0 \\\\\n-3 & 2 - \\lambda\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nand can be solved for \\(\\lambda = 3\\) using an augmented matrix form and row operations to reduce to reduced row echelon form where\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n3  - 3 & 0 \\\\\n-3 & 2 - 3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich results in the augmented matrix\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 0 & 0 & 0 \\\\ -3 & -1 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nReducing the augmented matrix to reduced row echelon form gives\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 0 & 0 & 0 \\\\ -3 & -1 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 1/3 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich has solution\n\\[\n\\begin{aligned}\nx_1  + \\frac{1}{3} x_2 & = 0 \\\\\nx_2  & = x_2\n\\end{aligned}\n\\]\nwhere the solution can be chosen by setting \\(x_2 = 1\\) giving the eigenvector associated with \\(\\lambda = 3\\) of \\(\\mathbf{x} = \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}\\). We can verify that this is an eigenvector with matrix multiplication to show \\(\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}\\)\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 3 & 0 \\\\ -3 & 2 \\end{pmatrix} \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}  = 3 \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\nUsing R, this is\n\nA <-  matrix(c(3, -3, 0, 2), 2, 2)\nlambda <- 3\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n\n     [,1]      [,2] [,3]\n[1,]    1 0.3333333    0\n[2,]    0 0.0000000    0\n\n\nVerifying that the eigenvalue \\(\\mathbf{x} = \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}\\) is an eigenvector is\n\nx <- c(-1/3, 1)\nall.equal(drop(A %*% x), lambda * x) # drop() makes a matrix with one column a vector\n\n[1] TRUE\n\n\nNow, because we know that \\(\\lambda = 3\\) is an eigenvalue of \\(\\mathbf{A}\\) and the homogeneous system of equations \\(\\left(\\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}\\) has a unique solution, the vector \\(\\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}\\) forms a basis for the 3-eigenspace of \\(\\mathbf{A}\\).\n\n\n\n\n\nExample 20.7 \n\nLet \\(\\mathbf{A} = \\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix}\\). Find the eigenvectors associated with the eigenvalues (a) \\(\\lambda = 3\\) and (b) \\(\\lambda = 1\\). For each eigen value, also find the basis for the associated eigen-space.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGiven the matrix \\(\\mathbf{A} = \\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix}\\), we can find the eigenvectors associated with the given eigenvalues\n\n\n\n\nThe eigenvalues associated with the first eigenvector \\(\\lambda = 3\\) by solving\n\n\\[\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n17/5  - \\lambda & 8/5 & -6/5 \\\\\n0 & 3 - \\lambda & 0 \\\\\n4/5 & 16/5 & 3/5 - \\lambda\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nand can be solved for \\(\\lambda\\) using an augmented matrix form and row operations to reduce to reduced row echelon form.\nLetting \\(\\lambda = 3\\), we have\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n17/5  - 3 & 8/5 & -6/5 \\\\\n0 & 3 - 3 & 0 \\\\\n4/5 & 16/5 & 3/5 - 3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich results in the augmented matrix\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 2/5 & 8/5 & -6/5 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 4/5 & 16/5 & -12/5 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nReducing the augmented matrix to reduced row echelon form gives\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 2/5 & 8/5 & -6/5 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 4/5 & 16/5 & -12/5 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 4 & -3 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich has solution\n\\[\n\\begin{aligned}\nx_1 + 4 x_2 - x_33 & = 0 \\\\\nx_2 & = x_2 \\\\\nx_3 & = x_3\n\\end{aligned}\n\\]\nWhere there are two free variables which suggests that the dimension of the solution space is 2 (the solution set defines a plane and will have 2 basis vectors. Fixing \\(x_2 = 1\\) and \\(x_3 = 0\\) gives the first eigenvector associated with \\(\\lambda = 3\\) of \\(\\mathbf{x}_1 = \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix}\\). We can verify that this is an eigenvector with matrix multiplication to show \\(\\mathbf{A} \\mathbf{x}_1 = \\lambda \\mathbf{x}_1\\)\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix} \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix} & = \\begin{pmatrix} -12 \\\\ 3 \\\\ 0 \\end{pmatrix}  = 3 \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nThe second basis vector for the eigenspace associated with \\(\\lambda = 3\\) can be found by fixing \\(x_2 = 0\\) and \\(x_3 = 1\\) to get the eigenvector \\(\\mathbf{x}_2 = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}\\). We can verify that this is an eigenvector with matrix multiplication to show \\(\\mathbf{A} \\mathbf{x}_2 = \\lambda \\mathbf{x}_2\\)\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} 9 \\\\ 0 \\\\ 3 \\end{pmatrix}  = 3 \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\nThus, the basis for 3-eigenspace is \\(\\left\\{ \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}\\right\\}\\). Note that this is the same as finding a basis for the null space of \\(\\left(\\mathbf{A} - \\lambda \\mathbf{I} \\right)\\).\nUsing R, this is\n\n\n\n\nA <- matrix(c(17/5, 0, 4/5, 8/5, 3, 16/5, -6/5, 0, 3/5), 3, 3)\nlambda <- 3\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4   -3    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n\n\nVerifying that the eigenvalue \\(\\mathbf{x}_1 = \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) is an eigenvector and that \\(\\mathbf{x}_2 = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}\\) is an eigenvector\n\nx_1 <- c(-4, 1, 0)\nall.equal(drop(A %*% x_1), lambda * x_1) # drop() makes a matrix with one column a vector\n\n[1] TRUE\n\nx_2 <- c(3, 0, 1)\nall.equal(drop(A %*% x_2), lambda * x_2) # drop() makes a matrix with one column a vector\n\n[1] TRUE\n\n\n\nlambda <- 1\n\n\nThe eigenvalues associated with the second eigenvector \\(\\lambda = 1\\) by solving\n\n\\[\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich can be written as\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n17/5  - \\lambda & 8/5 & -6/5 \\\\\n0 & 3 - \\lambda & 0 \\\\\n4/5 & 16/5 & 3/5 - \\lambda\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nand can be solved for \\(\\lambda\\) using an augmented matrix form and row operations to reduce to reduced row echelon form.\nLetting \\(\\lambda = 1\\), we have\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n17/5  - 1 & 8/5 & -6/5 \\\\\n0 & 3 - 1 & 0 \\\\\n4/5 & 16/5 & 3/5 - 1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n\\]\nwhich results in the augmented matrix\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 12/5 & 8/5 & -6/5 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 4/5 & 16/5 & -2/5 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nReducing the augmented matrix to reduced row echelon form gives\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 12/5 & 8/5 & -6/5 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 4/5 & 16/5 & -2/5 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & -1/2 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nwhich has solution\n\\[\n\\begin{aligned}\nx_1 - \\frac{1}{2} x_3 & = 0 \\\\\nx_2  & = 0 \\\\\nx_3 & = x_3\n\\end{aligned}\n\\]\nFixing \\(x_3 = 1\\) gives the eigenvector associated with \\(\\lambda= 1\\) of \\(\\mathbf{x} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}\\). We can verify that this is an eigenvector associated with eigenvalue \\(\\lambda = 1\\) with matrix multiplication to show \\(\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}\\)\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}  = NA \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\nTherefore, a basis for the 1-eigenspace is \\(\\left\\{ \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right\\}\\)\nUsing R, this is\n\n\n\n\nA <- matrix(c(17/5, 0, 4/5, 8/5, 3, 16/5, -6/5, 0, 3/5), 3, 3)\nlambda <- 1\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0 -0.5    0\n[2,]    0    1  0.0    0\n[3,]    0    0  0.0    0\n\n\nVerifying that the eigenvalue \\(\\mathbf{x} = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}\\) is an eigenvector associated with eigenvalue \\(\\lambda = 1\\) is\n\nx <- c(1/2, 0, 1)\nall.equal(drop(A %*% x), lambda * x) # drop() makes a matrix with one column a vector\n\n[1] TRUE\n\n\n\n\n\n\n20.1.1 Computing Eigenspaces\nLet \\(\\mathbf{A}\\) be a \\(n \\times n\\) matrix and let \\(\\lambda\\) be a scalar.\n\n\\(\\lambda\\) is an eigenvalue of \\(\\mathbf{A}\\) if and only if \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\) has a non-trivial solution. The matrix equation \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\) has a non-trivial solution if and only if null\\((\\mathbf{A} - \\lambda \\mathbf{I}) \\neq \\{\\mathbf{0} \\}\\)\nFinding a basis for the \\(\\lambda\\)-eigenspace of \\(\\mathbf{A}\\) is equivalent to finding a basis for null\\((\\mathbf{A} - \\lambda \\mathbf{I})\\) which can be done by finding parametric forms of the solutions of the homogeneous system of equations \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\).\nThe dimension of the \\(\\lambda\\)-eigenspace of \\(\\mathbf{A}\\) is equal to the number of free variables in the system of equations \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\) which is the number of non-pivot columns of \\(\\mathbf{A} - \\lambda \\mathbf{I}\\).\nThe eigenvectors with eigenvalue \\(\\lambda\\) are the nonzero vectors in null\\((\\mathbf{A} - \\lambda \\mathbf{I})\\) which are equivalent to the nontrivial solutions of \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\).\n\nNote that this leads of a fact about the \\(0\\)-eigenspace.\n\nDefinition 20.3 Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix. Then\n\nThe number 0 is an eigenvalue of \\(\\mathbf{A}\\) if and only if \\(\\mathbf{A}\\) is not invertible.\nIf 0 is an eigenvalue of \\(\\mathbf{A}\\), then the 0-eigenspace of \\(\\mathbf{A}\\) is null\\((\\mathbf{A})\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n0 is an eigenvalue of \\(\\mathbf{A}\\) if and only if null\\((\\mathbf{A} - 0 \\mathbf{I})\\) = null\\((\\mathbf{A})\\). By the invertible matrix theorem, \\(\\mathbf{A}\\) is invertible if and only if null\\((\\mathbf{A}) = \\{\\mathbf{0}\\}\\) but we know that the 0-eigenspace of \\(\\mathbf{A}\\) is not the trivial set \\(\\{\\mathbf{0}\\}\\) because 0 is an eigenvalue.\n\n\n\n\nTheorem 20.2 (Invertible Matrix Theorm + eigenspaces) This is an extension of the prior statement of the invertible matrix Theorem 9.5 Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix and \\(T: \\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) be the linear transformation given by \\(T(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\). Then the following statements are equivalent (i.e., they are all either simultaneously true or false).\n\n\\(\\mathbf{A}\\) is invertible.\n\\(\\mathbf{A}\\) has n pivot columns.\nnull\\((\\mathbf{A}) = \\{\\mathbf{0}\\}\\).\nThe columns of \\(\\mathbf{A}\\) are linearly independent.\nThe columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^n\\).\nThe matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a uniqu solution for each \\(\\mathbf{b} \\in \\mathcal{R}^n\\).\nThe transormation \\(T\\) is invertible.\nThe transormation \\(T\\) is one-to-one.\nThe transormation \\(T\\) is onto.\ndet\\((\\mathbf{A}) \\neq 0\\)\n0 is not an eigenvalue of \\(\\mathbf{A}\\)"
  },
  {
    "objectID": "20-characteristic-equations.html",
    "href": "20-characteristic-equations.html",
    "title": "21  The Characteristic Equation",
    "section": "",
    "text": "3 Blue 1 Brown –\nThe characteristic equation/polynomial encodes information about the eigenvalues of the characteristic equation. In the previous chapter, we showed how we can decide if a scalar \\(\\lambda\\) is an eigenvalue of a matrix and how to find the vectors associated with the eigenvalue. However, we did not learn how to find eigenvalues (other than to just randomly try \\(\\lambda\\)). The characteristic equation/polynomial allows for determining the eigenvalues \\(\\lambda\\).\nWhile not obvious, the function \\(f(\\lambda)\\) is a polynomial of \\(\\lambda\\) but requires computing the determinant of the matrix \\(\\mathbf{A} - \\lambda \\mathbf{I}\\) which contains an unknown value \\(\\lambda\\).\nOnce the characteristic equation is defined, we can use the equation to solve for the eigenvalues."
  },
  {
    "objectID": "20-characteristic-equations.html#similarity",
    "href": "20-characteristic-equations.html#similarity",
    "title": "21  The Characteristic Equation",
    "section": "21.1 Similarity",
    "text": "21.1 Similarity\nThe idea behind similar matrices is to understand how the linear transformations implied by the transformation behave. Two matrices are similar if their transformation behavior (rotation, expansion/contraction, etc.) is the same but the coordinates on which the matrix operates are different.\n\nDefinition 21.2 The matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are said to be similar if there exists an invertible matrix \\(\\mathbf{P}\\) where\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}\n\\end{aligned}\n\\]\nor equivalently\n\\[\n\\begin{aligned}\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P}=  \\mathbf{B}\n\\end{aligned}\n\\]\n\nTherefore, it is possible to change \\(\\mathbf{A}\\) into \\(\\mathbf{B}\\) with an invertible (one-to-one and onto) transformation.\n\nExample 21.5 Consider the following example with matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{P}\\) defined as below:\n\nA <- matrix(c(3, 0, 0, -2), 2, 2)\nA\n\n     [,1] [,2]\n[1,]    3    0\n[2,]    0   -2\n\nB <- matrix(c(-12, -10, 15, 13), 2, 2)\nB\n\n     [,1] [,2]\n[1,]  -12   15\n[2,]  -10   13\n\nP <- matrix(c(-2, 1, 3,-1), 2, 2)\nP\n\n     [,1] [,2]\n[1,]   -2    3\n[2,]    1   -1\n\nP %*% B %*% solve(P)\n\n     [,1] [,2]\n[1,]    3    0\n[2,]    0   -2\n\nsolve(P) %*% A %*% P\n\n     [,1] [,2]\n[1,]  -12   15\n[2,]  -10   13\n\n\n\n\nTheorem 21.2 If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are \\(n \\times n\\) similar matrices, then \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) will have the same characteristic polynomial and therefore the same eigenvalues.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are similar, then there exists an invertible matrix \\(\\mathbf{P}\\) such that\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}\n\\end{aligned}\n\\]\nTherefore\n\\[\n\\begin{aligned}\n\\mathbf{A}  - \\lambda \\mathbf{I} & = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} - \\lambda \\mathbf{I} \\\\\n& = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} - \\lambda \\mathbf{P} \\mathbf{P}^{-1} \\\\\n& =  \\mathbf{P} \\left( \\mathbf{B} \\mathbf{P}^{-1} - \\lambda \\mathbf{P}^{-1} \\right) \\\\\n& =  \\mathbf{P} \\left( \\mathbf{B} - \\lambda \\mathbf{I} \\right) \\mathbf{P}^{-1}\\\\\n\\end{aligned}\n\\]\nTo get the characteristic equation, we need to solve for the determinant\n\\[\n\\begin{aligned}\ndet\\left( \\mathbf{A}  - \\lambda \\mathbf{I} \\right) & = det\\left( \\mathbf{P} \\left( \\mathbf{B} - \\lambda \\mathbf{I} \\right) \\mathbf{P}^{-1} \\right) \\\\\n& = det\\left( \\mathbf{P} \\right)  det\\left( \\mathbf{B} - \\lambda \\mathbf{I} \\right) det\\left(\\mathbf{P}^{-1} \\right) \\\\\n\\end{aligned}\n\\]\nWe know that \\(det\\left(\\mathbf{P}^{-1} \\right)\\) = \\(\\frac{1}{det\\left(\\mathbf{P} \\right)}\\) (or, equivalently \\(det\\left(\\mathbf{P} \\right) det\\left(\\mathbf{P}^{-1} \\right) = det\\left(\\mathbf{P} \\mathbf{P}^{-1} \\right) = det(\\mathbf{I}) = 1\\)), we have \\(det\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) = det\\left( \\mathbf{B} - \\lambda \\mathbf{I} \\right)\\) so that \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) have the same characteristic polynomial (and the same eigenvalues)."
  },
  {
    "objectID": "20-characteristic-equations.html#the-geometric-interpetation-of-similar-matrices",
    "href": "20-characteristic-equations.html#the-geometric-interpetation-of-similar-matrices",
    "title": "21  The Characteristic Equation",
    "section": "21.2 The geometric interpetation of similar matrices",
    "text": "21.2 The geometric interpetation of similar matrices\nIn general, similar matrices do similar things in different spaces (different spaces in terms of different bases).\nExample here"
  },
  {
    "objectID": "21-Diagonalization.html",
    "href": "21-Diagonalization.html",
    "title": "22  Diagonalization",
    "section": "",
    "text": "Definition 22.1 A \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is diagonalizable if the matrix \\(\\mathbf{A}\\) is similar to a diagonal matrix. This is equivalent to saying there exists some invertible \\(n \\times n\\) matrix \\(\\mathbf{P}\\) and diagonal matrix \\(\\mathbf{D}\\) such that\n\\[\n\\begin{aligned}\n\\mathbf{A} & = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}\n\\end{aligned}\n\\]\n\n\nExample 22.1 Any diagonal matrix \\(\\mathbf{D}\\) is diagonalizable becuase it is self-similar.\n\n\nTheorem 22.1 (The Diagonalization Theorem) A \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is diagonalizable if and only if the matrix \\(\\mathbf{A}\\) has \\(n\\) linearly independent eigenvectors.\nIn addition, the \\(n \\times n\\) matrix \\(\\mathbf{A} = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}\\) with diagonal matrix \\(\\mathbf{D}\\) if and only if the columns of \\(\\mathbf{P}\\) are the lienarly independent eigenvectors of \\(\\mathbf{A}\\). Then, the diagonal elements of \\(\\mathbf{D}\\) are the eigenvalues of \\(\\mathbf{A}\\) that correspond to the eigenvectors in \\(\\mathbf{P}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis comes directly from Theorem 20.1 where if a \\(n \\times n\\) matrix has \\(n\\) distinct eigenvalues \\(\\lambda_1 \\neq \\lambda_2 \\neq \\cdots \\neq \\lambda_n\\), the the corresponding eigenvalues \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) are linearly independent.\n\n\n\nThis theorem implies that the matrix \\(\\mathbf{A}\\) is diagonalizable if and only if the eigenvectors of \\(\\mathbf{A}\\) form a basis for \\(\\mathcal{R}^n\\). When this is the case, the set of eigenvectors is called an eigenbasis.\n\nExample 22.2 Consider the following example of a diagonalizable matrix \\(\\mathbf{A}\\)\n\nA <- matrix(c(9, 2, 0, -3, 2, -4, 1, 0, 3), 3, 3)\nA\n\n     [,1] [,2] [,3]\n[1,]    9   -3    1\n[2,]    2    2    0\n[3,]    0   -4    3\n\neigen_A <- eigen(A)\nstr(eigen_A)\n\nList of 2\n $ values : num [1:3] 7.63 4.52 1.86\n $ vectors: num [1:3, 1:3] -0.905 -0.322 0.278 -0.407 -0.324 ...\n - attr(*, \"class\")= chr \"eigen\"\n\nP <- eigen_A$vectors\nP\n\n           [,1]       [,2]        [,3]\n[1,] -0.9050468 -0.4069141 -0.01938647\n[2,] -0.3217259 -0.3235720  0.27433148\n[3,]  0.2781774  0.8542377  0.96143976\n\nD <- diag(eigen_A$values)\nD\n\n         [,1]     [,2]     [,3]\n[1,] 7.626198 0.000000 0.000000\n[2,] 0.000000 4.515138 0.000000\n[3,] 0.000000 0.000000 1.858664\n\nP %*% D %*% solve(P)\n\n             [,1] [,2]          [,3]\n[1,] 9.000000e+00   -3  1.000000e+00\n[2,] 2.000000e+00    2 -9.144832e-16\n[3,] 1.342835e-15   -4  3.000000e+00\n\nall.equal(A, P %*% D %*% solve(P))\n\n[1] TRUE\n\n\n\n\nTheorem 22.2 Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) diagonalizable matrix with \\(\\mathbf{A} = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}\\). Then, the matrix power \\(\\mathbf{A}^p\\) is\n\\[\n\\begin{aligned}\n\\mathbf{A}^p = \\mathbf{P} \\mathbf{D}^p \\mathbf{P}^{-1}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn class\n\n\n\n\n\n\n\nExample 22.3 In this example, we apply the diagonalization theorem to the matrix \\(\\mathbf{A}\\)\nConsider the matrix \\(\\mathbf{A} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}\\) which has eigenvalues 1, 2, 3. Then the standard basis \\(\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\), \\(\\mathbf{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\), and \\(\\mathbf{e}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\) are corresponding eigenvectors (check the definition \\(\\mathbf{A} \\lambda = \\mathbf{v} \\lambda\\)) because\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} & = 1 * \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\\\\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} & = 2 * \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\\\\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} & = 3 * \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n\\]\nThus, by the diagonlaization theorem, we have \\(\\mathbf{A} = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}\\) where \\(\\mathbf{P}\\) is the identity matrix and \\(\\mathbf{D}\\) is the diagonal matrix with entries 1, 2, 3.\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} & = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}^{-1}\n\\end{aligned}\n\\]\nwhich gives us that \\(\\mathbf{A}\\) is similar to itself.\nHowever, there is nothing in the diagonalization theorem that says that we must put the eigenvalues in the order 1, 2, 3. If we put the eigenvalues in the order 3, 2, 1, then the corresponding eigenvectors are \\(\\mathbf{e}_3\\), \\(\\mathbf{e}_2\\), and \\(\\mathbf{e}_1\\). Using the diagonlaization theorem, we have \\(\\mathbf{A} = \\tilde{\\mathbf{P}} \\tilde{\\mathbf{D}} \\tilde{\\mathbf{P}}^{-1}\\) where \\(\\tilde{\\mathbf{P}}\\) is the matrix with columns \\(\\mathbf{e}_3\\), \\(\\mathbf{e}_2\\), and \\(\\mathbf{e}_1\\) and \\(\\tilde{\\mathbf{D}}\\) is the diagonal matrix with entries 3, 2, 1 which results in\n\\[\n\\begin{aligned}\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} & = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 3 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix}^{-1}\n\\end{aligned}\n\\]\nwhich implies that the matrices \\(\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 3 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) are similar to each other"
  },
  {
    "objectID": "22-orthogonal.html",
    "href": "22-orthogonal.html",
    "title": "23  Inner product, length, and orthogonality",
    "section": "",
    "text": "3 Blue 1 Brown – The dot prodcut\nThe properties of inner products are defined with the following theorem.\nBased on the theorem above, the inner product of a vector with itself (\\(\\mathbf{u}'\\mathbf{u}\\)) is strictly non-negative. Thus, we can define the length of the vector \\(\\mathbf{u}\\) (also called the norm of the vector \\(\\mathbf{u}\\)).\nAnother property of the norm is how the norm changes based on scalar multiplication. Let \\(\\mathbf{v} \\in \\mathcal{R}^n\\) be a vector and let \\(c\\) be a scalar. Then \\(\\|c \\mathbf{v}\\| = |c|\\|\\mathbf{v}\\|\\)"
  },
  {
    "objectID": "22-orthogonal.html#distance",
    "href": "22-orthogonal.html#distance",
    "title": "23  Inner product, length, and orthogonality",
    "section": "23.1 Distance",
    "text": "23.1 Distance\nIn two dimensions, the Euclidean distance between the points \\((x_1, y_1)\\) and \\((x_2, y_2)\\) is defined as \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\). In higher dimensions, a similar definition holds.\n\nDefinition 23.4 Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors in \\(\\mathcal{R}^n\\). Then the distance \\(dist(\\mathbf{u}, \\mathbf{v})\\) between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is\n\\[\n\\begin{aligned}\ndist(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|\n\\end{aligned}\n\\]\n\n\nExample 23.3 Distance between two 3-dimensional vectors\n\nu <- c(3, -5, 1)\nv <- c(4, 3, -2)\nsqrt(sum((u-v)^2))\n\n[1] 8.602325"
  },
  {
    "objectID": "22-orthogonal.html#orthogonal-vectors",
    "href": "22-orthogonal.html#orthogonal-vectors",
    "title": "23  Inner product, length, and orthogonality",
    "section": "23.2 Orthogonal vectors",
    "text": "23.2 Orthogonal vectors\nThe equivalent of perpendicular lines in \\(\\mathcal{R}^n\\) are known as orthogonal vectors.    \n\nDefinition 23.5 The two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathcal{R}^n\\) are orthogonal if\n\\[\n\\begin{aligned}\n\\mathbf{u}' \\mathbf{v} = 0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "22-orthogonal.html#angles-between-vectors",
    "href": "22-orthogonal.html#angles-between-vectors",
    "title": "23  Inner product, length, and orthogonality",
    "section": "23.3 Angles between vectors",
    "text": "23.3 Angles between vectors\nLet \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors \\(\\mathcal{R}^n\\). Then, the angle between the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is defined as the angle \\(\\theta\\) in the relationship\n\\[\n\\begin{aligned}\n\\mathbf{u}' \\mathbf{v} = \\| \\mathbf{u} \\| \\| \\mathbf{v} \\| cos(\\theta)\n\\end{aligned}\n\\]\nSolving for the angle \\(\\theta\\) results in the equation\n\\[\n\\begin{aligned}\n\\theta = arccos \\left( \\frac{\\mathbf{u}' \\mathbf{v}}{\\| \\mathbf{u} \\| \\| \\mathbf{v} \\|} \\right)\n\\end{aligned}\n\\]\nwhere \\(arccos(\\cdot)\\) is inverse cosine function, which is acos() in R.\nsee example: angles-as-n-gets-large.R\n\n\nExample 23.4 \n\nLet \\(\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 4 \\\\ 6 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} -5 \\\\ 2 \\\\ 4 \\end{pmatrix}\\). What is the angle between these two vectors?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe angle between the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) depends on the dot product between the two vectors and the norms (lengths) of the two vectors. The inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is\n\\[\n\\begin{aligned}\n\\mathbf{u}'\\mathbf{v} = \\begin{pmatrix} 1 & 4 & 6 \\end{pmatrix} \\begin{pmatrix} -5 \\\\ 2 \\\\ 4 \\end{pmatrix} = 1*-5 +4*2 + 6*4 = 27\n\\end{aligned}\n\\]\nwhere the vector \\(\\mathbf{u}\\) has length\n\\[\n\\|\\mathbf{u}\\| = \\sqrt{u_1^2 + u_2^2 + u_3^2} = \\sqrt{1^2 + 4^2 + 6^2} = \\sqrt{53} = 7.2801099\n\\]\nand the vector \\(\\mathbf{v}\\) has length\n\\[\n\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + v_3^2} = \\sqrt{-5^2 + 2^2 + 4^2} = \\sqrt{45} = 6.7082039\n\\]\nPlugging these into the equation for the angle \\(\\theta\\) gives\n\\[\n\\begin{aligned}\n\\theta = arccos \\left( \\frac{\\mathbf{u}' \\mathbf{v}}{\\| \\mathbf{u} \\| \\| \\mathbf{v} \\|} \\right) \\\\\n= arccos \\left( \\frac{27}{ 7.2801099 * 6.7082039} \\right) \\\\\n= 0.984997\n\\end{aligned}\n\\]\nwhich gives an angle of \\(\\theta\\) = 0.984997 radians between the vector \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). In degrees, this angle is \\(\\theta\\) = 0.984997 * \\(\\frac{180}{\\pi}\\) = 56.4361716 degrees.\nIn R, this angle can be found by finding the dot product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)\n\nu <- c(1, 4, 6)\nv <- c(-5, 2, 4)\nsum(u * v) # dot product of u and v\n\n[1] 27\n\n\nas well as the lengths of these two vectors\n\nsqrt(sum(u^2)) # length of u\n\n[1] 7.28011\n\nsqrt(sum(v^2)) # length of v\n\n[1] 6.708204\n\n\nCombining these, the angle \\(\\theta\\) can be calculate in radians as\n\ntheta <- acos(sum(u * v) / (sqrt(sum(u^2)) * sqrt(sum(v^2))))\ntheta\n\n[1] 0.984997\n\n\nand in degrees this is\n\ntheta * 180 / pi\n\n[1] 56.43617"
  },
  {
    "objectID": "22-orthogonal.html#orthogonal-sets",
    "href": "22-orthogonal.html#orthogonal-sets",
    "title": "23  Inner product, length, and orthogonality",
    "section": "23.4 Orthogonal sets",
    "text": "23.4 Orthogonal sets\nThe set of vectors \\(\\mathcal{S} = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) in \\(\\mathcal{R}^n\\) is said to be an orthogonal set if every pair of vectors is orthogonal. In other words, for all \\(i \\neq j\\), \\(\\mathbf{v}_i' \\mathbf{v}_j = 0\\). The set is called an orthonormal set if the set of vectors are orthogonal and for \\(i = 1, \\ldots, p\\), each vector \\(\\mathbf{v}_i\\) in the set has length \\(\\| \\mathbf{v}_i \\| = 1\\).\n\nExample 23.5 Show the set of vectors \\(\\left\\{ \\mathbf{v}_1 = \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\mathbf{v}_2 = \\begin{pmatrix} -\\frac{1}{2} \\\\ -2 \\\\ \\frac{7}{2} \\end{pmatrix}, \\mathbf{v}_3 = \\begin{pmatrix} -1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\right\\}\\) is orthogonal\n\nShow these are orthogonal using R\n\n\nIf the set of vectors \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) are an orthogonal set, then the set of vectors \\(\\left\\{ \\frac{\\mathbf{v}_1}{\\|\\mathbf{v}_1\\|}, \\ldots, \\frac{\\mathbf{v}_p}{\\|\\mathbf{v}_p\\|} \\right\\}\\) is an orthonormal set. Note that for each \\(i\\), the length of the vector \\(\\frac{\\mathbf{v}_i} {\\|\\mathbf{v}_i \\|} = 1\\)\n\nTheorem 23.2 Let the set \\(\\mathcal{S} = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) be an orthogonal set of nonzero vectors in \\(\\mathcal{R}^n\\). Then, the set of vectors in \\(\\mathcal{S}\\) are linearly independent and therefore are a basis for the space spanned by \\(\\mathcal{S}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAssume the set of vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\) are linearly dependent. Then, there exist coefficients \\(c_1, \\ldots, c_p\\) such that\n\\[\n\\begin{aligned}\n\\mathbf{0} & = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p\n\\end{aligned}\n\\]\nThen, multiplying both equations on the left by \\(\\mathbf{v}_1'\\) gives\n\\[\n\\begin{aligned}\n0 = \\mathbf{v}_1' \\mathbf{0} & = \\mathbf{v}_1' (c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p) \\\\\n& = c_1 \\mathbf{v}_1' \\mathbf{v}_1 + c_2  \\mathbf{v}_1' \\mathbf{v}_2 + \\cdots + c_p  \\mathbf{v}_1' \\mathbf{v}_p \\\\\n& = c_1 \\mathbf{v}_1' \\mathbf{v}_1 + c_2  0 + \\cdots + c_p 0 \\\\\n& = c_1 \\mathbf{v}_1' \\mathbf{v}_1\n\\end{aligned}\n\\]\nwhich is only equal to 0 when \\(c_1\\) is equal to 0 because \\(\\mathbf{v}_1\\) is a nonzero vector. The above left multiplication could be repeated for each vector \\(\\mathbf{v}_i\\) which gives all \\(c_i\\) must equal 0. As the only solution to the starting equation has all 0 coefficients, the set of vectors \\(\\mathcal{S}\\) must be linearly independent.\n\n\n\nA set of orthogonal vectors is called an orthogonal basis.\n\nTheorem 23.3 Let \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) be an orthogonal basis of the subspace \\(\\mathcal{W}\\) of \\(\\mathcal{R}^n\\). Then for each \\(\\mathbf{x} \\in \\mathcal{W}\\), the coefficients for the linear combination of basis vectors \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) for the vector \\(\\mathbf{x}\\) are\n\\[\n\\begin{aligned}\n\\mathbf{x} & = \\frac{\\mathbf{x}'\\mathbf{v}_1}{\\mathbf{v}_1'\\mathbf{v}_1} \\mathbf{v}_1 + \\frac{\\mathbf{x}'\\mathbf{v}_2}{\\mathbf{v}_2'\\mathbf{v}_2} \\mathbf{v}_2 + \\cdots +  \\frac{\\mathbf{x}'\\mathbf{v}_p}{\\mathbf{v}_p'\\mathbf{v}_p} \\mathbf{v}_p \\\\\n& = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\\\\n\\end{aligned}\n\\]\nwhere \\(c_j = \\frac{\\mathbf{x}'\\mathbf{v}_j}{\\mathbf{v}_j'\\mathbf{v}_j}\\). In other words, the coordinates of the vector \\(\\mathbf{x}\\) with respect to the orthogonal basis \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) are the linear projection of the vector \\(\\mathbf{x}\\) on the respective vectors \\(\\mathbf{v}_j\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe orthogonality of the basis \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) gives\n\\[\n\\begin{aligned}\n\\mathbf{x}'\\mathbf{v}_j & = \\left(c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\right)' \\mathbf{v}_j \\\\\n& = c_j \\mathbf{v}_j' \\mathbf{v}_j\n\\end{aligned}\n\\]\nBecause we know that \\(\\mathbf{v}_j'\\mathbf{v}_j\\) is not zero (a vector can’t be orthogonal to itself), we can divide the above equality by \\(\\mathbf{v}_j' \\mathbf{v}_j\\) and solve for \\(c_j = \\frac{\\mathbf{x}'\\mathbf{v}_j}{\\mathbf{v}_j'\\mathbf{v}_j}\\)\n\n\n\nThus, for a vector \\(\\mathbf{x}\\) in the standard basis, the coordinates of \\(\\mathbf{x}\\) with respect to an orthogonal basis can be easily calculated using dot products (rather than matrix inverses) which is an easier computation.\nIn fact, this is exactly the idea of using least squares estimation (linear regression, spline regression, etc.)."
  },
  {
    "objectID": "22-orthogonal.html#orthogonal-projections",
    "href": "22-orthogonal.html#orthogonal-projections",
    "title": "23  Inner product, length, and orthogonality",
    "section": "23.5 Orthogonal projections",
    "text": "23.5 Orthogonal projections\n\nDefinition 23.6 Let \\(\\mathbf{x}\\) be a vector in \\(\\mathcal{R}^n\\) and let \\(\\mathcal{W}\\) be a subspace of \\(\\mathcal{R}^n\\). Then the vector \\(\\mathbf{x}\\) can be written as the orthogonal decomposition\n\\[\n\\begin{aligned}\n\\mathbf{x} = \\mathbf{x}_{\\mathcal{W}} + \\mathbf{x}_{\\mathcal{W}^\\perp}\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{x}_{\\mathcal{W}}\\) is the vector in \\(\\mathcal{W}\\) that is closest to \\(\\mathbf{x}\\) and is called the orthogonal projection of \\(\\mathbf{x}\\) onto \\(\\mathcal{W}\\) and \\(\\mathbf{x}_{\\mathcal{W}^\\perp}\\) is the orthogonal projection of \\(\\mathbf{x}\\) onto \\(\\mathcal{W}^{\\perp}\\), the subspace \\(\\mathcal{W}^\\perp\\) of \\(\\mathcal{R}^n\\) that is complementary to \\(\\mathcal{W}\\) and is called the orthogonal complement.\n\nDraw picture in class - W is a plane, orthogonal projection of a vector onto the plane\nThis leads to the projection theorem that decomposes a vector \\(\\mathbf{x} \\in \\mathcal{R}^n\\) into components that are\n\nTheorem 23.4 Let \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) be an orthogonal basis of the subspace \\(\\mathcal{W}\\) of \\(\\mathcal{R}^n\\). Then for each \\(\\mathbf{x} \\in \\mathcal{R}^n\\), the orthogonal projection of \\(\\mathbf{x}\\) onto \\(\\mathcal{W}\\) is given by\n\\[\n\\begin{aligned}\n\\mathbf{x}_{\\mathcal{W}} & = \\frac{\\mathbf{x}'\\mathbf{v}_1}{\\mathbf{v}_1'\\mathbf{v}_1} \\mathbf{v}_1 + \\frac{\\mathbf{x}'\\mathbf{v}_2}{\\mathbf{v}_2'\\mathbf{v}_2} \\mathbf{v}_2 + \\cdots +  \\frac{\\mathbf{x}'\\mathbf{v}_p}{\\mathbf{v}_p'\\mathbf{v}_p} \\mathbf{v}_p \\\\\n& = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\\\\n\\end{aligned}\n\\]\nwhere the coefficient \\(c_j\\) corresponding to the vector \\(\\mathbf{v}_j\\) of the linear combination of vectors \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) is given by \\(c_j = \\frac{\\mathbf{x}'\\mathbf{v}_j}{\\mathbf{v}_j'\\mathbf{v}_j}\\). In other words, the coordinates of the vector \\(\\mathbf{x}\\) with respect to the orthogonal basis \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) are the linear projection of the vector \\(\\mathbf{x}\\) on the respective vectors \\(\\mathbf{v}_j\\).\n\nYou might be wondering what use orthogonal projections are. In fact, linear regression (and most common regression models) use orthogonal projections to fit a line (or surface) of best fit. This leads to the important theorem that allows us to project a vector \\(\\mathbf{y} \\in \\mathcal{R}^n\\) onto the column space of a \\(n \\times p\\) matrix \\(\\mathbf{X}\\) (which is exactly the linear regression of \\(\\mathbf{y}\\) onto \\(\\mathbf{X}\\)).\n\nTheorem 23.5 (Orthogonal Projection Theorem) Let \\(\\mathbf{X}\\) be a \\(n \\times p\\) matrix, let \\(\\mathcal{W} =\\) col(\\(\\mathbf{X}\\)), and let \\(\\mathbf{y}\\) be a vector in \\(\\mathcal{R}^n\\). Then the matrix equation\n\\[\n\\begin{aligned}\n\\mathbf{X}'\\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}' \\mathbf{y}\n\\end{aligned}\n\\]\nwith respect to the unknown coefficients \\(\\boldsymbol{\\beta}\\) is consistent and \\(\\mathbf{y}_{\\mathcal{W}} = \\mathbf{X}\\boldsymbol{\\beta}\\) for any solution \\(\\boldsymbol{\\beta}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nShow this in class\n\n\n\nIn addition, if the columns of \\(\\mathbf{X}\\) are linearly independent, then the coefficients \\(\\boldsymbol{\\beta}\\) are given by\n\\[\n\\begin{aligned}\n\\boldsymbol{\\beta} = \\left( \\mathbf{X}'\\mathbf{X} \\right)^{-1} \\mathbf{X}' \\mathbf{y}\n\\end{aligned}\n\\]\nwhich is the least squares solution to the linear regression problem. For example, let X and y be defined as below\n\nX <- cbind(1, c(2, -1, 3, -4, 5, 7, -2, 3))\ny <- c(5, 3, 4, -9, 11, 12, -5, 6)\n\nPlotting this data shows the strong positive linear relationship\n\n# The first column is a basis for a constant term (the intercept)\ndata.frame(x = X[, 2], y = y) %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_point()\n\n\n\n\nWe can solve for the coefficients beta using the linear project theorem\n\nbeta <- solve(t(X) %*% X) %*% t(X) %*% y\n\nand using this solution, solve for the projection \\(\\mathbf{y}_{\\mathcal{W}}\\) of \\(\\mathbf{y}\\) onto \\(\\mathbf{X}\\)\n\ny_W <- X %*% beta\n\nPlotting the projection \\(\\mathbf{y}_{\\mathcal{W}}\\) gives\n\n# The first column is a basis for a constant term (the intercept)\ndata.frame(x = X[, 2], y = y, y_W = y_W) %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x, y = y_W))\n\n\n\n\nThe complement of the project (called the residuals in statistics) is given by \\(\\mathbf{y}_{\\mathcal{W}^\\perp} = \\mathbf{y} - \\mathbf{y}_{\\mathcal{W}}\\)\n\ny_W_perp <- y - y_W\n\nand can be visualized as the orthogonal projection using segments\n\n# The first column is a basis for a constant term (the intercept)\ndata.frame(x = X[, 2], y = y, y_W = y_W) %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x, y = y_W)) +\n    geom_segment(aes(x = x, y = y_W, xend = x, yend = y_W + y_W_perp), color = \"blue\")\n\n\n\n\nRecall that the orthogonal projection gives the “closest” vector \\(\\mathbf{y}_W\\) to \\(\\mathbf{y}\\) that is in the subspace \\(\\mathcal{W}\\) that is the span of the column space of \\(\\mathbf{X}\\). See https://www.enchufa2.es/archives/least-squares-as-springs-the-shiny-app.html for an example."
  },
  {
    "objectID": "23-limits.html",
    "href": "23-limits.html",
    "title": "24  Graphs and Limits",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dasc2594)\nset.seed(2021)\nHere we start a transition to topics in vector calculus. We will start with a discussion of functions of two variables (although the functions are not assumed to be linear). We define a function of two variables explicitly as \\(z = f(x, y)\\)."
  },
  {
    "objectID": "23-limits.html#graphs-and-level-curves",
    "href": "23-limits.html#graphs-and-level-curves",
    "title": "24  Graphs and Limits",
    "section": "24.1 Graphs and level curves",
    "text": "24.1 Graphs and level curves\n\nExample 24.2 The parabola\nConsider the function of two variables\n\\[\n\\begin{aligned}\nf(x, y) = x^2 + y^2\n\\end{aligned}\n\\]\nwhich defines the surface\n\n\n\n\n\nThe 3-dimensional surface can be represented in 2-dimensions using level curves (think of a topographic map)\n\ndata.frame(expand.grid(x, y)) %>%\n    rename(x = Var1, y = Var2) %>%\n    mutate(z = parabola(x, y)) %>%\n    ggplot(aes(x = x, y = y, z = z)) +\n    geom_contour() +\n    coord_fixed(ratio = 1)\n\n\n\n\nwhere each curve in the (x, y) plane has exactly the same value of \\(f(x, y)\\). Alternatively, this can be represented using filled level curves\n\ndata.frame(expand.grid(x, y)) %>%\n    rename(x = Var1, y = Var2) %>%\n    mutate(z = parabola(x, y)) %>%\n    ggplot(aes(x = x, y = y, z = z)) +\n    geom_contour_filled() +\n    coord_fixed(ratio = 1)\n\n\n\n\nNotice that although the original parabola was continuous, these 2-d representations simplify the diagram by representing the contours as discrete values.\n\n\nExample 24.3 A saddle function\nConsider the function of two variables\n\\[\n\\begin{aligned}\nf(x, y) = x^2 - y^2\n\\end{aligned}\n\\]\nwhich defines the surface\n\n\n\n\n\nThe 3-dimensional surface can be represented in 2-dimensions using level curves (think of a topographic map)\n\ndata.frame(expand.grid(x, y)) %>%\n    rename(x = Var1, y = Var2) %>%\n    mutate(z = saddle(x, y)) %>%\n    ggplot(aes(x = x, y = y, z = z)) +\n    geom_contour() +\n    coord_fixed(ratio = 1)\n\n\n\n\nwhere each curve in the (x, y) plane has exactly the same value of \\(f(x, y)\\). Alternatively, this can be represented using filled level curves\n\ndata.frame(expand.grid(x, y)) %>%\n    rename(x = Var1, y = Var2) %>%\n    mutate(z = saddle(x, y)) %>%\n    ggplot(aes(x = x, y = y, z = z)) +\n    geom_contour_filled() +\n    coord_fixed(ratio = 1)\n\n\n\n\nNotice that although the original saddle was continuous, these 2-d representations simplify the diagram by representing the contours as discrete values."
  },
  {
    "objectID": "23-limits.html#limits",
    "href": "23-limits.html#limits",
    "title": "24  Graphs and Limits",
    "section": "24.2 Limits",
    "text": "24.2 Limits\nFor functions of several variables, we have to define limits and continuity for these multivariable settings. For now, we focus on two variable functions as the multivariable case follows similar from the two variable case.\nLet \\(P(x, y) \\rightarrow P_0(a, b)\\) be a path in the \\(x-y\\) plane that starts at the point \\(P(x, y)\\) and ends at the point \\(P_0(a, b)\\) with coordinates \\((a, b)\\). Thus, we can understand the limit as the fixed value of \\(f(x, y)\\) for which all paths that connect the points \\(P(x, y)\\) that are “close” to \\(P_0(a, b)\\) converge to.\nFor one-dimensional limits, “close” was defined as distance. Thus, for multivariable functions, “close” is defined as the Euclidean distance defined by a “ball” of radius \\(\\delta\\) and the limits examines the function output as the radius \\(\\delta\\) goes to 0.\nRecall that the distance \\(dist((x, y), (a, b))\\) between two points \\((x, y)\\) and \\((a, b)\\) is\n\\[\n\\begin{aligned}\ndist((x, y), (a, b)) = \\sqrt{(x-a)^2 + (y-b)^2}\n\\end{aligned}\n\\]\n** Draw images**\n\nDefinition 24.2 (Limit of a Function of Two Variables) The function f(x, y) has limit \\(L\\) as \\(P(x, y) \\rightarrow P_0(a, b)\\), written\n\\[\n\\begin{aligned}\n\\lim_{(x, y) \\rightarrow (a, b)} f(x, y) = \\lim_{P \\rightarrow P_0} f(x, y) = L,\n\\end{aligned}\n\\]\nif, for any \\(\\epsilon > 0\\) (the radius of the ball that defines the “closeness” of the point), there exists a \\(\\delta > 0\\) such that\n\\[\n\\begin{aligned}\n|f(x, y) - L| < \\epsilon\n\\end{aligned}\n\\]\nwhenever \\((x, y)\\) is in the domain of \\(f\\) and\n\\[\n\\begin{aligned}\n0 < \\sqrt{(x-a)^2 + (y-b)^2} < \\delta.\n\\end{aligned}\n\\]\n\nIn the definition, as the value of \\(\\delta\\) is getting smaller, the distance between the set of points \\(P(x, y)\\) within radius \\(\\delta\\) of the point \\(P_0(a, b)\\) is getting smaller. As a consequence, the limit in the definition above exists only if \\(f(x, y)\\) approaches the value \\(L\\) along all possible paths in the domain of \\(f\\).\n\nExample 24.4 In class notes * Future work: write out hand-written examples\n\n\nTheorem 24.1 Let \\(L\\) and \\(M\\) be real numbers and let \\(\\lim_{(x, y) \\rightarrow (a, b)}f(x, y) = L\\) and \\(\\lim_{(x, y) \\rightarrow (a, b)}g(x, y) = M\\) for functions \\(f(x, y)\\) and \\(g(x, y)\\). Let \\(c\\) be a constant and \\(n>0\\), then:\n\nSum of limits:\n\n\\[\n\\begin{aligned}\n\\lim_{(x, y) \\rightarrow (a, b)} \\left( f(x, y) + g(x, y) \\right) = L + M\n\\end{aligned}\n\\]\n\nDifference of limits:\n\n\\[\n\\begin{aligned}\n\\lim_{(x, y) \\rightarrow (a, b)} \\left( f(x, y) - g(x, y) \\right) = L - M\n\\end{aligned}\n\\]\n\nScalar multiple of the limit:\n\n\\[\n\\begin{aligned}\n\\lim_{(x, y) \\rightarrow (a, b)} c f(x, y) = c L\n\\end{aligned}\n\\]\n\nProduct of limits:\n\n\\[\n\\begin{aligned}\n\\lim_{(x, y) \\rightarrow (a, b)} f(x, y) g(x, y) = LM\n\\end{aligned}\n\\]\n\nQuotient of limits: As long as \\(M>0\\) we have\n\n\\[\n\\begin{aligned}\n\\lim_{(x, y) \\rightarrow (a, b)} \\frac{f(x, y)}{g(x, y)} = \\frac{L}{M}\n\\end{aligned}\n\\]\n\nPower of the limit:\n\n\\[\n\\begin{aligned}\n\\lim_{(x, y) \\rightarrow (a, b)} f(x, y)^n = L^n\n\\end{aligned}\n\\]\n\nRoot of the limit: If \\(n\\) is even, we assume \\(L > 0\\)\n\n\\[\n\\begin{aligned}\n\\lim_{(x, y) \\rightarrow (a, b)} f(x, y)^{1/n} = L^{1/n}\n\\end{aligned}\n\\]\n\n\nExample 24.5 Use the rules above to evaluate the limit \\(\\lim_{(x, y) \\rightarrow (2, 3)} 4x^3y + \\sqrt{xy}\\)\n\n\n24.2.1 Boundary points\n\nDefinition 24.3 Define a region \\(\\mathcal{D}\\) in \\(\\mathcal{R}^2\\).\n\nAn interior point \\(P\\) of \\(\\mathcal{D}\\) is a point that lies entirely in the region \\(\\mathcal{D}\\). Mathematically, a point \\(P\\) is an interior point of \\(\\mathcal{D}\\) if it is possible to define a ball of radius \\(\\epsilon>0\\) centered at \\(P\\) such that this ball only contains points within \\(\\mathcal{D}\\).\nA boundary point \\(P\\) of \\(\\mathcal{D}\\) is a point that lies on the edge of the region \\(\\mathcal{D}\\). Mathematically, a point \\(P\\) is an boundary point of \\(\\mathcal{D}\\) if every ball of radius \\(\\epsilon>0\\) centered at \\(P\\) contains at least one point in \\(\\mathcal{D}\\) and one point outside \\(\\mathcal{D}\\).\n\n\n\nExample 24.6 Let \\(f(x, y) = \\sqrt{1 - x^2 - y^2}\\). The boundary points are all the points on the unit circle an the interior points are all the points on the interior of the unit disk. draw picture\n\n\nDefinition 24.4 A region \\(\\mathcal{D}\\) is said to be open if it only contains interior points (i.e., \\(\\mathcal{D}\\) has no boundary points). A region \\(\\mathcal{D}\\) is said to be closed if the region contains all its boundary points.\n\nFigure: limit paths along the boundary\n\nExample 24.7 Consider the \\(\\lim_{(x, y) \\rightarrow (4, 4)} \\frac{x^2 - y^2}{x - y}\\). Because the point \\((4, 4)\\) is not a valid point in the domain (can’t divide by \\(4-4=0\\)), the point \\((4, 4)\\) is a boundary point of the domain. The boundary of the domain that is not contained in the domain is the set of points \\(x = y\\). Assuming we are not taking a path along the boundary, we know that \\(x \\neq y\\) in the interior of the domain. Hence,\n\\[\n\\begin{aligned}\n\\lim_{(x, y) \\rightarrow (4, 4)} \\frac{x^2 - y^2}{x - y} & = \\lim_{(x, y) \\rightarrow (4, 4)} \\frac{(x - y)(x + y)}{x - y} \\\\\n& = \\lim_{(x, y) \\rightarrow (4, 4)} x + y  = 4 + 4 = 8\\\\\n\\end{aligned}\n\\]\nfor all paths that do not cross the line \\(y = x\\).\n\n\nExample 24.8 nonexistence of limit in class"
  },
  {
    "objectID": "23-limits.html#continuity",
    "href": "23-limits.html#continuity",
    "title": "24  Graphs and Limits",
    "section": "24.3 Continuity",
    "text": "24.3 Continuity\nA very important property of functions is continuity. In a general sense, a function is continuous if two nearby input values result in nearby output values. As a graph, this means that there are no hops, skips, or jumps.\n\nDefinition 24.5 The function \\(f(x, y)\\) is said to be continuous at the point \\((a, b)\\) if the following are true\n\n\\(f(a, b)\\) is defined at \\((a, b)\\)\n\\(\\lim_{(x, y) \\rightarrow (a, b)} f(x, y)\\) exists\n\\(\\lim_{(x, y) \\rightarrow (a, b)} f(x, y) = f(a, b)\\)\n\n\n\nExample 24.9 checking continuity in class \\(f(x, y) = \\begin{cases} \\frac{x^2 - y^2}{x - y} & \\mbox{ if } x \\neq y \\\\ x + y & \\mbox{ if } x = y \\\\ \\end{cases}\\)"
  },
  {
    "objectID": "24-partial-derivatives.html",
    "href": "24-partial-derivatives.html",
    "title": "25  Partial Derivatives",
    "section": "",
    "text": "library(tidyverse)\nlibrary(plotly)\nlibrary(dasc2594)\nset.seed(2021)\nRecall that for a function of one variable, the derivative gives the rate of change of the function with respect to that variable. The function \\(f(x)\\) has an instantaneous rate of change \\(\\frac{d}{dx}f(x)\\), assuming the derivative \\(\\frac{d}{dx}f(x)\\) exists.\nThis concept can be extended to functions of multivariables where we now have to specify a direction in which the function changes. For example, consider a mountain which is very steep in the north/south direction but is much less steep in the east/west direction. Thus, the directional derivative in the north/south direction will have a larger absolute value (higher rate of change) than the directional derivative in the east/west direction.\nFor example, consider the function\nNote that the partial derivative asks the question “What is the rate of change of one variable holding all the other variables constant?” Thus, the question can be phrased as what is the derivative of the function \\(f(x, y)\\) at the point \\((a, b)\\) where we only let one variable change? To make the notation of a partial derivative clear, a special symbol is used where \\(\\frac{\\partial}{\\partial x}\\) is the partial derivative with respect to the \\(x\\) variable (holding the y variable constant).\ndraw surfaces with marginal slices\nNotice that you can find partial derivatives by holding all the other variables constant and then finding the equivalent univariate derivative."
  },
  {
    "objectID": "24-partial-derivatives.html#higher-order-partial-derivatives",
    "href": "24-partial-derivatives.html#higher-order-partial-derivatives",
    "title": "25  Partial Derivatives",
    "section": "25.1 Higher-order partial derivatives",
    "text": "25.1 Higher-order partial derivatives\nWe can calculate the partial derivatives of partial derivatives. The derivatives could be with respect to the same variable repeatedly or the derivatives could be with respect to different variables in which case we call these mixed partial derivatives. Notation for higher order partial derivatives is \\(\\frac{\\partial^2}{\\partial x \\partial y} f(x, y) = f_{xy}(x, y)\\) which says first take the partial derivative of with respect to \\(y\\) then take the partial derivative with respect to \\(x\\). The possible sets of second-order partial derivatives for functions of two variables are shown in the table below\n\n\n\n\n\n\n\nNotation 1\nNotation 2\n\n\n\n\n\\(\\frac{\\partial}{\\partial x}\\frac{\\partial}{\\partial x} f(x, y) = \\frac{\\partial^2}{\\partial x^2} f(x, y)\\)\n\\(f_{xx}(x, y)\\)\n\n\n\\(\\frac{\\partial}{\\partial y}\\frac{\\partial}{\\partial y} f(x, y) = \\frac{\\partial^2}{\\partial y^2} f(x, y)\\)\n\\(f_{yy}(x, y)\\)\n\n\n\\(\\frac{\\partial}{\\partial x}\\frac{\\partial}{\\partial y} f(x, y) = \\frac{\\partial^2}{\\partial x \\partial y} f(x, y)\\)\n\\(f_{xy}(x, y)\\)\n\n\n\\(\\frac{\\partial}{\\partial y}\\frac{\\partial}{\\partial x} f(x, y) = \\frac{\\partial^2}{\\partial y \\partial x} f(x, y)\\)\n\\(f_{yx}(x, y)\\)\n\n\n\n\nExample 25.3 Find the four second-order partial derivatives of \\(f(x, y) = 3x^2y^3 + 4xy - 3x^2\\)\n\nNote that the order in which mixed partial derivatives are taken can sometimes change the result. However, it is often the case that the order of the partial derivatives can be switched.\n\nTheorem 25.1 Let the function \\(f(x, y)\\) be defined on an open domain \\(\\mathcal{D}\\) of \\(\\mathcal{R}^2\\) and assume that \\(f_{xy}\\) and \\(f_{yx}\\) are continuous over the domain \\(\\mathcal{D}\\). Then, \\(f_{xy} = f_{yx}\\) for all points in the domain \\(\\mathcal{D}\\).\n\nMany of the commonly used functional forms in data science meet the criteria above. Thus, for many of the commonly used functions in data science, the order of evaluation of partial derivatives often does not matter. In practice, it is always good practice to verify this though."
  },
  {
    "objectID": "25-chain-rule.html",
    "href": "25-chain-rule.html",
    "title": "26  The chain rule",
    "section": "",
    "text": "library(tidyverse)\nlibrary(plotly)\nlibrary(dasc2594)\nset.seed(2021)\nRecall the univariate chain rule: If \\(y = f(x)\\) is a function of \\(x\\) and \\(z = g(y)\\) is a function of \\(y\\), a question of interest is “What is the change in \\(z\\) relative to change in \\(x\\)?\nWe can write \\(z = g(y) = g(f(x))\\) and using this notation, the change in \\(z\\) with respect to the variable \\(x\\) is \\(\\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx} = \\frac{df(y)}{dy}\\frac{dg(x)}{dx}\\). Written in functional form\n\\[\n\\begin{aligned}\n(g(f(x)))' = (g \\circ f)'(x) = g'(f(x)) f'(x)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "25-chain-rule.html#the-chain-rule-with-one-independent-variable",
    "href": "25-chain-rule.html#the-chain-rule-with-one-independent-variable",
    "title": "26  The chain rule",
    "section": "26.1 The chain rule with one independent variable",
    "text": "26.1 The chain rule with one independent variable\nDrawing in class\n\nDefinition 26.1 (Chain Rule For One Independent Variable) Let \\(z\\) be a differentiable function of two variables \\(x\\) and \\(y\\) so that \\(z = f(x, y)\\) and let \\(x=g(t)\\) be a function of \\(t\\) and \\(y=h(t)\\) a function of \\(t\\). Written in functional form, \\(z\\) can be written as \\(z = f(x, y) = f(g(t), h(t))\\), with \\(x=g(t)\\) and \\(y=h(t)\\). Then we can define the derivative of \\(z\\) with respect to \\(t\\) as\n\\[\n\\begin{aligned}\n\\frac{dz}{dt} = \\frac{\\partial z}{ \\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt}\n\\end{aligned}\n\\]\n\nFor the definition above, we have the dependent variable \\(z\\) and we have intermediate variables \\(x\\) and \\(y\\).\nNotice in the definition above that there is a mix of partial derivatives (\\(\\partial\\)) and ordinary derivatives (\\(d\\)).\n\n\n\n\n\n\n\n\nExample 26.2 Let \\(z = x^2 + e^y\\) and let \\(x = \\cos(t)\\) and \\(y = \\sin(t)\\)\n\nThe results from above can also be extended to have more than two intermediate variables.\nDraw picture"
  },
  {
    "objectID": "25-chain-rule.html#the-chain-rule-with-several-independent-variables",
    "href": "25-chain-rule.html#the-chain-rule-with-several-independent-variables",
    "title": "26  The chain rule",
    "section": "26.2 The chain rule with several independent variables",
    "text": "26.2 The chain rule with several independent variables\nOften, functions will have more than one independent variables.\n\nDefinition 26.2 (Chain Rule For Two Independent Variables) Let \\(z\\) be a differentiable function of two variables \\(x\\) and \\(y\\) so that \\(z = f(x, y)\\) and let \\(x=g(t, s)\\) be a function of \\(s\\) and \\(t\\) and \\(y=h(s, t)\\) a function of \\(s\\) and \\(t\\). Written in functional form, \\(z\\) can be written as \\(z = f(x, y) = f(g(s, t), h(s, t))\\), with \\(x=g(s, t)\\) and \\(y=h(s, t)\\). Then we can define the partial derivative of \\(z\\) with respect to \\(s\\) as\n\\[\n\\begin{aligned}\n\\frac{\\partial z}{\\partial s} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial s} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial s}\n\\end{aligned}\n\\]\nthe partial derivative of \\(z\\) with respect to \\(t\\) as\n\\[\n\\begin{aligned}\n\\frac{\\partial z}{\\partial t} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t}\n\\end{aligned}\n\\]\n\n\nExample 26.3 Let \\(z = f(x, y) = x^2 e^y\\) and let \\(x = 2s - t\\) and \\(y = 4s^3-3t^2\\)"
  },
  {
    "objectID": "25-chain-rule.html#the-chain-rule-in-matrix-notation",
    "href": "25-chain-rule.html#the-chain-rule-in-matrix-notation",
    "title": "26  The chain rule",
    "section": "26.3 The chain rule in matrix notation",
    "text": "26.3 The chain rule in matrix notation\nTo get a better understanding of the chain rule, it helps to show the chain rule using matrix notation. Using the matrix notation will enable you to apply the chain rule to any number of intermediate variables. For example, consider the extension of the definition for the chain rule of a function with one independent variable.\n\nDefinition 26.3 (Matrix Chain Rule For One Independent Variable) Let \\(z\\) be a differentiable function of two variables \\(x\\) and \\(y\\) so that \\(z = f(x, y)\\) and let \\(x=g(t)\\) be a function of \\(t\\) and \\(y=h(t)\\) a function of \\(t\\). Written in functional form, \\(z\\) can be written as \\(z = f(x, y) = f(g(t), h(t))\\), with \\(x=g(t)\\) and \\(y=h(t)\\). Then we can define the derivative of \\(z\\) with respect to \\(t\\) as\n\\[\n\\begin{aligned}\n\\frac{dz}{dt} = \\frac{\\partial z}{ \\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt}\n\\end{aligned}\n\\]\nWritten in matrix notation, this is\n\\[\n\\begin{aligned}\n\\frac{dz}{dt} = \\begin{pmatrix} \\frac{\\partial z}{ \\partial x} & \\frac{\\partial z}{\\partial y} \\end{pmatrix} \\begin{pmatrix} \\frac{dx}{dt} \\\\  \\frac{dy}{dt} \\end{pmatrix} = \\frac{\\partial z}{ \\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt}\n\\end{aligned}\n\\]\n\nThe definition above for the chain rule with two variables is given by\n\nDefinition 26.4 (Chain Rule For Two Independent Variables) Let \\(z\\) be a differentiable function of two variables \\(x\\) and \\(y\\) so that \\(z = f(x, y)\\) and let \\(x=g(t, s)\\) be a function of \\(s\\) and \\(t\\) and \\(y=h(s, t)\\) a function of \\(s\\) and \\(t\\). Written in functional form, \\(z\\) can be written as \\(z = f(x, y) = f(g(s, t), h(s, t))\\), with \\(x=g(s, t)\\) and \\(y=h(s, t)\\). Then we can define the partial derivative of \\(z\\) with respect to \\(s\\) as\n\\[\n\\begin{aligned}\n\\frac{\\partial z}{\\partial s} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial s} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial s}\n\\end{aligned}\n\\]\nwhich, in matrix notation is\n\\[\n\\begin{aligned}\n\\frac{dz}{dt} = \\begin{pmatrix} \\frac{\\partial z}{ \\partial x} & \\frac{\\partial z}{\\partial y} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial x}{\\partial s} \\\\  \\frac{\\partial y}{\\partial s} \\end{pmatrix} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial s} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial s}\n\\end{aligned}\n\\]\nThe partial derivative of \\(z\\) with respect to \\(t\\) as\n\\[\n\\begin{aligned}\n\\frac{\\partial z}{\\partial t} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t}\n\\end{aligned}\n\\]\nwhich, in matrix notation is\n\\[\n\\begin{aligned}\n\\frac{dz}{dt} = \\begin{pmatrix} \\frac{\\partial z}{ \\partial x} & \\frac{\\partial z}{\\partial y} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial x}{\\partial t} \\\\  \\frac{\\partial y}{\\partial t} \\end{pmatrix} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t}\n\\end{aligned}\n\\]\n\nThis use of matrix notation for derivatives will be useful in understanding the gradient."
  },
  {
    "objectID": "26-gradient.html",
    "href": "26-gradient.html",
    "title": "27  The gradient and directional derivatives",
    "section": "",
    "text": "library(tidyverse)\nlibrary(plotly)\nlibrary(dasc2594)\nset.seed(2021)\nPartial derivatives tell about how a rate of function changes in a particular direction (in the direction of a coordinate).\nThink about trying to find the maximum of a real-valued function (finding the minimum is equivalent to finding the maximum of the negative value of the function). Finding the maximum of a function is analogous to hiking up a mountain and trying to find the highest peak.\nSuppose you are standing on a mountain surface at the point \\((x, y, z)\\) in 3-dimensions where \\(z = f(x, y)\\) is the function that gives the height of the mountain at location \\((x, y)\\). If you are standing at the point \\((a, b)\\) in the \\((x, y)\\) coordinate system, you might want to get to the top of the mountain as quickly as possible. The direction that is the steepest uphill direction can be calculated using the concepts of the directional derivative and the gradient.\nTo understand how the directional derivative relates to partial derivatives, in the definition above, let \\(v_2 = 0\\) and to make \\(\\mathbf{v}\\) a unit vector, set \\(v_1 = 1\\) (\\(\\mathbf{v}\\) is the standard basis vector \\(\\mathbf{e}_1\\)). Then, the limit in the directional derivative definition above becomes \\[\n\\begin{aligned}\n\\lim_{h \\rightarrow 0} \\frac{f(a + h, b) - f(a, b)}{h}\n\\end{aligned}\n\\] which is the definition for the partial derivative of \\(f\\) in the \\(x\\) direction \\(f_x = \\frac{\\partial f}{\\partial x}\\) (?def-partial). Likewise, letting \\(\\mathbf{v} = \\mathbf{e}_2\\), the standard basis vector in the y direction, gives the partial derivative in the y direction \\(f_y = \\frac{\\partial f}{\\partial y}\\). Thus, one could pick any direction vector \\(\\mathbf{v}\\) and then calculate the partial derivative in that direction.\nNow observe that any line that goes through the point \\((a, b)\\) in the direction of the unit vector \\(\\mathbf{v} \\in \\mathcal{R}^2\\) can be written as the set of points \\(\\{(x = a + s v_1, y = b + s v_2) | s \\in \\mathcal{R} \\}\\) which forms a line through the point \\((a, b)\\) in the direction of \\(\\mathbf{v}\\). In this definition, the value \\(s\\) determines the length of the vector because \\(\\mathbf{v}\\) is a unit vector. At \\(s=0\\), this definition corresponds to the point \\((a, b)\\) and as \\(s\\) increases, the points \\((x, y)\\) are the set of points along the line that are distance \\(|s|\\) away from \\((a, b)\\). Notice that this set defines a function \\(g(s) = f(a + s v_1, b + s v_2) = f(x, y)\\) which is a single variable function of the two inputs \\(x\\) and \\(y\\) of \\(f(x, y)\\). Given this definition, the directional derivative of \\(f(x, y)\\) in the direction of \\(\\mathbf{v}\\) at the point \\((a, b)\\) is now given by\n\\[\n\\begin{aligned}\nD_{\\mathbf{v}} f(a, b) & = \\frac{d}{ds}g(s)|_{s=0} \\\\\n& = \\frac{\\partial f}{\\partial x} \\frac{dx}{ds} + \\frac{\\partial f}{\\partial y} \\frac{dy}{ds} |_{s=0} \\\\\n& = f_x(a, b) v_1 + f_y(a, b) v_2 \\\\\n& = \\begin{pmatrix} f_x(a, b) & f_y(a, b) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\\\\n\\end{aligned}\n\\] which is the dot product of the vectors \\(\\begin{pmatrix} f_x(a, b) \\\\ f_y(a, b) \\end{pmatrix}\\) and \\(\\mathbf{v}\\).\nNotice that the vector \\(\\mathbf{v}\\) is a unit vector and therefore the directional derivative is a weighted sum of the partial derivatives in the \\(x\\) and \\(y\\) directions weighted by the vector \\(\\mathbf{v}\\) (weighted sums are sums where the coefficients sum to 1–in this case the sum is in the “distance” metric). As a consequence, we can find the directional derivative in any direction by changing the vector \\(\\mathbf{v}\\)."
  },
  {
    "objectID": "26-gradient.html#the-gradient",
    "href": "26-gradient.html#the-gradient",
    "title": "27  The gradient and directional derivatives",
    "section": "27.1 The Gradient",
    "text": "27.1 The Gradient\nThe directional derivative is a dot product of the partial derivatives and a unit vector. The gradient is similar, but rather than return a single value (a number), the gradient returns a vector at a point \\((a, b)\\).\n\nDefinition 27.3 (The Gradient) Let \\(f(x, y)\\) be a differentiable function at \\((a, b)\\). Then, the gradient of \\(f\\) at \\((a, b)\\) is \\[\n\\begin{aligned}\n\\nabla f(a, b) & = \\begin{pmatrix} \\frac{\\partial f(x, y)}{\\partial x}|_{(x,y) = (a, b)} & \\frac{\\partial f(x, y)}{\\partial y}|_{(x,y) = (a, b)} \\end{pmatrix} \\\\\n& = \\frac{\\partial f(x, y)}{\\partial x}|_{(x,y) = (a, b)} \\mathbf{e}_1 + \\frac{\\partial f(x, y)}{\\partial y}|_{(x,y) = (a, b)} \\mathbf{e}_2,\n\\end{aligned}\n\\] where \\(\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\) are the standard basis vectors in \\(\\mathcal{R}^2\\).\n\nNotice that the directional derivative at the point \\((a , b)\\) can be calculated using the gradient where \\[\n\\begin{aligned}\nD_{\\mathbf{v}} f(a, b) & = \\nabla f(a, b) \\cdot \\mathbf{v} \\\\\n& = \\begin{pmatrix} \\frac{\\partial f(x, y)}{\\partial x}|_{(x,y) = (a, b)} & \\frac{\\partial f(x, y)}{\\partial y}|_{(x,y) = (a, b)} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}\n\\end{aligned}\n\\] the directional derivative is the dot product of the gradient \\(\\nabla f(a, b)\\) at the point \\((a, b)\\) with the unit vector \\(\\mathbf{v}\\).\n\nExample 27.2 Compute the gradient of \\(f(x, y) = 3x^2 + y^2\\) at the point \\((3, 1)\\).\n\ncalculate the gradient\ngraph the gradient using contour plots and segments\n\n\nThe gradient is critical in data science because is the tool that allows for finding the set of parameters for a given model that are “most likely” given the data. The gradient has the property in that at each point \\((a, b)\\) where \\(f(x, y)\\) is differentiable, the gradient points in the direction of the maximum rate of change.\n\nTheorem 27.1 (The gradient and rates of change) Let \\(f(x, y)\\) be a differentiable function at \\((a, b)\\) with \\(\\nabla f(a, b) \\neq 0\\). Then,\n\n\\(f\\) has its maximum rate of increase at the point \\((a, b)\\) in the direction of the gradient \\(\\nabla f(a, b)\\). Because the gradient is a weighted sum of the partial derivatives and the unit vector in the direction of the maximum change, the magnitude of the rate of change is \\(\\|\\nabla f(a, b)\\|\\) which is the length of the gradient vector.\n\\(f\\) has its maximum rate of decrease at the point \\((a, b)\\) in the direction of the gradient \\(-\\nabla f(a, b)\\). The rate of change in the direction of maximum rate of decrease is \\(-\\|\\nabla f(a, b)\\|\\).\nThe directional derivative is 0 in any direction orthogonal to \\(\\nabla f(a, b)\\).\n\n\n\nExample 27.3 Consider the function \\(f(x, y) = 3x^2 - 2xy + y^2\\). At the point \\((3, 1)\\), what is the direction of steepest descent? Steepest ascent?\n\ngraph the function as contours and plot the gradient as a segment\n\n\n\n27.1.1 The gradient and the tangent line\nTheorem 27.1 states that the directional derivative at the point \\((a, b)\\) is 0 in any direction that is orthogonal to \\(\\nabla f(a, b)\\). Because the directional derivative is the rate of change of the function in the direction of \\(\\mathbf{v}\\), the directional derivative being 0 means that the function \\(f(x, y)\\) is not is not changing in the direction of the vector \\(\\mathbf{v}\\). Therefore, we know that the vector \\(\\mathbf{v}\\) and the vector \\(\\nabla f(x, y)|_{(a,b)}\\) are orthogonal because the definition of the directional derivative in ?def-directional-gradient states that \\[\n\\begin{aligned}\nD_{\\mathbf{v}} f(a, b)  = \\nabla f(a, b) \\cdot \\mathbf{v} = \\left(\\nabla f(a, b) \\right)'  \\mathbf{v} = 0\n\\end{aligned}\n\\] which is only true if the gradient \\(\\nabla f(a, b)\\) is orthogonal to \\(\\mathbf{v}\\). Because the vector \\(\\mathbf{v}\\) points in the direction of 0 change in \\(f(x,y)\\), the vector \\(\\mathbf{v}\\) is a tangent line to the level curve. See drawing\nUsing this, one can calculate the tangent to the level curve at the point \\((a, b)\\) as the dot-product equation \\[\n\\begin{aligned}\n\\begin{pmatrix} f_x(x, y) & f_y(x, y) \\end{pmatrix} \\begin{pmatrix} x - a \\\\ y - b \\end{pmatrix} =  f_x (x-a) + f_y (y-b) = 0.\n\\end{aligned}\n\\]\n\nExample 27.4 For the function \\(f(x, y) = 3x^2 - 2xy + y^3\\), find a vector orthogonal to the gradient \\(\\nabla f(a, b)\\) at the point \\((a, b) = (2, 1)\\).\n\n\n\n27.1.2 The gradient in higher dimensions\nWe can extend the gradient to higher dimensional functions. Let \\(\\mathbf{x} = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n \\end{pmatrix}'\\) be a vector in \\(\\mathcal{R}^n\\). Then the gradient of \\(f\\) at a point \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_n)\\) is\n\\[\n\\begin{aligned}\n\\nabla f(\\mathbf{a}) = \\begin{pmatrix} \\frac{\\partial f(\\mathbf{x})}{\\partial x_1}|_{\\mathbf{x} = \\mathbf{a}} \\\\ \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}|_{\\mathbf{x} = \\mathbf{a}} \\\\  \\vdots \\\\ \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}|_{\\mathbf{x} = \\mathbf{a}} \\end{pmatrix} = \\begin{pmatrix} f_{x_1}(\\mathbf{a}) \\\\ f_{x_2}(\\mathbf{a}) \\\\  \\vdots \\\\ f_{x_n}(\\mathbf{a}) \\end{pmatrix}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "27-tangents.html",
    "href": "27-tangents.html",
    "title": "28  Tangent planes and linear approximations",
    "section": "",
    "text": "Let \\(f(\\mathbf{x})\\) be a differentiable function at a point \\(\\mathbf{a}\\). Because the function is differentiable at \\(\\mathbf{a}\\), this means that all paths \\(P_\\mathbf{a}\\) that approach the point \\(\\mathbf{a}\\) from all directions all take on values \\(f(P_\\mathbf{a})\\) that are “close” to \\(f(\\mathbf{a})\\). Mathematically, we describe this as smoothness. A more explicit description says that as the paths \\(P_{\\mathbf{a}}\\) get very close to \\(\\mathbf{a}\\), the space over which these paths are defined starts to look more and more like a flat surface–the tangent plane.\n\nExample 28.1 For this example, we plot the function \\(f(x, y) = x^2 + y^2\\) which has gradient \\(\\nabla f(x, y) = \\begin{pmatrix} 2x \\\\ 2y\\end{pmatrix}\\)\n\n# f(x, y)\ntarget_fun <- function(x, y) {\n    return(x^2 + y^2)\n}\n# gradient f(x, y)\ngrad_fun <- function(x, y) {\n    c(2 * x, 2 * y) # notice that the return value is a vector\n}\n# plot \nplot_tangent_plane(target_fun = target_fun, grad_fun = grad_fun, a=-1, b = 1)\n# zoomed in plot\nplot_tangent_plane(target_fun = target_fun, grad_fun = grad_fun, a=-1, b = 1, xlim = c(-1.5, 0.5), ylim = c(0.5, 1.5))\n# super zoomed in plot\nplot_tangent_plane(target_fun = target_fun, grad_fun = grad_fun, a=-1, b = 1, xlim = c(-1.1, -0.9), ylim = c(0.9, 1.1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA consequence of this result that when you zoom in on a differentiable function the function looks like a flat plane is that the function \\(f(x, y)\\) can be approximated locally as a linear function (local approximation just means that if you are really “close” to the point \\((a, b)\\) that the function will behave like a tangent plane if the function is differentiable). Intuitively, this makes sense as if the derivative exists, the directional derivatives are just vectors and a linear combination of vectors (in \\(\\mathcal{R}^2\\)) produces a tangent plane (in higher dimensions, this is called a hyperplane). This means that if the function \\(f(x, y)\\) is differentiable at the point \\((a, b)\\), then \\(f(x, y)\\) for points \\((x, y)\\) close to \\((a, b)\\) is approximated by the linear tangent plane.\nNotice in the code above that there are two functions needed to calculate the tangent plane: the function \\(f(x, y)\\) and the gradient \\(\\nabla f(x, y)\\). This can be seen in the definition of the tangent plane.\n\nDefinition 28.1 (The Tangent plane) Let \\(f(x, y)\\) be a differentiable function at the point \\((a, b)\\). Then the tangent plane to the surface defined by the function \\(f(x, y)\\) at the point \\((a, b)\\) is given by the equation\n\\[\n\\begin{aligned}\nz & = f(a, b) + f_x(a, b) (x - a) + f_y(a, b) (y - b) \\\\\n& = f(a, b) + \\nabla f(x, y) |_{(a, b)} \\cdot \\begin{pmatrix} x - a \\\\ y - b \\end{pmatrix} \\\\\n& = f(a, b) + (\\nabla f(x, y) |_{ (a, b)})' \\begin{pmatrix} x - a \\\\ y - b \\end{pmatrix} \\\\,\n\\end{aligned}\n\\]\nwhere the tangent plane is defined as the dot product of the graidient vector \\(\\nabla f(x, y) |_{(a, b)}\\) evaluated at the point \\((a, b)\\) and the vector \\(\\begin{pmatrix} x - a \\\\ y - b \\end{pmatrix}\\) that is the coordinate-wise distance of the point \\((x, y)\\) from the point \\((a, b)\\).\n\n\nExample 28.2 Find the equation for the tangent plane for the function \\(f(x, y) = x^2 \\cos(y) - y^2 \\cos(x)\\) at the point \\((\\frac{\\pi}{2}, \\frac{pi}{4})\\)\n\ncalculate by hand\nplot using plot_tangent_plane()\n\n\nThis leads to the linearization equation for functions of \\(n\\) variables.\n\nDefinition 28.2 (The Linearization of a Function) Let \\(f(\\mathbf{x})\\) be a differentiable function at the point \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_n)'\\) for a function of inputs \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)' \\in \\mathcal{R}^n\\). Then the linearization of the function \\(f(\\mathbf{x})\\) at the point \\(\\mathbf{a}\\) is given by the equation\n\\[\n\\begin{aligned}\nL(\\mathbf{x}) & = f(\\mathbf{a}) + \\nabla f(\\mathbf{x}) |_{\\mathbf{a}} \\cdot (\\mathbf{x} - \\mathbf{a}) \\\\\n& = f(\\mathbf{a}) + \\left( \\nabla f(\\mathbf{x}) |_{\\mathbf{a}} \\right)' (\\mathbf{x} - \\mathbf{a})\n\\end{aligned}\n\\]\n\nThe quality of the linearization is high for points “close” to \\(\\mathbf{a}\\) and has higher error (defined as \\(\\|L(\\mathbf{x}) - f(\\mathbf{x})\\|\\)) as \\(\\mathbf{x}\\) gets further from \\(\\mathbf{a}\\).\nFor points \\(\\mathbf{x}\\) “close” to the point \\(\\mathbf{a}\\), the exact difference in the function \\(z = f(\\mathbf{x})\\) is given by \\(\\Delta z = f(\\mathbf{x}) - f(\\mathbf{a})\\). Plugging in the linear approximation, the differential \\(d z = L(\\mathbf{x}) - f(\\mathbf{a})\\) is the linear approximation to the exact different \\(\\Delta z\\). Define \\(d \\mathbf{x} = \\begin{pmatrix} d x_1 \\\\ d x_2 \\\\ \\vdots \\\\ d x_n \\end{pmatrix} = \\begin{pmatrix} x_1 - a_1 \\\\ x_2 - a_2 \\\\ \\vdots \\\\ x_n - a_n \\end{pmatrix}\\) as the set of changes in each of the \\(n\\) coordinates with respect to the standard basis \\(\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}\\) so that the linear change \\(d z\\) of \\(f(\\mathbf{x})\\) at \\(\\mathbf{a}\\) is given by\n\\[\n\\begin{aligned}\nd z & =  \\nabla f(\\mathbf{x}) |_{\\mathbf{a}} \\cdot (\\mathbf{x} - \\mathbf{a}) \\\\\n& = \\nabla f(\\mathbf{x}) |_{\\mathbf{a}} \\cdot d \\mathbf{x} \\\\\n& = \\sum_{i=1}^n \\frac{\\partial f(\\mathbf{x})}{\\partial x_i} d x_i,\n\\end{aligned}\n\\]\nwhere the last term is a sum of the linear approximation in each of the \\(i = 1, \\ldots, n\\) coordinate directions.\n\nExample 28.3 Approximate the linear change of the function \\(f(x, y, z) = x^2 - 3xy^2z^2 - 4z^2\\) at the point \\((a, b, c) = (1, -2, -1)\\) evaluated at the point \\((x, y, z) = (0.95, -2.05, -1.05)\\). Compare this to the exact value of the function \\(f(x, y, z)\\)\n\n\n\nFirst, we find the gradient\n\\[\n\\begin{aligned}\n\\nabla f(x, y, z) = \\begin{pmatrix} 2x - 3y^2z^2 \\\\ -6xyz^2 \\\\ -6xy^2z - 8z \\end{pmatrix}\n\\end{aligned}\n\\]\nand evaluate the gradient at \\((a, b, c) = (1, -2, -1)\\) to get\n\\[\n\\begin{aligned}\n\\nabla f(1, -2, -1) = \\begin{pmatrix} 2(1) - 3(-2)^2(-1)^2 \\\\ -6(1)(-2)(-1)^2 \\\\ -6(1)(-2)^2(-1) - 8(-1) \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 12 \\\\ 32 \\end{pmatrix}\n\\end{aligned}\n\\]\nThe function evaluated at the point \\((a, b, c)\\) is \\(f(1, -2, -1) = (1)^2 - 3(1)(-2)^2(-1)^2 - 4(-1)^2 = -15\\).\nTherefore, the linear approximation \\(L(x, y, z)\\) of \\(f(x, y, z)\\) at the point \\((a, b, c)\\) is\n\\[\n\\begin{aligned}\nL(x, y, z) & = f(a, b, c) + \\nabla f(a, b, c) \\cdot \\begin{pmatrix} x - 1 \\\\ y - (-2) \\\\ z - (-1) \\end{pmatrix} \\\\\n& = -15 + 10(x - 1) - 12 (y + 2) - 32 (z + 1)\n\\end{aligned}\n\\]\nThe linear approximation evaluated at the point \\((0.95, -2.05, -1.05)\\) is\n\\[\n\\begin{aligned}\nL(0.95, -2.05, -1.05) & = -15 + 10(0.95 - 1) - 12 (-2.05 + 2) - 32 (-1.05 + 1) = -16.7.\n\\end{aligned}\n\\]\nCompared to the true value of the function \\(f(x, y, z)\\) is\n\\[\n\\begin{aligned}\nf(0.95, -2.05, -1.05) & = (1.05)^2 - 3(1.05)(-2.05)^2(-1.05)^2 - 4(-1.05)^2 = -16.71228,\n\\end{aligned}\n\\]\nwhich gives an approximation error of \\(f(x, y, z) - L(x, y, z) = -16.7122803 - -16.7 = -0.0122803\\)\n\ntarget_fun <- function(x, y, z) {\n        x^2 - 3 * x * y^2 * z^2 - 4 * z^2\n} \n        \ngrad_fun <- function(x, y, z) {\n        c(2*x - 3 * y^2 * z^2,\n          -6 * x * y * z^2,\n          -6 * x * y^2 * z - 8 * z)\n}\n\ngrad_fun(1, -2, -1)\n\n[1] -10  12  32\n\ntarget_fun(0.95, -2.05, -1.05)\n\n[1] -16.71228\n\nlinearization <- function(target_fun, grad_fun, x, y, z, a, b, c) {\n        target_fun(a, b, c) + sum(grad_fun(a, b, c) * c(x - a, y - b, z - c))\n}\n\n\nlinearization(target_fun, grad_fun, 0.95, -2.05, -1.05, 1, -2, -1)\n\n[1] -16.7\n\ntarget_fun(0.95, -2.05, -1.05)\n\n[1] -16.71228\n\n# approximation error\ntarget_fun(0.95, -2.05, -1.05) - linearization(target_fun, grad_fun, 0.95, -2.05, -1.05, 1, -2, -1)\n\n[1] -0.01228031"
  },
  {
    "objectID": "28-max-min.html",
    "href": "28-max-min.html",
    "title": "29  Minimums and Maximums",
    "section": "",
    "text": "library(tidyverse)\nlibrary(plotly)\nlibrary(dasc2594)\nset.seed(2021)\nIn general, we talk about finding either minimums or maximum values of a function. For simplicity, we focus here on finding the minimum value of a function \\(f(\\cdot)\\) because finding the maximum value of \\(f(\\cdot)\\) is equivalent to finding the minimum value of \\(-f(\\cdot)\\).\nTo characterize these minimum values, we consider two different types of minimums: local minimums and global minimums. For now, we focus on functions of two variables but these ideas are similar for functions of many variables."
  },
  {
    "objectID": "28-max-min.html#local-minimums",
    "href": "28-max-min.html#local-minimums",
    "title": "29  Minimums and Maximums",
    "section": "29.1 Local minimums",
    "text": "29.1 Local minimums\nA local minimum of the function \\(f(x, y)\\) is a point \\((a, b)\\) where the values of the function \\(f(a, b)\\) evaluated at \\((a, b)\\) is less than or equal to the function \\(f(a + \\Delta x, b + \\Delta y)\\) at any nearby point \\((a + \\Delta x, b + \\Delta y)\\), where \\(\\Delta x\\) and \\(\\Delta y\\) are very small values.\n\nDefinition 29.1 Let \\((a, b)\\) be a point in the domian \\(\\mathcal{D}\\) of the function \\(f(x, y)\\). If there exists an \\(\\epsilon > 0\\) such that \\(\\|(x, y) - (a, b)\\| < \\epsilon\\) (the point \\((x, y)\\) is close to the point \\((a, b)\\) for a given disk of radius \\(\\epsilon\\) at the point \\((a, b)\\)), then if \\(f(a, b) \\leq f(x, y)\\), the point \\((a, b)\\) is called a local minimum value of the function \\(f(x, y)\\).\n\nIn terms of finding a minimum of a surface, the local minimum is any point on the surface from which one cannot walk downhill if one can only take small steps. Like in univariate functions, local minimums also have a relationship to the partial derivatives.\n\nTheorem 29.1 If \\(f(x, y)\\) has a local minimum at the point \\((a, b)\\) and \\(f(x, y)\\) is a differentiable function at \\((a, b)\\), then the partial derivatives \\(\\frac{\\partial f(x, y)}{\\partial x}|_{(a, b)} = f_x(a, b) = 0\\) and \\(\\frac{\\partial f(x, y)}{\\partial y}|_{(a, b)} = f_y(a, b) = 0\\)\n\nNote: the converse is not necessarily true: just because \\(f_x(a, b) = 0\\) and \\(f_y(a, b) = 0\\), this doesn’t mean that we have a minimum point at \\((a, b)\\).\nAs stated above, the fact that the partial derivatives \\(f_x(a, b) = 0\\) and \\(f_y(a, b) = 0\\) does not imply that the point \\((a, b)\\) is a local minimum. Instead, the partial derivatives being 0 only implies that \\((a, b)\\) is a critical point of the function \\(f(x, y)\\).\n\nDefinition 29.2 The point \\((a, b)\\) is a critical point of the function \\(f(x, y)\\) if either\n\n\\(f_x(a, b) = 0\\) and \\(f_y(a, b) = 0\\) or\nat least one partial derivative does not exist at \\((a, b)\\).\n\n\n\nExample 29.1 find the critical points of \\(f(x, y) = xy(x - 3)(y - 4)\\). * plot the critical points using plotly()\n\nf <- function(x, y) {\n    x * y * (x - 3) * (y - 4)\n}\nx <- seq(3/2 + -3, 3/2 + 3, length.out = 40)\ny <- seq(2 + -3, 2 + 3, length.out = 40)\n\ncritical_points <- data.frame(x = c(3/2, 0, 3, 0, 3), y = c(2, 0, 0, 4, 4)) %>%\n    mutate(z = f(x, y), color = c(\"red\", rep(\"orange\", 4)), name = c(\"local minimum\", rep(\"saddle point\", 4)))\ndat <- expand_grid(x, y) %>%\n    mutate(z = f(x, y))\n\ndat %>%\n    ggplot(aes(x = x, y = y, z = z)) +\n    geom_contour(bins = 40) +\n    coord_fixed(ratio = 1) +\n    geom_point(data = critical_points, aes(color = name), size = 2) +\n    scale_color_manual(values = critical_points$color)\n\n\n\n\n\nplot_ly(x = x, y = y, z = matrix(dat$z, 40, 40)) %>%\n    add_surface(\n        contours = list(\n            z = list(\n                show=TRUE,\n                usecolormap=TRUE,\n                highlightcolor=\"#ff0000\",\n                project=list(z=TRUE)))) %>%\n    add_trace(x = critical_points$x, y = critical_points$y, \n              z = critical_points$z,\n              mode = \"markers\", type = \"scatter3d\", \n              marker = list(size = 5, \n                            color = c(\"red\", rep(\"orange\", 4)), \n                            symbol = 104),\n              name = c(\"local maximum\", rep(\"saddle point\", 4)))\n\n\n\n\n\n\n\n\n\n\n\nIf a critical point isn’t a local minimum/maximum, what else could this point be? One possible shape is what is called a saddle point. A saddle point is one in which the function \\(f(x, y)\\) is increasing in one direction (say, along the x-axis) and decreasing along another direction (say, the y-axis). The surface is often said to resemble the shape of a saddle (or a Pringles chip). An example saddle point is shown in the plot below.\n\nf <- function(x, y) {\n    x^2 - y^2\n}\nx <- seq(-1, 1, length.out = 40)\ny <- seq(-1, 1, length.out = 40)\ncritical_points <- data.frame(x = 0, y = 0) %>%\n    mutate(z = f(x, y))\ndat <- expand_grid(x, y) %>%\n    mutate(z = f(x, y))\n\nplot_ly(x = x, y = y, z = matrix(dat$z, 40, 40)) %>%\n    add_surface(\n        contours = list(\n            z = list(\n                show=TRUE,\n                usecolormap=TRUE,\n                highlightcolor=\"#ff0000\",\n                project=list(z=TRUE)))) %>%\n    add_trace(x = critical_points$x, y = critical_points$y, \n              z = critical_points$z,\n              mode = \"markers\", type = \"scatter3d\", \n              marker = list(size = 5, \n                            color = \"red\",\n                            symbol = 104),\n              name = \"saddle point\")\n\n\n\n\n\n\n\n\n\n\n\nDefinition 29.3 (A Saddle Point) A saddle point of a differentiable function \\(f(x, y)\\) is a critical point \\((a, b)\\) of \\(f(x, y)\\) where for all \\(\\epsilon > 0\\) there exists points \\((x, y)\\) within the disk of radius \\(\\epsilon\\) of the point \\((a, b)\\) (i.e., \\(\\|(x, y) - (a, b)\\| < \\epsilon\\)) where \\(f(x, y) > f(a, b)\\) and where \\(f(x, y) < f(a, b)\\). In other words, there are points nearby the critical point that have both higher and lower values than the value \\(f(a, b)\\).\n\nFinding critical points is a first step in finding minimums, however, we need a method to determine whether a critical point is a local minimum/maximum or a saddle point. To determine this, the second derivative test is useful.\n\nTheorem 29.2 (The Second Derivative Test) Let \\(f_x(a, b) = f_y(a, b) = 0\\) so that the point \\((a, b)\\) is a critical point. Also let the \\(f(x, y)\\) be second-order differentiable (the second partial derivatives exists) throughout the open disk \\(\\| (x, y) - (a, b)\\|< \\epsilon\\) for some \\(\\epsilon > 0\\). Then\n\nIf \\(f_{xx}(a, b)f_{yy}(a, b) - (f_{xy}(a, b))^2 > 0\\) and \\(f_{xx}(a, b) < 0\\), then \\(f(x, y)\\) has a local maximum at \\((a, b)\\).\nIf \\(f_{xx}(a, b)f_{yy}(a, b) - (f_{xy}(a, b))^2 > 0\\) and \\(f_{xx}(a, b) > 0\\), then \\(f(x, y)\\) has a local minimum at \\((a, b)\\).\nIf \\(f_{xx}(a, b)f_{yy}(a, b) - (f_{xy}(a, b))^2 < 0\\), then \\(f(x, y)\\) has a saddle point at \\((a, b)\\).\nIf \\(f_{xx}(a, b)f_{yy}(a, b) - (f_{xy}(a, b))^2 = 0\\), the test is inconclusive.\n\n\n\nExample 29.2 find the critical points of \\(f(x, y) = xy(x - 3)(y - 4)\\) and determine if they are local maxima, minima, saddle points, or undetermined.\n\n\nExample 29.3 find the critical points of \\(f(x, y) = x^4 + y^4\\) and determine if they are local maxima, minima, saddle points, or undetermined.\n\n\nf <- function(x, y) {\n    x^4 + y^4\n}\nx <- seq(-1, 1, length.out = 40)\ny <- seq(-1, 1, length.out = 40)\ncritical_points <- data.frame(x = 0, y = 0) %>%\n    mutate(z = f(x, y))\ndat <- expand_grid(x, y) %>%\n    mutate(z = f(x, y))\n\nplot_ly(x = x, y = y, z = matrix(dat$z, 40, 40)) %>%\n    add_surface(\n        contours = list(\n            z = list(\n                show=TRUE,\n                usecolormap=TRUE,\n                highlightcolor=\"#ff0000\",\n                project=list(z=TRUE)))) %>%\n    add_trace(x = critical_points$x, y = critical_points$y, \n              z = critical_points$z,\n              mode = \"markers\", type = \"scatter3d\", \n              marker = list(size = 5, \n                            color = \"red\",\n                            symbol = 104),\n              name = \"saddle point\")"
  },
  {
    "objectID": "28-max-min.html#global-maximums-and-minimums",
    "href": "28-max-min.html#global-maximums-and-minimums",
    "title": "29  Minimums and Maximums",
    "section": "29.2 Global maximums and minimums",
    "text": "29.2 Global maximums and minimums\nWe have shown how you can find critical points \\((a, b)\\) by finding points that have 0 partial derivatives in each direction (\\(f_x(a, b) = f_y(a, b) = 0\\)) and we have shown how to characterize these critical points using the second derivative test. In data science, we are often interested in finding the “best” model, not just the “best local” model. So a question arises is whether the local minima/maxima that we find are the global minima/maxima. To determine this, we have to define the global minima/maxima.\n\nDefinition 29.4 If \\(f(x, y)\\) is a function defined over a domain (or a subset of the domain) \\(\\mathcal{D} \\in \\mathcal{R}^2\\) that contains the point \\((a, b)\\), we say\n\n\\((a, b)\\) is a global maximum if \\(f(a, b) \\geq f(x, y)\\) for all \\((x, y) \\in \\mathcal{D}\\) and\n\\((a, b)\\) is a global minimum if \\(f(a, b) \\leq f(x, y)\\) for all \\((x, y) \\in \\mathcal{D}\\).\n\n\nThus, once you have found all the critical points, you also need to check all the boundary points for the domain (or subdomain) \\(\\mathcal{D}\\) as the global minimum/maximum might lie on the boundary.\nIn general finding the global maximum/minimum points can be done by applying the following procedure:\n\nLet \\(f(x, y)\\) be a function on a closed (the set contains its boundary points) and bounded (the set is not infinite) set. Then, the global minima/maxima can be found by\n\nDetermining the values \\(f(x, y)\\) at all the critical points.\nDetermining the values of \\(f(x, y)\\) at all the boundary points of \\(\\mathcal{D}\\).\nFinding the minimum/maximum of the set of function values from 1) and 2).\n\n\n\nExample 29.4 Let \\(f(x, y) = x^2 - 4xy + y^2\\) over the region \\([-2,2]\\times[-2, 2]\\)\n\nIn data science problems, it is often difficult to analyze the functions analytically (using the tools of calculus directly). Instead, one can use numeric techniques to find minima and maxima. Two techniques that we will discuss are grid search and gradient descent."
  },
  {
    "objectID": "28-max-min.html#grid-search-optimization",
    "href": "28-max-min.html#grid-search-optimization",
    "title": "29  Minimums and Maximums",
    "section": "29.3 Grid search optimization",
    "text": "29.3 Grid search optimization\nIf the number of input variables in the function is small, one can find global minima and maxima using grid search. To find a global minima/maxima in \\(\\mathcal{R}^2\\), one creates a grid over the domain \\(\\mathcal{D}\\) and evaluates the function \\(f(x, y)\\) at these grid points and finds the numerical solution.\n\nExample 29.5 Let \\(f(x, y) = x^2 - 2xy + y^2\\) over the region \\([-2,2]\\times[-2, 2]\\)\n\ntarget_fun <- function(x, y) {\n    x^2 - 4 * x * y + y^2\n}\n# N is the grid size\nN <- 1000\nx <- seq(-2, 2, length.out = N)\ny <- seq(-2, 2, length.out = N)\ndat <- expand_grid(x = x, y = y) %>%\n    mutate(z = target_fun(x, y))\n# find the minimum\ndat %>%\n    filter(z == min(z))\n\n# A tibble: 2 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1    -2    -2    -8\n2     2     2    -8\n\n# find the minimum\ndat %>%\n    filter(z == max(z))\n\n# A tibble: 2 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1    -2     2    24\n2     2    -2    24"
  },
  {
    "objectID": "28-max-min.html#gradient-descent",
    "href": "28-max-min.html#gradient-descent",
    "title": "29  Minimums and Maximums",
    "section": "29.4 Gradient Descent",
    "text": "29.4 Gradient Descent\nAnother method to find minima/maxima is gradient descent."
  }
]