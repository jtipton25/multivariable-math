# Inner product, length, and orthogonality


```{r, message = FALSE}
library(tidyverse)
library(dasc2594)
set.seed(2021)
```


:::{.definition}
Let $\mathbf{u}$ and $\mathbf{v}$ be vectors in $\mathcal{R}^n$. Then, the **inner product** of $\mathbf{u}$ and $\mathbf{v}$ is $\mathbf{u}' \mathbf{v}$. The vectors $\mathbf{u}$ and $\mathbf{v}$ are $n \times 1$ matrices where $\mathbf{u}'$ is a $1 \times n$ matrix and the inner product $\mathbf{u}' \mathbf{v}$ is a scalar ($1 \times 1$ matrix). The inner product is also sometimes called the dot product and written as $\mathbf{u} \cdot \mathbf{v}$. 

If the vectors
\begin{align*}
\mathbf{u} & = \begin{pmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{pmatrix} & & \mathbf{v} & = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix} 
\end{align*}
then $\mathbf{u}' \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots u_n v_n$
:::

:::{.example}
Find the inner product $\mathbf{u}'\mathbf{v}$ and $\mathbf{v}'\mathbf{u}$ of
\begin{align*}
\mathbf{u} & = \begin{pmatrix} 2 \\ -3 \\ 1 \end{pmatrix} & & \mathbf{v} & = \begin{pmatrix} 4 \\ -2 \\ 3 \end{pmatrix} 
\end{align*}

* do by hand
```{r}
u <- c(2, -3, 1)
v <- c(4, -2, 3)
# u'v
sum(u*v)
t(u) %*% v
# v'u
sum(v*u)
t(v) %*% u
```
:::

The properties of inner products are defined with the following theorem.

:::{.theorem}
Let $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ be vectors in $\mathcal{R}^n$ and let $c$ be a scalar. Then

a) $\mathbf{u}'\mathbf{v} = \mathbf{v}'\mathbf{u}$

b) $(\mathbf{u} + \mathbf{v})' \mathbf{w} = \mathbf{u}' \mathbf{w} + \mathbf{v}' \mathbf{w}$

c) $( c \mathbf{u} )' \mathbf{v} = c ( \mathbf{v}'\mathbf{u} )$

d) $\mathbf{u}'\mathbf{u} \geq 0$ with $\mathbf{u}'\mathbf{u} = 0$ only when $\mathbf{u} = \mathbf{0}$

:::


Based on the theorem above, the inner product of a vector with itself ($\mathbf{u}'\mathbf{u}$) is strictly non-negative. Thus, we can define the length of the vector $\mathbf{u}$ (also called the **norm** of the vector $\mathbf{u}$).


:::{.definition}
The length of a vector $\mathbf{v} \in \mathcal{R}^n$, also called the vector **norm** $\| \mathbf{v} \|$ is defined as
\begin{align*}
\| \mathbf{v} \| & = \sqrt{\mathbf{v}'\mathbf{v}} = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}
\end{align*}
:::

:::{.example}
Let $\mathbf{v} = \begin{pmatrix} a \\ b \end{pmatrix} \in \mathcal{R}^2$. Show that the definition of the norm satisfies the Pythagorean theorem.
:::


Another property of the norm is how the norm changes based on scalar multiplication. Let $\mathbf{v} \in \mathcal{R}^n$ be a vector and let $c$ be a scalar. Then $\|c \mathbf{v}\| = |c|\|\mathbf{v}\| $ 

:::{.definition}
A vector $mathbf{v} \in \mathcal{R}^n$ whose length/norm is 1 is called a **unit** vector. Any vector can be made into a unit vector through **normalization** by multiplying the vector $\mathbf{v}$ by $\frac{1}{\|\mathbf{v}\|}$ to get a unit vector $\mathbf{u} = \frac{\mathbf{v}}{\|\mathbf{v}\|}$ in the same direction as $\mathbf{v}$.
:::


## Distance

In two dimensions, the Euclidean distance between the points $(x_1, y_1)$ and $(x_2, y_2)$ is defined as $\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$. In higher dimensions, a similar definition holds.

:::{.definition}
Let $\mathbf{u}$ and $\mathbf{v}$ be vectors in $\mathcal{R}^n$. Then the distance $dist(\mathbf{u}, \mathbf{v})$ between $\mathbf{u}$ and $\mathbf{v}$ is 

\begin{align*}
dist(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|
\end{align*}
:::

:::{.example}
Distance between two 3-dimensional vectors
```{r}
u <- c(3, -5, 1)
v <- c(4, 3, -2)
sqrt(sum((u-v)^2))
```
:::

## Orthogonal vectors

The equivalent of perpendicular lines in $\mathcal{R}^n$ are known as orthogonal vectors.
<!-- Geometrically, two vectors are defined as orthogonal if the distance between $\mathbf{u}$ and $\mathbf{v}$ is the same as the distance between $\mathbf{u}$ and $-\mathbf{v}$. **Draw picture -- perpendicular triangle** -->
<!-- \begin{align*} -->
<!-- dist(\mathbf{u}, - \mathbf{v}) & =  -->
<!-- \end{align*} -->
:::{.definition}
The two vectors $\mathbf{u}$ and $\mathbf{v}$ in $\mathcal{R}^n$ are orthogonal if 
\begin{align*}
\mathbf{u}' \mathbf{v} = 0
\end{align*}
:::


## Angles between vectors

Let $\mathbf{u}$ and $\mathbf{v}$ be vectors $\mathcal{R}^n$. Then, the angle between the vectors $\mathbf{u}$ and $\mathbf{v}$ is defined as the angle $\theta$ in the relationship
\begin{align*}
\mathbf{u}' \mathbf{v} = \| \mathbf{u} \| \ |\mathbf{v} \| cos(\theta)
\end{align*}


## Orthogonal sets 

The set of vectors $\mathcal{S} = \{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$ in $\mathcal{R}^n$ is said to be an **orthogonal set** if every pair of vectors is orthogonal. In other words, for all $i \neq j$, $\mathbf{v}_i' \mathbf{v}_j = 0$. The set is called an **orthonomal set** if the set of vectors are orthogonal and for $i = 1, \ldots, p$, each vector $\mathbf{v}_i$ in the set has length $\| \mathbf{v}_i \| = 1$.



:::{.example}
Show the set of vectors $\{ \mathbf{v}_1 = \begin{pmatrix} 3 \\ 1 \\ 1 \end{pmatrix}, \mathbf{v}_2 = \begin{pmatrix} -\frac{1}{2} \\ -2 \\ \frac{7}{2} \end{pmatrix}, \mathbf{v}_3 = \begin{pmatrix} -1 \\ 2 \\ 1 \end{pmatrix} \}$

* Show these are orthogonal using `R`
:::


If the set of vectors $\{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$ are an orthogonal set, then the set of vectors $ \{ \frac{\mathbf{v}_1}{\|\mathbf{v}_1\|}, \ldots, \frac{\mathbf{v}_p}{\|\mathbf{v}_p\|} \}$ is an orthonormal set. Note that for each $i$, the length of the vector $\|\mathbf{v}_i\| = 1$

:::{.theorem}
Let the set $\mathcal{S} = \{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$ be an orthogonal set of nonzero vectors in $\mathcal{R}^n$. Then, the set of vectors in $\mathcal{S}$ are linearly independent and therefore are a basis for the space spanned by $\mathcal{S}$.
:::

:::{.proof}
Assume the set of vectors $\mathbf{v}_1, \ldots, \mathbf{v}_p$ are linearly dependent. Then, there exist coefficients $c_1, \ldots, c_p$ such that
\begin{align*}
\mathbf{0} & = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_p \mathbf{v}_p 
\end{align*}
Then, multiplying both equations on the left by $\mathbf{v}_1'$ gives
\begin{align*}
0 = \mathbf{v}_1' \mathbf{0} & = \mathbf{v}_1' (c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_p \mathbf{v}_p) \\
& = c_1 \mathbf{v}_1' \mathbf{v}_1 + c_2  \mathbf{v}_1' \mathbf{v}_2 + \cdots + c_p  \mathbf{v}_1' \mathbf{v}_p \\
& = c_1 \mathbf{v}_1' \mathbf{v}_1 + c_2  0 + \cdots + c_p 0 \\
& = c_1 \mathbf{v}_1' \mathbf{v}_1
\end{align*}
which is only equal to 0 when $c_1$ is equal to 0 because $\mathbf{v}_1$ is a nonzero vector. The above left multiplication could be repeated for each vector $\mathbf{v}_i$ which gives all $c_i$ must equal 0. As the only solution to the starting equation has all 0 coefficients, the set of vectors $\mathcal{S}$ must be linearly independent.
:::


A set of orthogonal vectors is called an **orthogonal basis**.


:::{.theorem}
Let $\{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$ be an orthogonal basis of the subspace $\mathcal{W}$ of $\mathcal{R}^n$. Then for each $\mathbf{x} \in \mathcal{W}$, the coefficients for the linear combination of basis vectors $\{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$ for the vector $\mathbf{x}$ are

\begin{align*}
\mathbf{x} & = \frac{\mathbf{x}'\mathbf{v}_1}{\mathbf{v}_1'\mathbf{v}_1} \mathbf{v}_1 + \frac{\mathbf{x}'\mathbf{v}_2}{\mathbf{v}_2'\mathbf{v}_2} \mathbf{v}_2 + \cdots +  \frac{\mathbf{x}'\mathbf{v}_p}{\mathbf{v}_p'\mathbf{v}_p} \mathbf{v}_p \\
& = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_p \mathbf{v}_p \\
\end{align*}
where $c_j = \frac{\mathbf{x}'\mathbf{v}_j}{\mathbf{v}_j'\mathbf{v}_j}$. In other words, the coordinates of the vector $\mathbf{x}$ with respect to the orthogonal basis $\{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$  are the linear projection of the vector $\mathbf{x}$ on the respective vectors $\mathbf{v}_j$.
:::


:::{.proof}
The orthogonality of the basis $\{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$ gives
\begin{align*}
\mathbf{x}'\mathbf{v}_j & = \left(c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_p \mathbf{v}_p \right)' \mathbf{v}_j \\
& = c_j \mathbf{v}_j' \mathbf{v}_j
\end{align*}
Because we know that $\mathbf{v}_j'\mathbf{v}_j$ is not zero (a vector can't be orthogonal to itself), we can divide the above equality by $\mathbf{v}_j' \mathbf{v}_j$ and solve for $c_j = \frac{\mathbf{x}'\mathbf{v}_j}{\mathbf{v}_j'\mathbf{v}_j}$
:::

Thus, for a vector $\mathbf{x}$ in the standard basis, the coordinates of $\mathbf{x}$ with respect to an orthogonal basis can be easily calculated using dot products (rather than matrix inverses) which is an easier computation.


## Orthogonal projections

:::{.definition}
Let $\mathbf{x}$ be a vector in $\mathcal{R}^n$ and let $\mathcal{W}$ be a subspace of $\mathcal{R}^n$. Then the vector $\mathbf{x}$ can be written as the **orthogonal decomposition** 
\begin{align*}
\mathbf{x} = \mathbf{x}_{\mathcal{W}} + \mathbf{x}_{\mathcal{W}^\perp}
\end{align*}
where $\mathbf{x}_{\mathcal{W}}$ is the vector in $\mathcal{W}$ that is closest to $\mathbf{x}$ and is called the **orthogonal projection of $\mathbf{x}$ onto $\mathcal{W}$** and $\mathbf{x}_{\mathcal{W}^\perp}$ is the orthogonal projection of $\mathbf{x}$ onto $\mathcal{W}^{\perp}$, the subspace of $\mathcal{R}^n$ that is complementary to $\mathcal{W}$.
:::

**Draw picture in class - W is a plane, orthogonal projection of a vector onto the plane**

This leads to the projection theorem that decomposes a vector $\mathbf{x} \in \mathcal{R}^n$ into components that are 

:::{.theorem}
Let $\{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$ be an orthogonal basis of the subspace $\mathcal{W}$ of $\mathcal{R}^n$. Then for each $\mathbf{x} \in \mathcal{R}^n$, the orthogonal projection of $\mathbf{x}$ onto $\mathcal{W}$ is given by
\begin{align*}
\mathbf{x}_{\mathcal{W}} & = \frac{\mathbf{x}'\mathbf{v}_1}{\mathbf{v}_1'\mathbf{v}_1} \mathbf{v}_1 + \frac{\mathbf{x}'\mathbf{v}_2}{\mathbf{v}_2'\mathbf{v}_2} \mathbf{v}_2 + \cdots +  \frac{\mathbf{x}'\mathbf{v}_p}{\mathbf{v}_p'\mathbf{v}_p} \mathbf{v}_p \\
& = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_p \mathbf{v}_p \\
\end{align*}
where the coefficient $c_j$ corresponding to the vector $\mathbf{v}_j$ of the linear combination of vectors $\{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$ is given by $c_j = \frac{\mathbf{x}'\mathbf{v}_j}{\mathbf{v}_j'\mathbf{v}_j}$. In other words, the coordinates of the vector $\mathbf{x}$ with respect to the orthogonal basis $\{ \mathbf{v}_1, \ldots, \mathbf{v}_p \}$  are the linear projection of the vector $\mathbf{x}$ on the respective vectors $\mathbf{v}_j$.
:::














