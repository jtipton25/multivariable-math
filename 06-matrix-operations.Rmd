# Matrix operations {#matrix-operations}

* **Note: add examples:**




## Properties of matrices

### Matrix Addition

**Matrix Addition:** If the matrices $\mathbf{A}$ and $\mathbf{B}$ are of the same dimension (e.g., both $\mathbf{A}$ and $\mathbf{B}$ have the same number of rows $m$ and the same number of columns $n$), then

$$
\begin{align*}
(\#eq:matrix-addition)
\mathbf{A} + \mathbf{B} & = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix} + 
\begin{pmatrix} b_{11} & b_{12} & \cdots & b_{1n} \\
b_{21} & b_{22} & \cdots & b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
b_{m1} & b_{m2} & \cdots & b_{mn}
\end{pmatrix} \\
& = \begin{pmatrix} a_{11} + b_{11} & b_{12} + b_{12} & \cdots & a_{1n} + b_{1n} \\
a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2n} + b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} + b_{m1} & a_{m2} + b_{m2} & \cdots & a_{mn} + b_{mn}
\end{pmatrix} \\
& = \left\{ a_{ij} + b_{ij} \right\}
\end{align*}
$$

**... Another way to **

If $\mathbf{A}$ and $\mathbf{B}$ are of the same dimension (same number of rows and columns) you can add the matrices together

```{r}
A <- matrix(c(4, 1, 33, 2, 0, -4), 3, 2)
B <- matrix(c(7, -24, 3, 9, 11, -9), 3, 2)
A
B
A + B
```


We can also write this using `for` loops

```{r}
# initialize an empty matrix to fill
C <- matrix(0, 3, 2)

for (i in 1:nrow(A)) {         # loop over the rows
    for (j in 1:ncol(A)) {     # loop over the columns
        C[i, j] <- A[i, j] + B[i, j]
    }
}
C
```

If $\mathbf{A}$ and $\mathbf{B}$ are of different dimensions (they differ in either the number of rows or columns), `R` will return an error warning you that the matrices are of different sizes and can't be added

```{r, error = TRUE}
A <- matrix(c(4, 1, 33, 2, 0, -4), 3, 2)
B <- matrix(c(7, -24, 3, 9), 2, 2)
A
B
A + B
```


```{theorem}
Let $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ be $m \times n$ matrices and let $a$ and $b$ be scalars, then:

1) $\mathbf{A} + \mathbf{B}  = \mathbf{B} + \mathbf{A}$ \hfill [(commutivity of addition)]{style="float:right"}

2)  $(\mathbf{A} + \mathbf{B}) + \mathbf{C} = \mathbf{A} + (\mathbf{B} + \mathbf{C})$ \hfill [(commutivity of addition)]{style="float:right"}

3) $\mathbf{A} + \mathbf{0}  = \mathbf{A}$  \hfill [(additive identity)]{style="float:right"}

4) $a (\mathbf{A} + \mathbf{B}) = a \mathbf{A} + a \mathbf{B}$ \hfill [(scalar ...)]{style="float:right"}

5) $(a + b)\mathbf{A} = a \mathbf{A} + b \mathbf{A}$ \hfill [(scalar ...)]{style="float:right"} 

6) $(ab)\mathbf{A} = a (b\mathbf{A})$ \hfill [(scalar ...)]{style="float:right"} 
```

### Matrix Multipliation


**Matrix Multiplication:** If $\mathbf{A} = \left\{ a_{ij} \right\}$ is an $m \times n$ matrix and $\mathbf{B} = \left\{ a_{jk} \right\}$ is a $n \times p$ matrix, then the matrix product $\mathbf{C} = \mathbf{A} \mathbf{B}$ is an $m \times p$ matrix where $\mathbf{C} = \left\{ \sum_{j=1}^p a_{ij} b{jk} \right\}$


$$
\begin{align}
(\#eq:matrix-multiplication)
\mathbf{A} \mathbf{B} & = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix} 
\begin{pmatrix} b_{11} & b_{12} & \cdots & b_{1p} \\
b_{21} & b_{22} & \cdots & b_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
b_{n1} & b_{n2} & \cdots & b_{np}
\end{pmatrix} \\
& = \begin{pmatrix} \sum_{j=1}^n a_{1j} b_{j1} & \sum_{j=1}^n a_{1j} b_{j2} & \cdots & \sum_{j=1}^n a_{1j} b_{jp} \\
\sum_{j=1}^n a_{2j} b_{j1} &\sum_{j=1}^n a_{2j} b_{j2} & \cdots & \sum_{j=1}^n a_{2j} b_{jp} \\
\vdots & \vdots & \ddots & \vdots \\
\sum_{j=1}^n a_{mj} b_{j1} &\sum_{j=1}^n a_{nj} b_{j2} & \cdots & \sum_{j=1}^n a_{mj} b_{jp}
\end{pmatrix} 
\end{align}
$$


Another way to define matrix multiplication is through inner product notation. Define the $m \times n$ matrix $\mathbf{A}$ and the $n \times p$ matrix $\mathbf{B}$ as the partition

$$
\begin{align*}
\mathbf{A} & = \begin{pmatrix} \mathbf{a}_{1}' \\
\mathbf{a}_{2}' \\ 
\vdots \\
\mathbf{a}_{m}' \end{pmatrix}
& \mbox{ and } 
&& \mathbf{B} & = \begin{pmatrix} \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{p} \end{pmatrix} 
\end{align*}
$$

where $\mathbf{a}_i$ and $\mathbf{b}_k$ are both $n$-vectors. Then, we have $\mathbf{C} = \mathbf{A} \mathbf{B}$ can be written as

$$
\begin{align*}
\mathbf{A} \mathbf{B}  = \mathbf{A} \begin{pmatrix} \mathbf{b}_1 & \mathbf{b}_2 & \cdots & \mathbf{b}_p
\end{pmatrix}  = \begin{pmatrix} \mathbf{A} \mathbf{b}_1 & \mathbf{A} \mathbf{b}_2 & \cdots & \mathbf{A} \mathbf{b}_p
\end{pmatrix} 
\end{align*}
$$
 
Note that in this representation, each column of the matrix $\mathbf{A}\mathbf{B}$ is a linear combination the the columns of $\mathbf{A}$ with coefficients given by the corresponding column of $\mathbf{B}$.
 
$$
\begin{align*}
\mathbf{A} \mathbf{B} & = \begin{pmatrix} \mathbf{a}_1' \mathbf{b}_1 & \mathbf{a}_1' \mathbf{b}_2 & \cdots & \mathbf{a}_1' \mathbf{b}_q \\
\mathbf{a}_2' \mathbf{b}_1 & \mathbf{a}_2' \mathbf{b}_2 & \cdots & \mathbf{a}_2' \mathbf{b}_q \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{a}_n' \mathbf{b}_1 & \mathbf{a}_n' \mathbf{b}_2  & \cdots & \mathbf{a}_n' \mathbf{b}_q
\end{pmatrix} \\
& = \left\{ \mathbf{a}_i' \mathbf{b}_k  \right\}.
\end{align*}
$$
 
Written in this notation, we arrive at the multiplication rule for $\mathbf{C} = \mathbf{A} \mathbf{B}$ -- the $ik$th element $c_{ik}$ of $\mathbf{C}$ is the inner product of the $i$th row of $\mathbf{A}$ and the $j$th column of $\mathbf{B}$.


### Properties of Matrix Multiplication

Define the $m \times m$ **identity matrix** $\mathbf{I}_m$ with ones on the diagonal and zeros off diagonal as
$$
\mathbf{I}_m = \begin{pmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ 0 & 0 & \ddots & 0 \\ 0 & 0 & \cdots & 1\end{pmatrix}
$$
Let $\mathbf{A}$ be an $m \times n$ matrix, then:

1) Let $\mathbf{B}$ be an $n \times p$ matrix and $\mathbf{C}$ a $p \times q$ matrix. Then $\mathbf{A}(\mathbf{B}\mathbf{C}) = (\mathbf{A}\mathbf{B})\mathbf{C}$ is an $m \times q$ matrix.

2) Let $\mathbf{B}$ and $\mathbf{C}$ be $n \times p$ matrices. Then $\mathbf{A}(\mathbf{B} + \mathbf{C}) = \mathbf{A}\mathbf{B} + \mathbf{A}\mathbf{C}$ is an $m \times p$ matrix.

3) Let $\mathbf{B}$ and $\mathbf{C}$ be $p \times m$ matrices. Then $(\mathbf{B} + \mathbf{C})\mathbf{A} = \mathbf{B}\mathbf{A} + \mathbf{C}\mathbf{A}$ is an $p \times m$ matrix.

4) Let $\mathbf{B}$ be an $p \times m$ matrix and $c$ a scalar. Then $c(\mathbf{A} \mathbf{B}) = (c \mathbf{A}) \mathbf{B} = \mathbf{A}(c\mathbf{B})$ is an $p \times m$ matrix.

5) $\mathbf{I}_m \mathbf{A} = \mathbf{A} \mathbf{I}_n = \mathbf{A}$


**Examples: in class**

**Note:** Matrix multiplication violates some of the rules of multiplication that you might be used to. Pay attention for the following:

1) In general $\mathbf{A} \mathbf{B} \neq \mathbf{B} \mathbf{A}$ (sometimes these are equal, but usually are not)

2) $\mathbf{A}\mathbf{B} = \mathbf{A} \mathbf{C}$ **does not** imply $\mathbf{B} = \mathbf{C}$

3) $\mathbf{A}\mathbf{B} = \mathbf{0}$ does not imply that $\mathbf{A} = \mathbf{0}$ or $\mathbf{B} = \mathbf{0}$



### Matrix Multiplication complexity (Big O notation)

In the study of algorithms, the notation $O(n)$ is used to describe the number of calculations that need to be done to evaluate the equation. As an example, consider $\mathbf{A} = \begin{pmatrix}3 & 1 \\ 2 & -3 \end{pmatrix}$, $\mathbf{B} = \begin{pmatrix} -2 & 4 \\ -1 & 2 \end{pmatrix}$, and $\mathbf{x} = \begin{pmatrix} -3 \\ 1 \end{pmatrix}$. 

**By hand:** Calculate 

1) $(\mathbf{A} \mathbf{B}) \mathbf{x}$

2) $\mathbf{A} (\mathbf{B} \mathbf{x})$

Which was easier? Which required less calculation?

* Matrix-matrix multiplication of and $m \times n$ matrix $\mathbf{A}$ and an $m \times p$ matrix $\mathbf{B}$ has complexity $O(m n p)$.

* Matrix-vector multiplication of and $m \times n$ matrix $\mathbf{A}$ and an $p$-vector $\mathbf{x}$ has complexity $O(n m)$.

From example above:

1) $O(m n p)$ matrix-matrix multiplication $(\mathbf{A} \mathbf{B})$ followed by $O(m n)$ matrix-vector multiplication $(\mathbf{A} \mathbf{B}) \mathbf{x}$ which has computational complexity $O(m n p) + O(m n)$

2) $O(m n)$ matrix-vector multiplication $(\mathbf{B} \mathbf{x})$ followed by $O(m n)$ matrix-vector multiplication $\mathbf{A} (\mathbf{B} \mathbf{x})$ which has computational complexity $O(m n) + O(m n)$


### Matrix powers

Powers of a $n \times n$ (square) matrix are defined as the product of $\mathbf{A}$ multiplied $k$ times
$$
\mathbf{A}^k = \underbrace{\mathbf{A} \cdots \mathbf{A}}_k
$$

### Matrix Transpose


The matrix transpose is an operator that swaps the rows and columns of a matrix. If $\mathbf{A}$ is an $m \times n$ matrix, then $\mathbf{A}'$ is a $\m \times n$ matrix (Note: some use $\mathbf{A}^T$ to denote a transpose; I prefer the $'$ notation as it is much simpler and cleaner notation). The matrix 

$$
\begin{align*}
\mathbf{A} & = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1p} \\
a_{21} & a_{22} & \cdots & a_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{np}
\end{pmatrix}
\end{align*}
$$
has transpose

$$
\begin{align*}
\mathbf{A}' & = \begin{pmatrix} a_{11} & a_{21} & \cdots & a_{p1} \\
a_{12} & a_{22} & \cdots & a_{p2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \cdots & a_{pn}
\end{pmatrix},
\end{align*}
$$

```{theorem}
Let $\mathbf{A}$ be an $m \times n$ matrix, then

1) $(\mathbf{A}')' = \mathbf{A}$.

2) Let $\mathbf{B}$ be an $m \times n$ matrix, then $(\mathbf{A} + \mathbf{B})' = \mathbf{A}' + \mathbf{B}'$.

3) For any scalar $c$, $(c \mathbf{A})' = c \mathbf{A}'$.

4) Let $\mathbf{B}$ be an $n \times p$ matrix, then $( \mathbf{A} \mathbf{B})' = \mathbf{B}' \mathbf{A}'$ is an $p \times m$ matrix.
```

**Note:** The power of video games: GPUs and modern CPUs are becoming more and more parallelized. Because the $ij$th element of $\mathbf{A}\mathbf{B}$ requires only the $i$th row of $\mathbf{A}$ and the $j$th column of $\mathbf{B}$, matrix multiplication is easily parallelized under modern computing architectures. Thanks to video games, this parallelization has been made faster than ever.


**Examples: in class**








<!-- ## Direct Sums -->

<!-- **Direct Sums:** For the $n \times p$ matrix $\mathbf{A}$ and the $m \times q$ matrix $\mathbf{B}$, the direct sum is -->

<!-- $$ -->
<!-- \begin{align*} -->
<!-- \mathbf{C} & = \mathbf{A} \bigoplus \mathbf{B} \\ -->
<!-- & = \begin{pmatrix} \mathbf{A} & \mathbf{0}_{n \times q} \\ -->
<!-- \mathbf{0}_{m \times p} & \mathbf{B} -->
<!-- \end{pmatrix} \\ -->
<!-- \end{align*} -->
<!-- $$ -->

<!-- where $\mathbf{0}_{n \times q}$ and $\mathbf{0}_{m \times p}$ is an are $n \times q$ and $m \times p$ matrices of all 0s, respectively.  -->

<!-- The direct sum can be generalized to any arbitrary number of matrices as  -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- \mathbf{C} & = \bigoplus_{i=1}^K \mathbf{A}_i \\ -->
<!-- & = \begin{pmatrix} \mathbf{A}_1 & \mathbf{0} & \cdots & \mathbf{0} \\ -->
<!-- \mathbf{0} & \mathbf{A}_2 &  \cdots & \mathbf{0} \\ -->
<!-- \vdots & \vdots & \ddots & \vdots \\ -->
<!-- \mathbf{0} & \mathbf{0} & \cdots & \mathbf{A}_K \\ -->
<!-- \end{pmatrix} \\ -->
<!-- \end{align*} -->
<!-- $$ -->

<!-- where the matrices $\mathbf{0}$ are all zeros of the appropriate dimension. -->










