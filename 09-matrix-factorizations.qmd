# Matrix Factorizations {#matrix-factorizations}

```{r setup-09, message = FALSE}
library(tidyverse)
library(dasc2594)
library(mvnfast)
library(MASS)
```


In scalar mathematics, a factorization is an expression that writes a scalar $a$ as a product of two or more scalars. For example, the scalar 2 has a square-root factorization of $2  =\sqrt{2} * \sqrt{2}$ and 15 has a prime factorization of $15 = 3 * 5$. A matrix factorization is a similar concept where a matrix $\mathbf{A}$ can be represented by a product or two or more matrices (e.g., $\mathbf{A} = \mathbf{B} \mathbf{C}$). In data science, matrix factorizations are fundamental to working with data. 

## The LU factorization

First, we define lower and upper triangular matrices. 

::: {#def-}
The matrix $\mathbf{A}$ is said to be lower triangular if
$$
\begin{aligned}
\mathbf{A} = \begin{pmatrix} 
a_{11} & 0 & 0 & \cdots & 0 \\
a_{21} & a_{22} & 0 & \cdots & 0 \\
a_{31} & a_{32} & a_{33} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn} \\
\end{pmatrix}
\end{aligned}
$$    
Similarly, the matrix $\mathbf{A}$ is said to be upper triangular if
$$
\begin{aligned}
\mathbf{A} = \begin{pmatrix} 
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn} \\
\end{pmatrix}
\end{aligned}
$$    
:::



The LU factorization of a matrix $\mathbf{A}$ reduces the matrix $\mathbf{A}$ into two components. The first component $\mathbf{L}$ is a lower-triangular matrix and the second component $\mathbf{U}$ is an upper triangular matrix.

Using the LU factorization, the matrix factorization $\mathbf{A} = \mathbf{L} \mathbf{U}$ can be used in the matrix equation $\mathbf{A} \mathbf{x} = \mathbf{L} \mathbf{U}\mathbf{x} = \mathbf{b}$ by first solving the sub-equation $\mathbf{L} \mathbf{y} = \mathbf{b}$ and then solving the second sub-equation $\mathbf{U} \mathbf{x} = \mathbf{y}$ for $\mathbf{x}$. Thus, the matrix factorization applied to the matrix equation gives the pair of equations

$$
\begin{aligned}
\mathbf{L} \mathbf{y} & = \mathbf{b} \\
\mathbf{U} \mathbf{x} & = \mathbf{y}
\end{aligned}
$${#eq-LU}

At first glance, this seems like we are trading the challenge of solving one system of equations $\mathbf{A}\mathbf{x}$ @eq-matrixequation for the two equations in @eq-LU. However, the computational benefits arise due to the fact that $\mathbf{L}$ and $\mathbf{U}$ are triangular matrices and solving matrix equations with triangular matrices is much faster. 

```{r example-LU, echo = FALSE, include = FALSE}
# create a LU matrix
set.seed(11)
L <- matrix(sample(-3:3, 16, replace = TRUE), 4, 4)
L[upper.tri(L)] <- 0
U <- matrix(sample(-3:3, 16, replace = TRUE), 4, 4)
U[lower.tri(U)] <- 0
A <- L %*% U
while(!isTRUE(all.equal(rref(A), diag(4)))) {
    L <- matrix(sample(-3:3, 16, replace = TRUE), 4, 4)
    L[upper.tri(L)] <- 0
    U <- matrix(sample(-3:3, 16, replace = TRUE), 4, 4)
    U[lower.tri(U)] <- 0
    A <- L %*% U
}

x <- sample(-4:4, 4)
b <- A %*% x
```


::: {#exm-}
Let $\mathbf{A} = `r array_to_latex(A)`$ which has the LU decomposition

$$
\begin{aligned}
\mathbf{A} = `r array_to_latex(A)` = `r array_to_latex(L)` `r array_to_latex(U)`
\end{aligned}
$$
and consider the system of equations defined by the matrix equation $\mathbf{A} \mathbf{x} = \mathbf{b}$ where $\mathbf{b} = `r array_to_latex(as.matrix(b))`$. 

1) solve $\mathbf{L} \mathbf{y} = \mathbf{b}$ using an augmented matrix and RREF. 

2) solve $\mathbf{U} \mathbf{x} = \mathbf{y}$ using an augmented matrix and RREF.

3) compare to the solution $\mathbf{A}\mathbf{x} = \mathbf{b}$ using an augmented matrix and RREF.
:::


::: {.callout-note icon=false collapse="true" appearance="simple"}
## Solution

```{r exampleLU-solution, echo = FALSE}
L1 <- cbind(L, b)
L2 <- L1
L2[2, ] <- -1/2 * L2[2, ] - L2[1, ]
L3 <- L2
L3[3, ] <- - L3[3, ] - L3[1, ]
L4 <- L3
L4[4, ] <- -1/2 * L4[4, ] - L4[1, ]
L5 <- L4
L5[2, ] <- 2 * L5[2, ]
L6 <- L5
L6[3, ] <- 1 / 2 * L6[3, ] - L6[2, ]
L7 <- L6
L7[4, ] <- 2/3 * L7[4, ] - L7[2, ]
L8 <- L7
L8[3, ] <- 2/3 * L8[3, ]
y <- L8[, 5]
```

For the example, we will show how to solve a system of equations using the LU decomposition for the equation defined above.
 
1) Solve $\mathbf{L} \mathbf{y} = \mathbf{b}$ using augmented matrix


$$
\begin{aligned}
& & `r array_to_latex(L1)` & \stackrel{r_2 \leftarrow -\frac{1}{2} r_2 - r_1}{\sim} & `r array_to_latex(L2)` \\
& \stackrel{r_3 \leftarrow - r_3 - r_1}{\sim} & `r array_to_latex(L3)` & \stackrel{r_4 \leftarrow - \frac{1}{2} r_4 - r_1}{\sim} & `r array_to_latex(L4)` \\
& \stackrel{r_2 \leftarrow 2 r_2}{\sim} & `r array_to_latex(L5)` & \stackrel{r_3 \leftarrow \frac{1}{2} r_3 - r_2}{\sim} & `r array_to_latex(L6)` \\
& \stackrel{r_4 \leftarrow \frac{2}{3} r_4 - r_2}{\sim} & `r array_to_latex(L7)` & \stackrel{r_3 \leftarrow \frac{2}{3} r_3}{\sim} & `r array_to_latex(L8)`
\end{aligned}
$$

```{r example-LU-solution2, echo = FALSE}
U1 <- cbind(U, y)
U2 <- U1
U2[2, ] <- 1/2 * U2[2, ]
U3 <- U2
U3[3, ] <- 1/2 * U3[3, ]
U4 <- U3
U4[4, ] <- -1/3 * U4[4, ]
U5 <- U4
U5[1, ] <- U5[1, ] - 2 * U5[3, ]
U6 <- U5
U6[1, ] <- U6[1, ] - U6[4, ]
U7 <- U6
U7[2, ] <- U7[2, ] - 3/2 * U7[4, ]
U8 <- U7
U8[3, ] <- U8[3, ] + 3/2 * U8[4, ]
x <- U8[, 5]
```

2) solve $\mathbf{U} \mathbf{x} = \mathbf{y}$ using an augmented matrix and RREF.


$$
\begin{aligned}
& & `r array_to_latex(U1)` & \stackrel{r_2 \leftarrow \frac{1}{2} r_2}{\sim} & `r array_to_latex(U2)` \\
& \stackrel{r_3 \leftarrow \frac{1}{2} r_3}{\sim} & `r array_to_latex(U3)` & \stackrel{r_4 \leftarrow - \frac{1}{3} r_4}{\sim} & `r array_to_latex(U4)` \\
& \stackrel{r_1 \leftarrow r_1 -2  r_3}{\sim} & `r array_to_latex(U5)` & \stackrel{r_1 \leftarrow r_1 - 2 r_4}{\sim} & `r array_to_latex(U6)` \\
& \stackrel{r_2 \leftarrow r_2 - \frac{3}{2} r_4}{\sim} & `r array_to_latex(U7)` & 
\stackrel{r_3 \leftarrow r_3 + \frac{3}{2} r_4}{\sim} & `r array_to_latex(U8)` 
\end{aligned}
$$

3) compare to the solution $\mathbf{A}\mathbf{x} = \mathbf{b}$ using an augmented matrix and RREF.


```{r example-LU-solution-3, echo = FALSE}
A1 <- cbind(A, b)
A2 <- A1
A2[2, ] <- A2[2, ] + 2 * A2[1, ]
A3 <- A2
A3[3, ] <- A3[3, ] + A3[1, ]
A4 <- A3
A4[4, ] <- A4[4, ] + 2 * A4[1, ]
A5 <- A4
A5[2, ] <- -1/2 * A5[2, ]
A6 <- A5
A6[3, ] <- A6[3, ] + 4 * A6[2, ]
A7 <- A6
A7[4, ] <- A7[4, ] + 6 * A7[2, ]
A8 <- A7
A8[3, ] <- -1/6 * A8[3, ]
A9 <- A8
A9[4, ] <- 1/9 * A9[4, ]
A10 <- A9
A10[1, ] <- A10[1, ] - 2 * A10[3, ]
A11 <- A10
A11[1, ] <- A11[1, ] - A11[4, ]
A12 <- A11
A12[2, ] <- A12[2, ] - 3/2 * A12[4, ]
A13 <- A12
A13[3, ] <- A13[3, ] + 3/2 * A13[4,]
```
$$
\begin{aligned}
& & `r array_to_latex(A1)` & \stackrel{r_2 \leftarrow r_2 + 2 r_1}{\sim} & `r array_to_latex(A2)` \\
& \stackrel{r_3 \leftarrow r_3 + r_1}{\sim} & `r array_to_latex(A3)` & \stackrel{r_4 \leftarrow r_4 + 2 r_1}{\sim} & `r array_to_latex(A4)` \\
& \stackrel{r_2 \leftarrow - \frac{1}{2} r_2}{\sim} & `r array_to_latex(A5)` & \stackrel{r_3 \leftarrow r_3 + 4 r_2}{\sim} & `r array_to_latex(A6)` \\
& \stackrel{r_4 \leftarrow r_4 + 6 r_2}{\sim} & `r array_to_latex(A7)` & 
\stackrel{r_3 \leftarrow - \frac{1}{6} r_3}{\sim} & `r array_to_latex(A8)`\\
& \stackrel{r_4 \leftarrow \frac{1}{9} r_4}{\sim} & `r array_to_latex(A9)` & \stackrel{r_1 \leftarrow r_1 - 2 r_3}{\sim} & `r array_to_latex(A10)` \\
& \stackrel{r_1 \leftarrow r_1 - r_4}{\sim} & `r array_to_latex(A11)` & 
\stackrel{r_2 \leftarrow r_2 - \frac{3}{2} r_4}{\sim} & `r array_to_latex(A12)` \\
& \stackrel{r_3 \leftarrow r_3 + \frac{3}{2} r_4}{\sim} & `r array_to_latex(A13)`  
\end{aligned}
$$

While it might not be completely obvious, once one has calculated the LU decomposition, it can often be much faster to solve systems of equations with the LU decomposition.
:::


:::{.#exm-}
in lab:
Solve some large systems of equations by brute force which shows how the LU decomposition is faster. 
:::    

### Geometric interpretation of the LU factorization

* **Draw image in class -- composition of transformations $T_A(\cdot) = T_L(T_U(\cdot))$**



## Obtaining the LU factorization

Notice that the upper-triangular matrix $\mathbf{U}$ is in echelon form. Congratulations! you know how to construct a matrix $\mathbf{U}$ by reducing the matrix $\mathbf{A}$ to an echelon form $\mathbf{U}$ using elementary matrices $\mathbf{E}_1, \ldots \mathbf{E}_k$. Now, we only need to find the lower triangular matrix $\mathbf{L}$. 

Combining the LU factorization and the fact that we can find an upper triangular matrix $\mathbf{U}$ using elementary row matrices, we have

$$
\begin{aligned}
\mathbf{A} & = \mathbf{L} \mathbf{U} \\
\mathbf{E}_k \cdots \mathbf{E}_1 \mathbf{A} & = \mathbf{U}.
\end{aligned}
$${#eq-LUalg}
We also know that each of the elementary row matrices $\mathbf{E}_j$ are invertible (you can always re-swap rows, subtract instead of add rows, etc.) which says that each inverse $\mathbf{E}_j^{-1}$ exists. Thus, the product $\mathbf{E}_k \cdots \mathbf{E}_1$ must have an inverse which is 
$$
\begin{aligned}
(\mathbf{E}_k \cdots \mathbf{E}_1)^{-1} & = \mathbf{E}_1^{-1} \cdots \mathbf{E}_k^{-1}.
\end{aligned}
$$
Plugging this inverse into @eq-LUalg gives (left multiplying by $(\mathbf{E}_k \cdots \mathbf{E}_1)^{-1}$ on both sides)
$$
\begin{aligned}
(\mathbf{E}_k \cdots \mathbf{E}_1)^{-1} (\mathbf{E}_k \cdots \mathbf{E}_1) \mathbf{A} & = (\mathbf{E}_k \cdots \mathbf{E}_1)^{-1}\mathbf{U} \\
 \mathbf{A} & = (\mathbf{E}_k \cdots \mathbf{E}_1)^{-1}\mathbf{U} \\
 & = \mathbf{L} \mathbf{U}
\end{aligned}
$$
where $\mathbf{L} = (\mathbf{E}_k \cdots \mathbf{E}_1)^{-1}$


**Algorithm for finding the LU decomposition**

Given the matrix $\mathbf{A}$

1) Find elementary matrices $\mathbf{E}_1, \ldots, \mathbf{E}_k$ such that $\mathbf{E}_k \cdots \mathbf{E}_1 \mathbf{A}$ is in row echelon form (if this is possible, otherwise an LU factorization does not exist). Call this matrix $\mathbf{U}$, the upper triangular component of the LU factorization.

2) The, the lower triangular $\mathbf{L} = (\mathbf{E}_k \cdots \mathbf{E}_1)^{-1}$.


Notice that the algorithm does not say to find a specific matrix $\mathbf{U}$. In general, any row echelon form matrix $\mathbf{U}$ will work.

## The Cholesky factor

A Cholesky decomposition is special type of LU decomposition. A Cholesky decomposition is an LU decomposition on a **symmetric, positive-definite** square matrix. 

::: {#def-}

* A matrix $\mathbf{A}$ is said to by symmetric if $\mathbf{A} = \mathbf{A}'$

* A $n \times n$ matrix is said to be positive definite if for all $\mathbf{x} \in \mathcal{R}^n$, the quadratic form $\mathbf{x}' \mathbf{A }\mathbf{x} \geq 0$        

:::

Note: the condition of positive definiteness is actually impossible to check. Can you show this is true for all vectors? Luckily, a $n \times n$ symmetric matrix is positive definite if and only if the matrix $\mathbf{A}$ is invertible (which we know about by the invertible matrix theorem @thm-invertiblematrix).

::: {#def-}
Let $\mathbf{A}$ be a symmetric, positive definite matrix (by this, $\mathbf{A}$ is a $n \times n$ square matrix). Then
$$
\begin{aligned}
\mathbf{A} = \mathbf{L} \mathbf{L}'
\end{aligned}
$$
is the Cholesky decomposition of $\mathbf{A}$ if $\mathbf{L}$ is a lower-triangular matrix. Also, the lower triangular Cholesky matrix $\mathbf{L}$ is unique. 
:::

What makes the Cholesky factor special? 

* The decomposition $\mathbf{A} = \mathbf{L} \mathbf{U}$ has the property that $\mathbf{U} = \mathbf{L}'$ so that the computer only has to store one of the matrix components (reduce memory demands). As about half of the elements of $\mathbf{L}$ are 0, matrix multiplication is much less computationally demanding as about half of the flops are not required to be evaluated (x * 0 =  0).

* The Cholesky factor is unique. There is only one Cholesky factor for each symmetric positive definite matrix. 

* The Cholesky has properties related to multivariate normal distributions. 

Let $\mathbf{y} \sim \operatorname{N}(\mathbf{0}, \boldsymbol{\Sigma})$, and $\boldsymbol{\Sigma} = \mathbf{L} \mathbf{L}'$. Then, if $\mathbf{z} \sim \operatorname{N}(\mathbf{0}, \mathbf{I})$, then $\mathbf{L} \mathbf{z} \sim \operatorname{N}(\mathbf{0}, \boldsymbol{\Sigma})$. We say the $\mathbf{y}$ and $\mathbf{L}\mathbf{z}$ are equal in distribution.

```{r example-cholesky, message = FALSE}
# simulate N 2-dimensional random normal vectors 
N <- 5000
mu    <- rep(0, 2)
Sigma <- matrix(c(2, 1.5, 1.5, 2), 2, 2)
y <- rmvn(N, mu, Sigma)

# calculate the Cholesky factor
L <- t(chol(Sigma))    # R calculates the upper (right) Cholesky factor by default
z <- rmvn(N, mu, diag(2))
Lz <- t(L %*% t(z))    # pay attention to the dimensions of L and z here...

data.frame(
    observation = 1:N,
    x1          = c(y[, 1], z[, 1], Lz[, 1]),
    x2          = c(y[, 2], z[, 2], Lz[, 2]),
    variable    = factor(rep(c("y", "z", "Lz"), each = N), levels = c("y", "z", "Lz"))
) %>%
    ggplot(aes(x = x1, y = x2, color = variable)) +
    geom_point(alpha = 0.1) +
    geom_density2d() +
    facet_wrap(~ variable)
```

