{
  "hash": "8165b4fb5d7683454af48121db85cb88",
  "result": {
    "markdown": "# Eigenvectors and Eigenvalues\n\n- [3 Blue 1 Brown -- Eigenvalues](https://www.3blue1brown.com/lessons/eigenvalues)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(dasc2594)\nset.seed(2021)\n```\n:::\n\n\nWe have just learned about change of basis in an abstract sense. Now, we will learn about a special change of basis that is \"data-driven\" called an eigenvector. Eigenvectors and the corresponding eigenvalues are a vital tool in data science for data compression and modeling.\n\n::: {#def-}\nAn **eigenvector** of an $n \\times n$ matrix $\\mathbf{A}$ is a nonzero vector $\\mathbf{x}$ such that the matrix equation\n\n$$\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}\n\\end{aligned}\n$$\n\nfor some scalar $\\lambda$. If there exists some $\\lambda \\neq 0$ (a non-trivial solutions), then $\\lambda$ is called an **eigenvalue** of $\\mathbf{A}$ corresponding to the eigenvector $\\mathbf{x}$.\n:::\n\n\n::: {.example}\nIt is easy to check if a vector is an eigenvalue:\n\n::: {.cell}\n\n:::\n\n\n\nLet $\\mathbf{A} = \\begin{pmatrix} 0 & 6 & 8 \\\\ 1/2 & 0 & 0 \\\\ 0 & 1/2 & 0 \\end{pmatrix}$, $\\mathbf{u} = \\begin{pmatrix} 16 \\\\ 4 \\\\ 1 \\end{pmatrix}$, and $\\mathbf{v} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$. Determine if $\\mathbf{u}$ or $\\mathbf{v}$ are eigenvectors of $\\mathbf{A}$. If they are eigenvectors, what are the associated eigenvalues.\n:::\n\n\n::: {.callout-note icon=false collapse=\"true\" appearance=\"simple\"} \n## Solution\n\nHere we demonstrate the eigenvector/eigenvalue relationship. \n\na) If $\\mathbf{u}$ is an eigenvector of a matrix $\\mathbf{A}$, then there exists some constant $\\lambda$ such that $\\mathbf{A} \\mathbf{u} = \\lambda \\mathbf{u}$. Checking this gives\n\n$$\n\\begin{aligned}\n\\mathbf{A} \\mathbf{u} & = \\begin{pmatrix} 0 & 6 & 8 \\\\ 1/2 & 0 & 0 \\\\ 0 & 1/2 & 0 \\end{pmatrix} \\begin{pmatrix} 16 \\\\ 4 \\\\ 1 \\end{pmatrix} \\\\\n& = \\begin{pmatrix} 32 \\\\ 8 \\\\ 2 \\end{pmatrix} \\\\ \n& = 2 \\begin{pmatrix} 16 \\\\ 4 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich shows that $\\mathbf{u}$ is an eigenvector of $\\mathbf{A}$ with associated eigenvalue $\\lambda = 2$. Now, we check if $\\mathbf{v}$ is an eigenvector of $\\mathbf{A}$\n\n$$\n\\begin{aligned}\n\\mathbf{A} \\mathbf{v} & = \\begin{pmatrix} 0 & 6 & 8 \\\\ 1/2 & 0 & 0 \\\\ 0 & 1/2 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix} \\\\\n& = \\begin{pmatrix} 28 \\\\ 1 \\\\ 1 \\end{pmatrix} \n\\end{aligned}\n$$\n\nwhere there is no number $\\lambda$ such that $\\begin{pmatrix} 28 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\lambda \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$. In `R`, this can be shown\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(0, 1/2, 0, 6, 0, 1/2, 8, 0, 0), 3, 3)\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]  0.0  6.0    8\n[2,]  0.5  0.0    0\n[3,]  0.0  0.5    0\n```\n:::\n\n```{.r .cell-code}\nu <- c(16, 4, 1)\nv <- c(2, 2, 2)      \n# is u an eigenvector of A?\nA %*% u\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]   32\n[2,]    8\n[3,]    2\n```\n:::\n\n```{.r .cell-code}\n# yes, because A %*% u = 2 u\n# 2 is the eigenvalue associated with u\n\n# is v an eigenvector of A?\nA %*% v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]   28\n[2,]    1\n[3,]    1\n```\n:::\n\n```{.r .cell-code}\n# not an eigenvectos because A %*% v is not equal to lambda * v for some lambda\n```\n:::\n\n:::\n\n\n\n\n::: {#exm-}\nIt is easy to check if a vector is an eigenvalue:\n\n::: {.cell}\n\n:::\n\n\n\n Let $\\mathbf{A} = \\begin{pmatrix} 2 & 1 \\\\ 0 & 1 \\end{pmatrix}$, $\\mathbf{u} = \\begin{pmatrix} - \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{pmatrix}$, and $\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Determine if $\\mathbf{u}$ or $\\mathbf{v}$ are eigenvectors of $\\mathbf{A}$. If they are eigenvectors, what are the associated eigenvalues. Now, plot $\\mathbf{u}$, $\\mathbf{A} \\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{A} \\mathbf{v}$ to show this relationship geometrically.\n:::\n\n\n\n::: {.callout-note icon=false collapse=\"true\" appearance=\"simple\"} \n## Solution\n\nFirst, we determine if the vectors $\\mathbf{u}$ and $\\mathbf{v}$ are eigenvectors of $\\mathbf{A}$. \n\nIf $\\mathbf{u}$ is an eigenvector of a matrix $\\mathbf{A}$, then there exists some constant $\\lambda$ such that $\\mathbf{A} \\mathbf{u} = \\lambda \\mathbf{u}$. Checking this gives\n\n$$\n\\begin{aligned}\n\\mathbf{A} \\mathbf{u} & = \\begin{pmatrix} 2 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} - \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{pmatrix} \\\\\n& = \\begin{pmatrix} - \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{pmatrix} \n\\end{aligned}\n$$\n\nwhich shows that $\\mathbf{u}$ is an eigenvector of $\\mathbf{A}$ with associated eigenvalue $\\lambda = 1$. Now, we check if $\\mathbf{v}$ is an eigenvector of $\\mathbf{A}$\n\n$$\n\\begin{aligned}\n\\mathbf{A} \\mathbf{v} & = \\begin{pmatrix} 2 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\\\\n& = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} \n\\end{aligned}\n$$\n\nwhere there is no number $\\lambda$ such that $\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. In `R`, this can be shown\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(2, 0, 1, 1), 2, 2)\nu <- c(-sqrt(2)/2, sqrt(2) / 2)\nv <- c(1, 1)\n\n# is u an eigenvector of A?\nA %*% u\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] -0.7071068\n[2,]  0.7071068\n```\n:::\n\n```{.r .cell-code}\n# yes, because A %*% u = u\n# 1 is the eigenvalue associated with u\n\n# is v an eigenvector of A?\nA %*% v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    3\n[2,]    1\n```\n:::\n\n```{.r .cell-code}\n# not an eigenvectos because A %*% v is not equal to lambda * v for some lambda\n```\n:::\n\n\n\nNow, we will plot the vectors $\\mathbf{u}$ and $\\mathbf{v}$ as well as the vectors transformed by the matrix $\\mathbf{A}$ (i.e., $\\mathbf{A} \\mathbf{u}$ and $\\mathbf{A} \\mathbf{v}$). The code below plot the vector $\\mathbf{u}$ in dark blue and the transformed vector $\\mathbf{A} \\mathbf{u}$ in light blue. The code also plots the vector $\\mathbf{v}$ in dark red and the transformed vector $\\mathbf{A} \\mathbf{v}$ in light red.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n    geom_segment(aes(x = 0, xend = u[1], y = 0, yend = u[2]), color = \"dark blue\") +\n    geom_segment(aes(x = 0, xend = (A %*% u)[1], y = 0, yend = (A %*% u)[2]), color = \"light blue\", lty = 2) +\n    geom_segment(aes(x = 0, xend = v[1], y = 0, yend = v[2]), color = \"dark red\") +\n    geom_segment(aes(x = 0, xend = (A %*% v)[1], y = 0, yend = (A %*% v)[2]), color = \"red\") +\n    coord_cartesian(xlim = c(-5, 5), ylim = c(-5, 5))\n```\n\n::: {.cell-output-display}\n![](19-eigenvectors-and-eigenvalues_files/figure-html/example-eigenvalue-5-1.png){width=672}\n:::\n:::\n\nNotice that the multiplication of $\\mathbf{u}$ by $\\mathbf{A}$ gives a vector $\\mathbf{A} \\mathbf{u}$ that points along the same line as $\\mathbf{u}$ because $\\mathbf{u}$ is an eigenvector of $\\mathbf{A}$. In comparison, the vector $\\mathbf{v}$ is not an eigenvector of $\\mathbf{A}$ and multiplication of $\\mathbf{v}$ by $\\mathbf{A}$ gives a vector $\\mathbf{A} \\mathbf{v}$ that *does not* point along the same line as the vector $\\mathbf{u}$. \n:::\n\n\n\n::: {#exm-}\nCome up with another example and another plot that shows the similar result as the example above.\n:::\n\n\n\n::: {.callout-note icon=false collapse=\"true\" appearance=\"simple\"} \n## Solution\nThe solution (TBD)\n:::\n\n\nThus, we end up with the understanding that nn eigenvector is a (nonzero) vector $\\mathbf{x}$ that gets mapped to a scalar multiple of itself $\\lambda \\mathbf{x}$ by the matrix transformation defined by $T: \\mathbf{x} \\rightarrow \\mathbf{A}\\mathbf{x} = \\mathbf{x}$. As such, when $\\mathbf{x}$ is an eigenvector of $\\mathbf{A}$ we say that $\\mathbf{x}$ and $\\mathbf{A} \\mathbf{x}$ are collinear with the origin ($\\mathbf{0}$) and each other in the sense that these points lie on the same line that goes through the origin. \n\n**Note:** The matrix $\\mathbf{A}$ must be an $n \\times n$ square matrix. A similar decomposition (called the singular value decomposition) can be used for rectangular matrices.\n\n::: {#exm-}\nExample: reflection\nDraw images: [https://textbooks.math.gatech.edu/ila/eigenvectors.html](https://textbooks.math.gatech.edu/ila/eigenvectors.html)\n:::\n\n\n:::{#thm-distincteigenvalues}\n## The Distinct Eigenvalues Theorem\n\nLet $\\mathbf{v}_1, \\ldots, \\mathbf{v}_n$ be eigenvectors of a matrix $\\mathbf{A}$ and suppose the corresponding eigenvalues are $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$ are all distinct (different values). Then, the set of vectors $\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}$ is linearly independent.\n:::\n\n\n::: {.callout-tip icon=false collapse=\"true\" appearance=\"simple\"} \n## Proof\n\nSuppose the set $\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}$ is linearly dependent. Then, there is some $j$ such that $\\mathbf{v}_j = \\sum_{k = 1}^{j-1} x_k \\mathbf{v}_k$. If we choose the first linearly dependent vector as $j$, we know that the subset of vectors $\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_{j-1}\\}$ is linearly independent and \n\n$$\n\\begin{aligned}\n\\mathbf{v}_j & = x_1 \\mathbf{v}_1 + \\cdots x_{j-1} + \\mathbf{v}_{j-1}\n\\end{aligned}\n$$\n\nfor some scalars $x_1, \\ldots, x_{j-1}$. Multiplying the equation above on the left by $\\mathbf{A}$ on both sides gives\n\n$$\n\\begin{aligned}\n\\mathbf{A}\\mathbf{v}_j & = \\mathbf{A} (x_1 \\mathbf{v}_1 + \\cdots + x_{j-1} \\mathbf{v}_{j-1}) \\\\\n\\lambda_j \\mathbf{v}_j & = x_1 \\mathbf{A} \\mathbf{v}_1 + \\cdots + x_{j-1} \\mathbf{A} \\mathbf{v}_{j-1} \\\\\n& =  x_1 \\lambda_1 \\mathbf{v}_1 + \\cdots x_{j-1} \\lambda_{j-1} + \\mathbf{v}_{j-1} \\\\\n\\end{aligned}\n$$\n\nMultiplying the first equation by $\\lambda_j$ and subtracting this from the second equation gives\n\n$$\n\\begin{aligned}\n\\mathbf{0} = \\lambda_j \\mathbf{v}_j - \\lambda_j \\mathbf{v}_j \n& = x_1 (\\lambda_1 - \\lambda_j) \\mathbf{v}_1 + \\cdots x_{j-1} + (\\lambda_{j-1} - \\lambda_j) \\mathbf{v}_{j-1} \\\\\n\\end{aligned}\n$$\n\nBecause $\\lambda_k \\neq \\lambda_j$ for all $k < j$, the equation above implies a linear dependence among the set of vectors $\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_{j-1}\\}$ which is a contradiction. Therefore, our assumption that there exists a linearly dependent vector $\\mathbf{v}_j$ is violated and all the $\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}$ are linearly independent.\n:::\n\n\n## Eigenspaces\n\nGiven a square $n \\times n$ matrix $\\mathbf{A}$, we know how to check if a given vector $\\mathbf{x}$ is an eigenvector and then how to find the eigenvalue associated with that eigenvector. Next, we want to check if a given number is an eigenvalue of $\\mathbf{A}$ and to find all the eigenvectors corresponding to that eigenvalue. \n\nGiven a square $n \\times n$ matrix $\\mathbf{A}$ and a scalar $\\lambda$, the eigenvectors of $\\mathbf{A}$ associated with the scalar $\\lambda$ (if there are eigenvectors associated with $\\lambda$) are the nonzero solutoins to the equation $\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}$. This can be written as\n\n$$\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x} & = \\lambda \\mathbf{x} \\\\\n\\mathbf{A} \\mathbf{x} -\\lambda \\mathbf{x} & = \\mathbf{0} \\\\\n\\mathbf{A} \\mathbf{x} -\\lambda \\mathbf{I} \\mathbf{x} & = \\mathbf{0} \\\\\n\\left( \\mathbf{A} -\\lambda \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}. \\\\\n\\end{aligned}\n$$\n\nTherefore, the eigenvectors of $\\mathbf{A}$ associated with $\\lambda$, if there are any, are the nontrivial solutions of the homogeneous matrix equation $\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}$. In other words, the eigenvectors are the nonzero vectors in the null space null$\\left( \\mathbf{A} -\\lambda \\mathbf{I} \\right)$. If there is not a nontrivial solution (solution $\\mathbf{x} \\neq \\mathbf{0}$), then $\\lambda$ is not an eigenvalue of $\\mathbf{A}$.\n\nHey, we know how to find solutions to homogeneous systems of equations! Thus, we know how to find the eigenvectors of $\\mathbf{A}$. All we have to do is solve the system of linear equations $\\left( \\mathbf{A} -\\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}$ for a given $\\lambda$ (actually, for all $\\lambda$s, which we can't do). If only there was some way to find eigenvalues $\\lambda$ (hint: there is and it is coming next chapter).\n\n::: {#exm-}\n\n::: {.cell}\n\n:::\n\nLet $\\mathbf{A} = \\begin{pmatrix} 3 & 6 & -8 \\\\ 0 & 0 & 6 \\\\ 0 & 0 & 2 \\end{pmatrix}$. Then an eigenvector with eigenvector $\\lambda$ is a nontrival solution to \n\n$$\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich can be written as \n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n3  - \\lambda & 6 & -8 \\\\\n0 & 0 - \\lambda & 6 \\\\\n0 & 0 & 2 - \\lambda\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich can be solved for a given $\\lambda$ using an augmented matrix form and row operations to reduce to reduced row echelon form. \n\nLetting $\\lambda = 3$, we have\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n3  - 3 & 6 & -8 \\\\\n0 & 0 - 3 & 6 \\\\\n0 & 0 & 2 - 3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich can be written as the matrix equation\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 0 & 6 & -8 \\\\ 0 & -3 & 6 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nNote that the columns of the matrix above are not linearly independent. Thus, we can solve a non-unique solution (the solution set is a line going through the origin) by finding the reduce row echelon form of an augmented matrix\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 0 & 6 & -8 & 0 \\\\ 0 & -3 & 6 & 0 \\\\ 0 & 0 & -1 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich has solution \n\n$$\n\\begin{aligned}\nx_1 & = x_1 \\\\\nx_2 & = 0 \\\\\nx_3 & = 0\n\\end{aligned}\n$$\n\nFixing $x_1 = 1$ gives the eigenvector associated with $\\lambda = 3$ of $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. We can verify that this is an eigenvector with matrix multiplication\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 3 & 6 & -8 \\\\ 0 & 0 & 6 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} & = \\begin{pmatrix} 3 \\\\ 0 \\\\ 0 \\end{pmatrix}  = 3 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\n\nUsing `R`, this can be done as\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- 3\n# apply rref to the augmented matrix\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    0    1    0    0\n[2,]    0    0    1    0\n[3,]    0    0    0    0\n```\n:::\n:::\n\nwhere the solution set is determined from the RREF form of the augmented matrix of the equation $\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}$\n:::\n\n\n::: {#exm-}\n\n::: {.cell}\n\n:::\n\nLet $\\mathbf{A} = \\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix}$. Find the eigenvectors associated with the eigenvalues (a) $\\lambda_1 = -4$, (b) $\\lambda_2 = 3$, and (c) $\\lambda_3 = -1$.\n:::\n\n\n::: {.callout-note icon=false collapse=\"true\" appearance=\"simple\"} \n## Solution\n\nGiven the matrix $\\mathbf{A} = \\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix}$, we can find the eigenvectors associated with the given eigenvalues\n\na) The eigenvalues associated with the first eigenvector $\\lambda_1 = -4$ by solving\n\n$$\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda_1 \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich can be written as \n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n-21/5  - \\lambda_1 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - \\lambda_1 & 3/5 \\\\\n-4 & -10 & 5 - \\lambda_1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nand can be solved for $\\lambda_1$ using an augmented matrix form and row operations to reduce to reduced row echelon form. \n\nLetting $\\lambda_1 = -4$, we have\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n-21/5  - -4 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - -4 & 3/5 \\\\\n-4 & -10 & 5 - -4\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich results in the augmented matrix\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} -1/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & 6/5 & 3/5 & 0 \\\\ -4 & -10 & 9 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nReducing the augmented matrix to reduced row echelon form gives\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} -1/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & 6/5 & 3/5 & 0 \\\\ -4 & -10 & 9 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & -1 & 0 \\\\ 0 & 1 & -1/2 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich has solution \n\n$$\n\\begin{aligned}\nx_1 - x_3 & = 0 \\\\\nx_2 - \\frac{1}{2} x_3 & = 0 \\\\\nx_3 & = x_3\n\\end{aligned}\n$$\n\nFixing $x_3 = 1$ gives the eigenvector associated with $\\lambda_1 = -4$ of $\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}$. We can verify that this is an eigenvector with matrix multiplication to show $\\mathbf{A} \\mathbf{x}_1 = \\lambda_1 \\mathbf{x}_1$\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} -4 \\\\ -2 \\\\ -4 \\end{pmatrix}  = -4 \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n$$\n\nUsing `R`, this is \n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(-21/5, -6/5, -4, -34/5, -14/5, -10, 18/5,  3/5, 5), 3, 3)\nlambda_1 <- -4\nrref(cbind(A - lambda_1 * diag(nrow(A)), 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    0 -1.0    0\n[2,]    0    1 -0.5    0\n[3,]    0    0  0.0    0\n```\n:::\n:::\n\nVerifying that the eigenvalue $\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}$ is an eigenvector is\n\n::: {.cell}\n\n```{.r .cell-code}\nx_1 <- c(1, 1/2, 1)\nall.equal(drop(A %*% x_1), lambda_1 * x_1) # drop() makes a matrix with one column a vector\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n\nb) The eigenvalues associated with the second eigenvector $\\lambda_2 = 3$ by solving\n\n$$\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda_2 \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich can be written as \n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n-21/5  - \\lambda_2 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - \\lambda_2 & 3/5 \\\\\n-4 & -10 & 5 - \\lambda_2\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nand can be solved for $\\lambda_2$ using an augmented matrix form and row operations to reduce to reduced row echelon form. \n\nLetting $\\lambda_2 = 3$, we have\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n-21/5  - 3 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - 3 & 3/5 \\\\\n-4 & -10 & 5 - 3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich results in the augmented matrix\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} -36/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & -29/5 & 3/5 & 0 \\\\ -4 & -10 & 2 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nReducing the augmented matrix to reduced row echelon form gives\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} -36/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & -29/5 & 3/5 & 0 \\\\ -4 & -10 & 2 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & -1/2 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich has solution \n\n$$\n\\begin{aligned}\nx_1 - \\frac{1}{2} x_3 & = 0 \\\\\nx_2  & = 0 \\\\\nx_3 & = x_3\n\\end{aligned}\n$$\n\nFixing $x_3 = 1$ gives the eigenvector associated with $\\lambda_2 = 3$ of $\\mathbf{x}_2 = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}$. We can verify that this is an eigenvector with matrix multiplication to show $\\mathbf{A} \\mathbf{x}_2 = \\lambda_2 \\mathbf{x}_2$\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} 3/2 \\\\ 0 \\\\ 3 \\end{pmatrix}  = 3 \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n$$\n\nUsing `R`, this is \n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(-21/5, -6/5, -4, -34/5, -14/5, -10, 18/5,  3/5, 5), 3, 3)\nlambda_2 <- 3\nrref(cbind(A - lambda_2 * diag(nrow(A)), 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    0 -0.5    0\n[2,]    0    1  0.0    0\n[3,]    0    0  0.0    0\n```\n:::\n:::\n\nVerifying that the eigenvalue $\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}$ is an eigenvector is\n\n::: {.cell}\n\n```{.r .cell-code}\nx_2 <- c(1/2, 0, 1)\nall.equal(drop(A %*% x_2), lambda_2 * x_2) # drop() makes a matrix with one column a vector\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nc) The eigenvalues associated with the third eigenvector $\\lambda_3 = -1$ by solving\n\n$$\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda_3 \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich can be written as \n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n-21/5  - \\lambda_3 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - \\lambda_3 & 3/5 \\\\\n-4 & -10 & 5 - \\lambda_3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nand can be solved for $\\lambda_3$ using an augmented matrix form and row operations to reduce to reduced row echelon form. \n\nLetting $\\lambda_3 = -1$, we have\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n-21/5  - -1 & -34/5 & 18/5 \\\\\n-6/5 & -14/5 - -1 & 3/5 \\\\\n-4 & -10 & 5 - -1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich results in the augmented matrix\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} -16/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & -9/5 & 3/5 & 0 \\\\ -4 & -10 & 6 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nReducing the augmented matrix to reduced row echelon form gives\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} -16/5 & -34/5 & 18/5 & 0 \\\\ -6/5 & -9/5 & 3/5 & 0 \\\\ -4 & -10 & 6 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich has solution \n\n$$\n\\begin{aligned}\nx_1 + x_3 & = 0 \\\\\nx_2  - x_3 & = 0 \\\\\nx_3 & = x_3\n\\end{aligned}\n$$\n\nFixing $x_3 = 1$ gives the eigenvector associated with $\\lambda_3 = -1$ of $\\mathbf{x}_3 = \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}$. We can verify that this is an eigenvector with matrix multiplication to show $\\mathbf{A} \\mathbf{x}_2 = \\lambda_2 \\mathbf{x}_2$\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} -21/5 & -34/5 & 18/5 \\\\ -6/5 & -14/5 & 3/5 \\\\ -4 & -10 & 5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\end{pmatrix}  = -1 \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n$$\n\nUsing `R`, this is \n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(-21/5, -6/5, -4, -34/5, -14/5, -10, 18/5,  3/5, 5), 3, 3)\nlambda_3 <- -1\nrref(cbind(A - lambda_3 * diag(nrow(A)), 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    1    0\n[2,]    0    1   -1    0\n[3,]    0    0    0    0\n```\n:::\n:::\n\nVerifying that the eigenvalue $\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}$ is an eigenvector is\n\n::: {.cell}\n\n```{.r .cell-code}\nx_3 <- c(-1, 1, 1)\nall.equal(drop(A %*% x_3), lambda_3 * x_3) # drop() makes a matrix with one column a vector\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nNow, let's compare the output of the `eigen()` function in `R` to these eigenvectors calculated \"by hand.\" The `eigen()` function returns the two objects named `$values` that contain the eigenvalues of $\\mathbf{A}$ and the object `$vectors` that contains a matrix of eigenvectors as the columns fo the matrix. Each eigenvalue corresponds to the respective column of the eigenvector matrix.\n\n::: {.cell}\n\n```{.r .cell-code}\neigen(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1] -4  3 -1\n\n$vectors\n          [,1]          [,2]       [,3]\n[1,] 0.6666667 -4.472136e-01 -0.5773503\n[2,] 0.3333333 -3.041122e-16  0.5773503\n[3,] 0.6666667 -8.944272e-01  0.5773503\n```\n:::\n:::\n\n\nNote that the eigenvectors returned by the `eigen()` function are the same as those in the example, but the vectors are different. However, the vectors from `eigen()` point in the same direction as those found \"by hand\" and only differ in the length of the vector. For example, we found the eigenvector associated with the eigenvalue -4 to be $\\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}$ which points in the same direction as the vector from `eigen()` of $\\begin{pmatrix} 2/3 \\\\ 1/3 \\\\ 2/3 \\end{pmatrix}$ which is just a scalar multiple of the vector found \"by hand.\" Recall that when we found a solution using RREF and the augmented matrix, the solution set was infinite (a line) and we just set the free variable equal to 1. Another equally valid solution would be to set the free variable so that the total length of the vector is 1, and this is what the `eigen()` function does.\n:::\n\n\n::: {#def-}\nLet $\\mathbf{A}$ be an $n \\times n$ matrix and let $\\lambda$ be an eigenvalue of $\\mathbf{A}$. Then, the **$\\lambda$-eigenspace** of $\\mathbf{A}$ is the solution set of the matrix equation $\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}$ which is the subspace null($\\mathbf{A} - \\lambda \n\\mathbf{I}$).\n:::\n\nTherefore, the $\\lambda$-eigenspace is a subspace (the null space of any matrix is a subspace) that contains the zero vector $\\mathbf{0}$ and all the eigenvectors of $\\mathbf{A}$ with corresponding eigenvalue $\\lambda$.\n\n::: {#exm-}\n\n::: {.cell}\n\n:::\n\nFor $\\lambda$ = (a) -2, (b) 1, and (c) 3, decide if $\\lambda$ is a eigenvalue of the matrix $\\mathbf{A} = \\begin{pmatrix} 3 & 0 \\\\ -3 & 2 \\end{pmatrix}$ and if so, compute a basis for the $\\lambda$-eigenspace.\n:::\n\n\n\n::: {.callout-note icon=false collapse=\"true\" appearance=\"simple\"} \n## Solution\n\nGiven the matrix $\\mathbf{A}$ defined in the example, we will check if any of the values of $\\lambda$ are eigenvalues. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <-  matrix(c(3, -3, 0, 2), 2, 2)\n```\n:::\n\n\na) First, we check if $\\lambda = -2$ is an eigenvalue of $\\mathbf{A}$. If $\\lambda = -2$ is an eigenvalue of $\\mathbf{A}$, then there is a non-trivial solution to\n\n$$\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda\\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nThe homogeneous system of equations can be written as \n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n3  - \\lambda & 0 \\\\\n-3 & 2 - \\lambda \n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nand can be solved for $\\lambda = -2$ using an augmented matrix form and row operations to reduce to reduced row echelon form where\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n3  - -2 & 0 \\\\\n-3 & 2 - -2\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich results in the augmented matrix\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 5 & 0 & 0 \\\\ -3 & 4 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nReducing the augmented matrix to reduced row echelon form gives\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 5 & 0 & 0 \\\\ -3 & 4 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich has solution \n\n$$\n\\begin{aligned}\nx_1  & = 0 \\\\\nx_2  & = 0 \n\\end{aligned}\n$$\n\nwhich is the trivial solution. Thus, $\\lambda = -2$ is not an eigenvalue of $\\mathbf{A}$.\n\nUsing `R`, this is \n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nA <-  matrix(c(3, -3, 0, 2), 2, 2)\nlambda <- -2\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n```\n:::\n:::\n\nBecause there is only the trivial solution $\\mathbf{x} = \\mathbf{0}$, $\\lambda = -2$ is not an eigenvalue of $\\mathbf{A}$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- 1\n```\n:::\n\nb) Next, we check if $\\lambda = 1$ is an eigenvalue of $\\mathbf{A}$. If $\\lambda = 1$ is an eigenvalue of $\\mathbf{A}$, then there is a non-trivial solution to\n\n$$\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda\\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nThe homogeneous system of equations can be written as \n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n3  - \\lambda & 0 \\\\\n-3 & 2 - \\lambda \n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nand can be solved for $\\lambda = 1$ using an augmented matrix form and row operations to reduce to reduced row echelon form where\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n3  - 1 & 0 \\\\\n-3 & 2 - 1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich results in the augmented matrix\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 2 & 0 & 0 \\\\ -3 & 1 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nReducing the augmented matrix to reduced row echelon form gives\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 2 & 0 & 0 \\\\ -3 & 1 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich has solution \n\n$$\n\\begin{aligned}\nx_1  & = 0 \\\\\nx_2  & = 0 \n\\end{aligned}\n$$\n\nwhich is the trivial solution. Thus, $\\lambda = 1$ is not an eigenvalue of $\\mathbf{A}$.\n\nFixing $x_3 = 1$ gives the eigenvector associated with $\\lambda_3 = NA$ of $\\mathbf{x}_3 = \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}$. We can verify that this is an eigenvector with matrix multiplication to show $\\mathbf{A} \\mathbf{x}_2 = \\lambda_2 \\mathbf{x}_2$\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 3 & 0 \\\\ -3 & 2 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} -3 \\\\ 5 \\end{pmatrix}  = 1 \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n$$\n\nUsing `R`, this is \n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nA <-  matrix(c(3, -3, 0, 2), 2, 2)\nlambda <- 1\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n```\n:::\n:::\n\nBecause there is only the trivial solution $\\mathbf{x} = \\mathbf{0}$, $\\lambda = 1$ is not an eigenvalue of $\\mathbf{A}$.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- 3\n```\n:::\n\nc) Next, we check if $\\lambda = 3$ is an eigenvalue of $\\mathbf{A}$. If $\\lambda = 3$ is an eigenvalue of $\\mathbf{A}$, then there is a non-trivial solution to\n\n$$\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda\\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nThe homogeneous system of equations can be written as \n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n3  - \\lambda & 0 \\\\\n-3 & 2 - \\lambda \n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nand can be solved for $\\lambda = 3$ using an augmented matrix form and row operations to reduce to reduced row echelon form where\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n3  - 3 & 0 \\\\\n-3 & 2 - 3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich results in the augmented matrix\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 0 & 0 & 0 \\\\ -3 & -1 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nReducing the augmented matrix to reduced row echelon form gives\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 0 & 0 & 0 \\\\ -3 & -1 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 1/3 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich has solution \n\n$$\n\\begin{aligned}\nx_1  + \\frac{1}{3} x_2 & = 0 \\\\\nx_2  & = x_2\n\\end{aligned}\n$$\n\nwhere the solution can be chosen by setting $x_2 = 1$ giving the eigenvector associated with $\\lambda = 3$ of $\\mathbf{x} = \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}$. We can verify that this is an eigenvector with matrix multiplication to show $\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}$\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 3 & 0 \\\\ -3 & 2 \\end{pmatrix} \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}  = 3 \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n$$\n\nUsing `R`, this is \n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <-  matrix(c(3, -3, 0, 2), 2, 2)\nlambda <- 3\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]      [,2] [,3]\n[1,]    1 0.3333333    0\n[2,]    0 0.0000000    0\n```\n:::\n:::\n\nVerifying that the eigenvalue $\\mathbf{x} = \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}$ is an eigenvector is\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(-1/3, 1)\nall.equal(drop(A %*% x), lambda * x) # drop() makes a matrix with one column a vector\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nNow, because we know that $\\lambda = 3$ is an eigenvalue of $\\mathbf{A}$ and the homogeneous system of equations $\\left(\\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}$ has a unique solution, the vector $\\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}$ forms a basis for the 3-eigenspace of $\\mathbf{A}$.\n:::\n\n\n::: {#exm-}\n\n::: {.cell}\n\n:::\n\nLet $\\mathbf{A} = \\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix}$. Find the eigenvectors associated with the eigenvalues (a) $\\lambda = 3$ and (b) $\\lambda = 1$. For each eigen value, also find the basis for the associated eigen-space.\n:::\n\n\n::: {.callout-note icon=false collapse=\"true\" appearance=\"simple\"} \n## Solution\n\nGiven the matrix $\\mathbf{A} = \\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix}$, we can find the eigenvectors associated with the given eigenvalues\n\n\n::: {.cell}\n\n:::\n\n\na) The eigenvalues associated with the first eigenvector $\\lambda = 3$ by solving\n\n$$\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich can be written as \n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n17/5  - \\lambda & 8/5 & -6/5 \\\\\n0 & 3 - \\lambda & 0 \\\\\n4/5 & 16/5 & 3/5 - \\lambda\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nand can be solved for $\\lambda$ using an augmented matrix form and row operations to reduce to reduced row echelon form. \n\nLetting $\\lambda = 3$, we have\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n17/5  - 3 & 8/5 & -6/5 \\\\\n0 & 3 - 3 & 0 \\\\\n4/5 & 16/5 & 3/5 - 3\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich results in the augmented matrix\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 2/5 & 8/5 & -6/5 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 4/5 & 16/5 & -12/5 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nReducing the augmented matrix to reduced row echelon form gives\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 2/5 & 8/5 & -6/5 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 4/5 & 16/5 & -12/5 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 4 & -3 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich has solution \n\n$$\n\\begin{aligned}\nx_1 + 4 x_2 - x_33 & = 0 \\\\\nx_2 & = x_2 \\\\\nx_3 & = x_3\n\\end{aligned}\n$$\n\nWhere there are two free variables which suggests that the dimension of the solution space is 2 (the solution set defines a plane and will have 2 basis vectors. Fixing $x_2 = 1$ and $x_3 = 0$ gives the first eigenvector associated with $\\lambda = 3$ of $\\mathbf{x}_1 = \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix}$. We can verify that this is an eigenvector with matrix multiplication to show $\\mathbf{A} \\mathbf{x}_1 = \\lambda \\mathbf{x}_1$\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix} \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix} & = \\begin{pmatrix} -12 \\\\ 3 \\\\ 0 \\end{pmatrix}  = 3 \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nThe second basis vector for the eigenspace associated with $\\lambda = 3$ can be found by fixing $x_2 = 0$ and $x_3 = 1$ to get the eigenvector $\\mathbf{x}_2 = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}$. We can verify that this is an eigenvector with matrix multiplication to show $\\mathbf{A} \\mathbf{x}_2 = \\lambda \\mathbf{x}_2$\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} 9 \\\\ 0 \\\\ 3 \\end{pmatrix}  = 3 \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n$$\n\nThus, the basis for 3-eigenspace is $\\left\\{ \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}\\right\\}$. Note that this is the same as finding a basis for the null space of $\\left(\\mathbf{A} - \\lambda \\mathbf{I} \\right)$.\n\n\nUsing `R`, this is \n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(17/5, 0, 4/5, 8/5, 3, 16/5, -6/5, 0, 3/5), 3, 3)\nlambda <- 3\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    4   -3    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n```\n:::\n:::\n\nVerifying that the eigenvalue $\\mathbf{x}_1 = \\begin{pmatrix} -4 \\\\ 1 \\\\ 0 \\end{pmatrix}$ is an eigenvector and that $\\mathbf{x}_2 = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}$ is an eigenvector\n\n::: {.cell}\n\n```{.r .cell-code}\nx_1 <- c(-4, 1, 0)\nall.equal(drop(A %*% x_1), lambda * x_1) # drop() makes a matrix with one column a vector\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nx_2 <- c(3, 0, 1)\nall.equal(drop(A %*% x_2), lambda * x_2) # drop() makes a matrix with one column a vector\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- 1\n```\n:::\n\nb) The eigenvalues associated with the second eigenvector $\\lambda = 1$ by solving\n\n$$\n\\begin{aligned}\n\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich can be written as \n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n17/5  - \\lambda & 8/5 & -6/5 \\\\\n0 & 3 - \\lambda & 0 \\\\\n4/5 & 16/5 & 3/5 - \\lambda\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nand can be solved for $\\lambda$ using an augmented matrix form and row operations to reduce to reduced row echelon form. \n\nLetting $\\lambda = 1$, we have\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} \n17/5  - 1 & 8/5 & -6/5 \\\\\n0 & 3 - 1 & 0 \\\\\n4/5 & 16/5 & 3/5 - 1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} & = \\mathbf{0}\n\\end{aligned}\n$$\n\nwhich results in the augmented matrix\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 12/5 & 8/5 & -6/5 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 4/5 & 16/5 & -2/5 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nReducing the augmented matrix to reduced row echelon form gives\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 12/5 & 8/5 & -6/5 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 4/5 & 16/5 & -2/5 & 0 \\end{pmatrix} & \\stackrel{rref}{\\sim} \\begin{pmatrix} 1 & 0 & -1/2 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n\\end{aligned}\n$$\n\nwhich has solution \n\n$$\n\\begin{aligned}\nx_1 - \\frac{1}{2} x_3 & = 0 \\\\\nx_2  & = 0 \\\\\nx_3 & = x_3\n\\end{aligned}\n$$\n\nFixing $x_3 = 1$ gives the eigenvector associated with $\\lambda= 1$ of $\\mathbf{x} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}$. We can verify that this is an eigenvector associated with eigenvalue $\\lambda = 1$ with matrix multiplication to show $\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}$\n\n$$\n\\begin{aligned}\n\\begin{pmatrix} 17/5 & 8/5 & -6/5 \\\\ 0 & 3 & 0 \\\\ 4/5 & 16/5 & 3/5 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix} & = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}  = NA \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\end{aligned}\n$$\n\nTherefore, a basis for the 1-eigenspace is $\\left\\{ \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right\\}$\n\n\nUsing `R`, this is \n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(17/5, 0, 4/5, 8/5, 3, 16/5, -6/5, 0, 3/5), 3, 3)\nlambda <- 1\nrref(cbind(A - lambda * diag(nrow(A)), 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    0 -0.5    0\n[2,]    0    1  0.0    0\n[3,]    0    0  0.0    0\n```\n:::\n:::\n\nVerifying that the eigenvalue $\\mathbf{x} = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\end{pmatrix}$ is an eigenvector associated with eigenvalue $\\lambda = 1$ is\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(1/2, 0, 1)\nall.equal(drop(A %*% x), lambda * x) # drop() makes a matrix with one column a vector\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n:::\n\n\n\n\n\n### Computing Eigenspaces\n\nLet $\\mathbf{A}$ be a $n \\times n$ matrix and let $\\lambda$ be a scalar.\n\n1) $\\lambda$ is an eigenvalue of $\\mathbf{A}$ if and only if $(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}$ has a non-trivial solution. The matrix equation $(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}$ has a non-trivial solution if and only if null$(\\mathbf{A} - \\lambda \\mathbf{I}) \\neq \\{\\mathbf{0} \\}$\n\n2) Finding a basis for the $\\lambda$-eigenspace of $\\mathbf{A}$ is equivalent to finding a basis for  null$(\\mathbf{A} - \\lambda \\mathbf{I})$ which can be done by finding parametric forms of the solutions of the homogeneous system of equations $(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}$.\n\n3) The dimension of the $\\lambda$-eigenspace of $\\mathbf{A}$ is equal to the number of free variables in the system of equations $(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}$ which is the number of non-pivot columns of $\\mathbf{A} - \\lambda \\mathbf{I}$.\n\n4) The eigenvectors with eigenvalue $\\lambda$ are the nonzero vectors in null$(\\mathbf{A} - \\lambda \\mathbf{I})$ which are equivalent to the nontrivial solutions of $(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}$.\n\nNote that this leads of a fact about the $0$-eigenspace. \n\n::: {#def-}\nLet $\\mathbf{A}$ be an $n \\times n$ matrix. Then\n\n1) The number 0 is an eigenvalue of $\\mathbf{A}$ if and only if $\\mathbf{A}$ is not invertible.\n\n2) If 0 is an eigenvalue of $\\mathbf{A}$, then the 0-eigenspace of $\\mathbf{A}$ is null$(\\mathbf{A})$.\n:::\n\n::: {.callout-tip icon=false collapse=\"true\" appearance=\"simple\"} \n## Proof\n\n0 is an eigenvalue of $\\mathbf{A}$ if and only if null$(\\mathbf{A} - 0 \\mathbf{I})$ = null$(\\mathbf{A})$. By the invertible matrix theorem, $\\mathbf{A}$ is invertible if and only if null$(\\mathbf{A}) = \\{\\mathbf{0}\\}$ but we know that the 0-eigenspace of $\\mathbf{A}$ is not the trivial set $\\{\\mathbf{0}\\}$ because 0 is an eigenvalue.\n:::\n\n\n:::{#thm-}\n## Invertible Matrix Theorm + eigenspaces\nThis is an extension of the prior statement of the invertible matrix @thm-invertiblematrix\nLet $\\mathbf{A}$ be an $n \\times n$ matrix and $T: \\mathcal{R}^n \\rightarrow \\mathcal{R}^n$ be the linear transformation given by $T(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}$. Then the following statements are equivalent (i.e., they are all either simultaneously true or false).\n\n1) $\\mathbf{A}$ is invertible.\n\n2) $\\mathbf{A}$ has n pivot columns.\n\n3) null$(\\mathbf{A}) = \\{\\mathbf{0}\\}$.\n\n4) The columns of $\\mathbf{A}$ are linearly independent.\n\n5) The columns of $\\mathbf{A}$ span $\\mathcal{R}^n$.\n\n6) The matrix equation $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ has a uniqu solution for each $\\mathbf{b} \\in \\mathcal{R}^n$.\n\n7) The transormation $T$ is invertible.\n\n8) The transormation $T$ is one-to-one.\n\n9) The transormation $T$ is onto.\n\n10) det$(\\mathbf{A}) \\neq 0$\n    \n11) 0 is not an eigenvalue of $\\mathbf{A}$\n    \n:::\n\n",
    "supporting": [
      "19-eigenvectors-and-eigenvalues_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}