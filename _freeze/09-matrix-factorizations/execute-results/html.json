{
  "hash": "e7048ed51f3266d6b28ad40a0bd8d03f",
  "result": {
    "markdown": "# Matrix Factorizations {#matrix-factorizations}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n    library(tidyverse)\nlibrary(dasc2594)\nlibrary(mvnfast)\nlibrary(MASS)\n```\n:::\n\n\n\nIn scalar mathematics, a factorization is an expression that writes a scalar $a$ as a product of two or more scalars. For example, the scalar 2 has a square-root factorization of $2  =\\sqrt{2} * \\sqrt{2}$ and 15 has a prime factorization of $15 = 3 * 5$. A matrix factorization is a similar concept where a matrix $\\mathbf{A}$ can be represented by a product or two or more matrices (e.g., $\\mathbf{A} = \\mathbf{B} \\mathbf{C}$). In data science, matrix factorizations are fundamental to working with data. \n\n## The LU factorization\n\nFirst, we define lower and upper triangular matrices. \n\n::: {#def-}\nThe matrix $\\mathbf{A}$ is said to be lower triangular if\n$$\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} \na_{11} & 0 & 0 & \\cdots & 0 \\\\\na_{21} & a_{22} & 0 & \\cdots & 0 \\\\\na_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn} \\\\\n\\end{pmatrix}\n\\end{aligned}\n$$    \nSimilarly, the matrix $\\mathbf{A}$ is said to be upper triangular if\n$$\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} \na_{11} & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\n0 & a_{22} & a_{23} & \\cdots & a_{2n} \\\\\n0 & 0 & a_{33} & \\cdots & a_{3n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & a_{nn} \\\\\n\\end{pmatrix}\n\\end{aligned}\n$$    \n:::\n\n\n\nThe LU factorization of a matrix $\\mathbf{A}$ reduces the matrix $\\mathbf{A}$ into two components. The first component $\\mathbf{L}$ is a lower-triangular matrix and the second component $\\mathbf{U}$ is an upper triangular matrix.\n\nUsing the LU factorization, the matrix factorization $\\mathbf{A} = \\mathbf{L} \\mathbf{U}$ can be used in the matrix equation $\\mathbf{A} \\mathbf{x} = \\mathbf{L} \\mathbf{U}\\mathbf{x} = \\mathbf{b}$ by first solving the sub-equation $\\mathbf{L} \\mathbf{y} = \\mathbf{b}$ and then solving the second sub-equation $\\mathbf{U} \\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$. Thus, the matrix factorization applied to the matrix equation gives the pair of equations\n\n$$\n\\begin{aligned}\n\\mathbf{L} \\mathbf{y} & = \\mathbf{b} \\\\\n\\mathbf{U} \\mathbf{x} & = \\mathbf{y}\n\\end{aligned}\n$${#eq-LU}\n\nAt first glance, this seems like we are trading the challenge of solving one system of equations $\\mathbf{A}\\mathbf{x}$ @eq-matrixequation for the two equations in @eq-LU. However, the computational benefits arise due to the fact that $\\mathbf{L}$ and $\\mathbf{U}$ are triangular matrices and solving matrix equations with triangular matrices is much faster. \n\n\n\n\n\n\n::: {#exm-}\nLet $\\mathbf{A} = \\begin{pmatrix} 1 & 0 & 2 & -2 \\\\ -2 & -2 & -4 & 1 \\\\ -1 & -4 & -8 & 5 \\\\ -2 & -6 & -4 & 4 \\end{pmatrix}$ which has the LU decomposition\n\n$$\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} 1 & 0 & 2 & -2 \\\\ -2 & -2 & -4 & 1 \\\\ -1 & -4 & -8 & 5 \\\\ -2 & -6 & -4 & 4 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ -2 & -1 & 0 & 0 \\\\ -1 & -2 & -3 & 0 \\\\ -2 & -3 & 0 & -3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 2 & -2 \\\\ 0 & 2 & 0 & 3 \\\\ 0 & 0 & 2 & -3 \\\\ 0 & 0 & 0 & -3 \\end{pmatrix}\n\\end{aligned}\n$$\nand consider the system of equations defined by the matrix equation $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ where $\\mathbf{b} = \\begin{pmatrix} -5 \\\\ -7 \\\\ -2 \\\\ -14 \\end{pmatrix}$. \n\n1) solve $\\mathbf{L} \\mathbf{y} = \\mathbf{b}$ using an augmented matrix and RREF. \n\n2) solve $\\mathbf{U} \\mathbf{x} = \\mathbf{y}$ using an augmented matrix and RREF.\n\n3) compare to the solution $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ using an augmented matrix and RREF.\n:::\n\n\n::: {.callout-note icon=false collapse=\"true\" appearance=\"simple\"}\n## Solution\n\n\n::: {.cell}\n\n:::\n\n\nFor the example, we will show how to solve a system of equations using the LU decomposition for the equation defined above.\n \n1) Solve $\\mathbf{L} \\mathbf{y} = \\mathbf{b}$ using augmented matrix\n\n\n$$\n\\begin{aligned}\n& & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ -2 & -1 & 0 & 0 & -7 \\\\ -1 & -2 & -3 & 0 & -2 \\\\ -2 & -3 & 0 & -3 & -14 \\end{pmatrix} & \\stackrel{r_2 \\leftarrow -\\frac{1}{2} r_2 - r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1/2 & 0 & 0 & 17/2 \\\\ -1 & -2 & -3 & 0 & -2 \\\\ -2 & -3 & 0 & -3 & -14 \\end{pmatrix} \\\\\n& \\stackrel{r_3 \\leftarrow - r_3 - r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1/2 & 0 & 0 & 17/2 \\\\ 0 & 2 & 3 & 0 & 7 \\\\ -2 & -3 & 0 & -3 & -14 \\end{pmatrix} & \\stackrel{r_4 \\leftarrow - \\frac{1}{2} r_4 - r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1/2 & 0 & 0 & 17/2 \\\\ 0 & 2 & 3 & 0 & 7 \\\\ 0 & 3/2 & 0 & 3/2 & 12 \\end{pmatrix} \\\\\n& \\stackrel{r_2 \\leftarrow 2 r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1 & 0 & 0 & 17 \\\\ 0 & 2 & 3 & 0 & 7 \\\\ 0 & 3/2 & 0 & 3/2 & 12 \\end{pmatrix} & \\stackrel{r_3 \\leftarrow \\frac{1}{2} r_3 - r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1 & 0 & 0 & 17 \\\\ 0 & 0 & 3/2 & 0 & -27/2 \\\\ 0 & 3/2 & 0 & 3/2 & 12 \\end{pmatrix} \\\\\n& \\stackrel{r_4 \\leftarrow \\frac{2}{3} r_4 - r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1 & 0 & 0 & 17 \\\\ 0 & 0 & 3/2 & 0 & -27/2 \\\\ 0 & 0 & 0 & 1 & -9 \\end{pmatrix} & \\stackrel{r_3 \\leftarrow \\frac{2}{3} r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & -5 \\\\ 0 & 1 & 0 & 0 & 17 \\\\ 0 & 0 & 1 & 0 & -9 \\\\ 0 & 0 & 0 & 1 & -9 \\end{pmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n:::\n\n\n2) solve $\\mathbf{U} \\mathbf{x} = \\mathbf{y}$ using an augmented matrix and RREF.\n\n\n$$\n\\begin{aligned}\n& & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 2 & 0 & 3 & 17 \\\\ 0 & 0 & 2 & -3 & -9 \\\\ 0 & 0 & 0 & -3 & -9 \\end{pmatrix} & \\stackrel{r_2 \\leftarrow \\frac{1}{2} r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 2 & -3 & -9 \\\\ 0 & 0 & 0 & -3 & -9 \\end{pmatrix} \\\\\n& \\stackrel{r_3 \\leftarrow \\frac{1}{2} r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & -3 & -9 \\end{pmatrix} & \\stackrel{r_4 \\leftarrow - \\frac{1}{3} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} \\\\\n& \\stackrel{r_1 \\leftarrow r_1 -2  r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 1 & 4 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} & \\stackrel{r_1 \\leftarrow r_1 - 2 r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} \\\\\n& \\stackrel{r_2 \\leftarrow r_2 - \\frac{3}{2} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 4 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} & \n\\stackrel{r_3 \\leftarrow r_3 + \\frac{3}{2} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 4 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} \n\\end{aligned}\n$$\n\n3) compare to the solution $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ using an augmented matrix and RREF.\n\n\n\n::: {.cell}\n\n:::\n\n$$\n\\begin{aligned}\n& & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ -2 & -2 & -4 & 1 & -7 \\\\ -1 & -4 & -8 & 5 & -2 \\\\ -2 & -6 & -4 & 4 & -14 \\end{pmatrix} & \\stackrel{r_2 \\leftarrow r_2 + 2 r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & -2 & 0 & -3 & -17 \\\\ -1 & -4 & -8 & 5 & -2 \\\\ -2 & -6 & -4 & 4 & -14 \\end{pmatrix} \\\\\n& \\stackrel{r_3 \\leftarrow r_3 + r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & -2 & 0 & -3 & -17 \\\\ 0 & -4 & -6 & 3 & -7 \\\\ -2 & -6 & -4 & 4 & -14 \\end{pmatrix} & \\stackrel{r_4 \\leftarrow r_4 + 2 r_1}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & -2 & 0 & -3 & -17 \\\\ 0 & -4 & -6 & 3 & -7 \\\\ 0 & -6 & 0 & 0 & -24 \\end{pmatrix} \\\\\n& \\stackrel{r_2 \\leftarrow - \\frac{1}{2} r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & -4 & -6 & 3 & -7 \\\\ 0 & -6 & 0 & 0 & -24 \\end{pmatrix} & \\stackrel{r_3 \\leftarrow r_3 + 4 r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & -6 & 9 & 27 \\\\ 0 & -6 & 0 & 0 & -24 \\end{pmatrix} \\\\\n& \\stackrel{r_4 \\leftarrow r_4 + 6 r_2}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & -6 & 9 & 27 \\\\ 0 & 0 & 0 & 9 & 27 \\end{pmatrix} & \n\\stackrel{r_3 \\leftarrow - \\frac{1}{6} r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 9 & 27 \\end{pmatrix}\\\\\n& \\stackrel{r_4 \\leftarrow \\frac{1}{9} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 2 & -2 & -5 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} & \\stackrel{r_1 \\leftarrow r_1 - 2 r_3}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 1 & 4 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} \\\\\n& \\stackrel{r_1 \\leftarrow r_1 - r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 3/2 & 17/2 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} & \n\\stackrel{r_2 \\leftarrow r_2 - \\frac{3}{2} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 4 \\\\ 0 & 0 & 1 & -3/2 & -9/2 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix} \\\\\n& \\stackrel{r_3 \\leftarrow r_3 + \\frac{3}{2} r_4}{\\sim} & \\begin{pmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 4 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 3 \\end{pmatrix}  \n\\end{aligned}\n$$\n\nWhile it might not be completely obvious, once one has calculated the LU decomposition, it can often be much faster to solve systems of equations with the LU decomposition.\n:::\n\n\n:::{.#exm-}\nin lab:\nSolve some large systems of equations by brute force which shows how the LU decomposition is faster. \n:::    \n\n### Geometric interpretation of the LU factorization\n\n* **Draw image in class -- composition of transformations $T_A(\\cdot) = T_L(T_U(\\cdot))$**\n\n\n\n## Obtaining the LU factorization\n\nNotice that the upper-triangular matrix $\\mathbf{U}$ is in echelon form. Congratulations! you know how to construct a matrix $\\mathbf{U}$ by reducing the matrix $\\mathbf{A}$ to an echelon form $\\mathbf{U}$ using elementary matrices $\\mathbf{E}_1, \\ldots \\mathbf{E}_k$. Now, we only need to find the lower triangular matrix $\\mathbf{L}$. \n\nCombining the LU factorization and the fact that we can find an upper triangular matrix $\\mathbf{U}$ using elementary row matrices, we have\n\n$$\n\\begin{aligned}\n\\mathbf{A} & = \\mathbf{L} \\mathbf{U} \\\\\n\\mathbf{E}_k \\cdots \\mathbf{E}_1 \\mathbf{A} & = \\mathbf{U}.\n\\end{aligned}\n$${#eq-LUalg}\nWe also know that each of the elementary row matrices $\\mathbf{E}_j$ are invertible (you can always re-swap rows, subtract instead of add rows, etc.) which says that each inverse $\\mathbf{E}_j^{-1}$ exists. Thus, the product $\\mathbf{E}_k \\cdots \\mathbf{E}_1$ must have an inverse which is \n$$\n\\begin{aligned}\n(\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1} & = \\mathbf{E}_1^{-1} \\cdots \\mathbf{E}_k^{-1}.\n\\end{aligned}\n$$\nPlugging this inverse into @eq-LUalg gives (left multiplying by $(\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}$ on both sides)\n$$\n\\begin{aligned}\n(\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1} (\\mathbf{E}_k \\cdots \\mathbf{E}_1) \\mathbf{A} & = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\mathbf{U} \\\\\n \\mathbf{A} & = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\mathbf{U} \\\\\n & = \\mathbf{L} \\mathbf{U}\n\\end{aligned}\n$$\nwhere $\\mathbf{L} = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}$\n\n\n**Algorithm for finding the LU decomposition**\n\nGiven the matrix $\\mathbf{A}$\n\n1) Find elementary matrices $\\mathbf{E}_1, \\ldots, \\mathbf{E}_k$ such that $\\mathbf{E}_k \\cdots \\mathbf{E}_1 \\mathbf{A}$ is in row echelon form (if this is possible, otherwise an LU factorization does not exist). Call this matrix $\\mathbf{U}$, the upper triangular component of the LU factorization.\n\n2) The, the lower triangular $\\mathbf{L} = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}$.\n\n\nNotice that the algorithm does not say to find a specific matrix $\\mathbf{U}$. In general, any row echelon form matrix $\\mathbf{U}$ will work.\n\n## The Cholesky factor\n\nA Cholesky decomposition is special type of LU decomposition. A Cholesky decomposition is an LU decomposition on a **symmetric, positive-definite** square matrix. \n\n::: {#def-spd} \n\nIf a matrix $\\mathbf{A}$ meets the following two conditions, the matrix $\\mathbf{A}$ is said to be **symmetric, positive-definite.**\n\n1) A matrix $\\mathbf{A}$ is said to by symmetric if $\\mathbf{A} = \\mathbf{A}'$\n\nb) A $n \\times n$ matrix is said to be positive definite if for all $\\mathbf{x} \\in \\mathcal{R}^n$, the quadratic form $\\mathbf{x}' \\mathbf{A }\\mathbf{x} \\geq 0$        \n\n:::\n\nNote: the condition of positive definiteness is actually impossible to check. Can you show this is true for all vectors? Luckily, a $n \\times n$ symmetric matrix is positive definite if and only if the matrix $\\mathbf{A}$ is invertible (which we know about by the invertible matrix theorem @thm-invertiblematrix).\n\n::: {#def-}\nLet $\\mathbf{A}$ be a symmetric, positive definite matrix (by this, $\\mathbf{A}$ is a $n \\times n$ square matrix). Then\n$$\n\\begin{aligned}\n\\mathbf{A} = \\mathbf{L} \\mathbf{L}'\n\\end{aligned}\n$$\nis the Cholesky decomposition of $\\mathbf{A}$ if $\\mathbf{L}$ is a lower-triangular matrix. Also, the lower triangular Cholesky matrix $\\mathbf{L}$ is unique. \n:::\n\nWhat makes the Cholesky factor special? \n\n* The decomposition $\\mathbf{A} = \\mathbf{L} \\mathbf{U}$ has the property that $\\mathbf{U} = \\mathbf{L}'$ so that the computer only has to store one of the matrix components (reduce memory demands). As about half of the elements of $\\mathbf{L}$ are 0, matrix multiplication is much less computationally demanding as about half of the flops are not required to be evaluated (x * 0 =  0).\n\n* The Cholesky factor is unique. There is only one Cholesky factor for each symmetric positive definite matrix. \n\n* The Cholesky has properties related to multivariate normal distributions. \n\nLet $\\mathbf{y} \\sim \\operatorname{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$, and $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}'$. Then, if $\\mathbf{z} \\sim \\operatorname{N}(\\mathbf{0}, \\mathbf{I})$, then $\\mathbf{L} \\mathbf{z} \\sim \\operatorname{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$. We say the $\\mathbf{y}$ and $\\mathbf{L}\\mathbf{z}$ are equal in distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate N 2-dimensional random normal vectors \nN <- 5000\nmu    <- rep(0, 2)\nSigma <- matrix(c(2, 1.5, 1.5, 2), 2, 2)\ny <- rmvn(N, mu, Sigma)\n\n# calculate the Cholesky factor\nL <- t(chol(Sigma))    # R calculates the upper (right) Cholesky factor by default\nz <- rmvn(N, mu, diag(2))\nLz <- t(L %*% t(z))    # pay attention to the dimensions of L and z here...\n\ndata.frame(\n    observation = 1:N,\n    x1          = c(y[, 1], z[, 1], Lz[, 1]),\n    x2          = c(y[, 2], z[, 2], Lz[, 2]),\n    variable    = factor(rep(c(\"y\", \"z\", \"Lz\"), each = N), levels = c(\"y\", \"z\", \"Lz\"))\n) %>%\n    ggplot(aes(x = x1, y = x2, color = variable)) +\n    geom_point(alpha = 0.1) +\n    geom_density2d() +\n    facet_wrap(~ variable)\n```\n\n::: {.cell-output-display}\n![](09-matrix-factorizations_files/figure-html/example-cholesky-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "09-matrix-factorizations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}