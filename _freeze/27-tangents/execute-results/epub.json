{
  "hash": "58770951d11278e6a2c5a7d82a03a5c9",
  "result": {
    "markdown": "# Tangent planes and linear approximations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(dasc2594)\nset.seed(2021)\n```\n:::\n\n\n\n\nLet $f(\\mathbf{x})$ be a differentiable function at a point $\\mathbf{a}$. Because the function is differentiable at $\\mathbf{a}$, this means that all paths $P_\\mathbf{a}$ that approach the point $\\mathbf{a}$ from all directions all take on values $f(P_\\mathbf{a})$ that are \"close\" to $f(\\mathbf{a})$. Mathematically, we describe this as **smoothness**. A more explicit description says that as the paths $P_{\\mathbf{a}}$ get very close to $\\mathbf{a}$, the space over which these paths are defined starts to look more and more like a flat surface--the **tangent plane**. \n\n::: {#exm-}\nFor this example, we plot the function $f(x, y) = x^2 + y^2$ which has gradient $\\nabla f(x, y) = \\begin{pmatrix} 2x \\\\ 2y\\end{pmatrix}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# f(x, y)\ntarget_fun <- function(x, y) {\n    return(x^2 + y^2)\n}\n# gradient f(x, y)\ngrad_fun <- function(x, y) {\n    c(2 * x, 2 * y) # notice that the return value is a vector\n}\n# plot \nplot_tangent_plane(target_fun = target_fun, grad_fun = grad_fun, a=-1, b = 1)\n# zoomed in plot\nplot_tangent_plane(target_fun = target_fun, grad_fun = grad_fun, a=-1, b = 1, xlim = c(-1.5, 0.5), ylim = c(0.5, 1.5))\n# super zoomed in plot\nplot_tangent_plane(target_fun = target_fun, grad_fun = grad_fun, a=-1, b = 1, xlim = c(-1.1, -0.9), ylim = c(0.9, 1.1))\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](./webshot-images/tangent1.png){width=100%}\n:::\n\n::: {.cell-output-display}\n![](./webshot-images/tangent2.png){width=100%}\n:::\n\n::: {.cell-output-display}\n![](./webshot-images/tangent3.png){width=100%}\n:::\n:::\n\n\n\n\n:::\n\n\nA consequence of this result that when you zoom in on a differentiable function the function looks like a flat plane is that the function $f(x, y)$ can be approximated locally as a linear function (local approximation just means that if you are really \"close\" to the point $(a, b)$ that the function will behave like a tangent plane if the function is differentiable). Intuitively, this makes sense as if the derivative exists, the directional derivatives are just vectors and a linear combination of vectors (in $\\mathcal{R}^2$) produces a tangent plane (in higher dimensions, this is called a **hyperplane**). This means that if the function $f(x, y)$ is differentiable at the point $(a, b)$, then $f(x, y)$ for points $(x, y)$ close to $(a, b)$ is approximated by the linear tangent plane.\n\n\nNotice in the code above that there are two functions needed to calculate the tangent plane: the function $f(x, y)$ and the gradient $\\nabla f(x, y)$. This can be seen in the definition  of the tangent plane.\n\n::: {#def-}\n## The Tangent plane\n\nLet $f(x, y)$ be a differentiable function at the point $(a, b)$. Then the tangent plane to the surface defined by the function $f(x, y)$ at the point $(a, b)$ is given by the equation\n\n\n\n$$\n\\begin{aligned}\nz & = f(a, b) + f_x(a, b) (x - a) + f_y(a, b) (y - b) \\\\\n& = f(a, b) + \\nabla f(x, y) |_{(a, b)} \\cdot \\begin{pmatrix} x - a \\\\ y - b \\end{pmatrix} \\\\\n& = f(a, b) + (\\nabla f(x, y) |_{ (a, b)})' \\begin{pmatrix} x - a \\\\ y - b \\end{pmatrix} \\\\,\n\\end{aligned}\n$$\n\n\n\nwhere the tangent plane is defined as the dot product of the graidient vector $\\nabla f(x, y) |_{(a, b)}$ evaluated at the point $(a, b)$ and the vector $\\begin{pmatrix} x - a \\\\ y - b \\end{pmatrix}$ that is the coordinate-wise distance of the point $(x, y)$ from the point $(a, b)$.\n\n:::\n\n::: {#exm-}\nFind the equation for the tangent plane for the function $f(x, y) = x^2 \\cos(y) - y^2 \\cos(x)$ at the point $(\\frac{\\pi}{2}, \\frac{pi}{4})$\n\n* calculate by hand\n* plot using `plot_tangent_plane()`\n    \n:::\n\n\nThis leads to the linearization equation for functions of $n$ variables. \n\n::: {#def-}\n## The Linearization of a Function\n\nLet $f(\\mathbf{x})$ be a differentiable function at the point $\\mathbf{a} = (a_1, a_2, \\ldots, a_n)'$ for a function of inputs $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)' \\in \\mathcal{R}^n$. Then the linearization of the function $f(\\mathbf{x})$ at the point $\\mathbf{a}$ is given by the equation\n\n\n\n$$\n\\begin{aligned}\nL(\\mathbf{x}) & = f(\\mathbf{a}) + \\nabla f(\\mathbf{x}) |_{\\mathbf{a}} \\cdot (\\mathbf{x} - \\mathbf{a}) \\\\\n& = f(\\mathbf{a}) + \\left( \\nabla f(\\mathbf{x}) |_{\\mathbf{a}} \\right)' (\\mathbf{x} - \\mathbf{a})\n\\end{aligned}\n$$\n\n\n\n\n:::\n\nThe quality of the linearization is high for points \"close\" to $\\mathbf{a}$ and has higher error (defined as $\\|L(\\mathbf{x}) - f(\\mathbf{x})\\|$) as $\\mathbf{x}$ gets further from $\\mathbf{a}$.\n\nFor points $\\mathbf{x}$ \"close\" to the point $\\mathbf{a}$, the exact difference in the function $z = f(\\mathbf{x})$ is given by $\\Delta z =  f(\\mathbf{x}) - f(\\mathbf{a})$. Plugging in the linear approximation, the differential $d z = L(\\mathbf{x}) - f(\\mathbf{a})$ is the linear approximation to the exact different $\\Delta z$. Define $d \\mathbf{x} = \\begin{pmatrix} d x_1 \\\\ d x_2 \\\\ \\vdots \\\\ d x_n \\end{pmatrix} = \\begin{pmatrix} x_1 - a_1 \\\\ x_2 - a_2 \\\\ \\vdots \\\\ x_n - a_n \\end{pmatrix}$ as the set of changes in each of the $n$ coordinates with respect to the standard basis $\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}$ so that the linear change $d z$ of $f(\\mathbf{x})$ at $\\mathbf{a}$ is given by \n\n\n\n\n$$\n\\begin{aligned}\nd z & =  \\nabla f(\\mathbf{x}) |_{\\mathbf{a}} \\cdot (\\mathbf{x} - \\mathbf{a}) \\\\\n& = \\nabla f(\\mathbf{x}) |_{\\mathbf{a}} \\cdot d \\mathbf{x} \\\\\n& = \\sum_{i=1}^n \\frac{\\partial f(\\mathbf{x})}{\\partial x_i} d x_i,\n\\end{aligned}\n$$\n\n\n\nwhere the last term is a sum of the linear approximation in each of the $i = 1, \\ldots, n$ coordinate directions.\n\n::: {#exm-}\nApproximate the linear change of the function $f(x, y, z) = x^2 - 3xy^2z^2 - 4z^2$ at the point $(a, b, c) = (1, -2, -1)$ evaluated at the point $(x, y, z) = (0.95, -2.05, -1.05)$. Compare this to the exact value of the function $f(x, y, z)$\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nFirst, we find the gradient \n\n\n\n\n$$\n\\begin{aligned}\n\\nabla f(x, y, z) = \\begin{pmatrix} 2x - 3y^2z^2 \\\\ -6xyz^2 \\\\ -6xy^2z - 8z \\end{pmatrix}\n\\end{aligned}\n$$\n\n\n\n\nand evaluate the gradient at $(a, b, c) = (1, -2, -1)$ to get \n\n\n\n\n$$\n\\begin{aligned}\n\\nabla f(1, -2, -1) = \\begin{pmatrix} 2(1) - 3(-2)^2(-1)^2 \\\\ -6(1)(-2)(-1)^2 \\\\ -6(1)(-2)^2(-1) - 8(-1) \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 12 \\\\ 32 \\end{pmatrix} \n\\end{aligned}\n$$\n\n\n\n\nThe function evaluated at the point $(a, b, c)$ is $f(1, -2, -1) = (1)^2 - 3(1)(-2)^2(-1)^2 - 4(-1)^2 = -15$.\n\nTherefore, the linear approximation $L(x, y, z)$ of $f(x, y, z)$ at the point $(a, b, c)$ is \n\n\n\n\n$$\n\\begin{aligned}\nL(x, y, z) & = f(a, b, c) + \\nabla f(a, b, c) \\cdot \\begin{pmatrix} x - 1 \\\\ y - (-2) \\\\ z - (-1) \\end{pmatrix} \\\\\n& = -15 + 10(x - 1) - 12 (y + 2) - 32 (z + 1)\n\\end{aligned}\n$$\n\n\n\n\nThe linear approximation evaluated at the point $(0.95, -2.05, -1.05)$ is \n\n\n\n$$\n\\begin{aligned}\nL(0.95, -2.05, -1.05) & = -15 + 10(0.95 - 1) - 12 (-2.05 + 2) - 32 (-1.05 + 1) = -16.7.\n\\end{aligned}\n$$\n\n\n\n\nCompared to the true value of the function $f(x, y, z)$ is \n\n\n\n\n$$\n\\begin{aligned}\nf(0.95, -2.05, -1.05) & = (1.05)^2 - 3(1.05)(-2.05)^2(-1.05)^2 - 4(-1.05)^2 = -16.71228,\n\\end{aligned}\n$$\n\n\n\nwhich gives an approximation error of $f(x, y, z) - L(x, y, z) = -16.7122803 - -16.7 = -0.0122803$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget_fun <- function(x, y, z) {\n        x^2 - 3 * x * y^2 * z^2 - 4 * z^2\n} \n        \ngrad_fun <- function(x, y, z) {\n        c(2*x - 3 * y^2 * z^2,\n          -6 * x * y * z^2,\n          -6 * x * y^2 * z - 8 * z)\n}\n\ngrad_fun(1, -2, -1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -10  12  32\n```\n:::\n\n```{.r .cell-code}\ntarget_fun(0.95, -2.05, -1.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -16.71228\n```\n:::\n\n```{.r .cell-code}\nlinearization <- function(target_fun, grad_fun, x, y, z, a, b, c) {\n        target_fun(a, b, c) + sum(grad_fun(a, b, c) * c(x - a, y - b, z - c))\n}\n\n\nlinearization(target_fun, grad_fun, 0.95, -2.05, -1.05, 1, -2, -1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -16.7\n```\n:::\n\n```{.r .cell-code}\ntarget_fun(0.95, -2.05, -1.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -16.71228\n```\n:::\n\n```{.r .cell-code}\n# approximation error\ntarget_fun(0.95, -2.05, -1.05) - linearization(target_fun, grad_fun, 0.95, -2.05, -1.05, 1, -2, -1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.01228031\n```\n:::\n:::\n\n\n\n\n:::\n\n\n\n\n",
    "supporting": [
      "27-tangents_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": {},
    "postProcess": true
  }
}