{
  "hash": "cbb99380cc31a77bc0774acea3ba49e9",
  "result": {
    "markdown": "# Inner product, length, and orthogonality\n\n- [3 Blue 1 Brown -- The dot prodcut](https://www.3blue1brown.com/lessons/dot-products)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(dasc2594)\nset.seed(2021)\n```\n:::\n\n\n\n\n\n::: {#def-}\nLet $\\mathbf{u}$ and $\\mathbf{v}$ be vectors in $\\mathcal{R}^n$. Then, the **inner product** of $\\mathbf{u}$ and $\\mathbf{v}$ is $\\mathbf{u}' \\mathbf{v}$. The vectors $\\mathbf{u}$ and $\\mathbf{v}$ are $n \\times 1$ matrices where $\\mathbf{u}'$ is a $1 \\times n$ matrix and the inner product $\\mathbf{u}' \\mathbf{v}$ is a scalar ($1 \\times 1$ matrix). The inner product is also sometimes called the dot product and written as $\\mathbf{u} \\cdot \\mathbf{v}$. \n\nIf the vectors\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{u} = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{pmatrix} & & \\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix} \n\\end{aligned}\n$$\n\n\n\nthen $\\mathbf{u}' \\mathbf{v} = u_1 v_1 + u_2 v_2 + \\cdots u_n v_n$\n:::\n\n::: {#exm-}\nFind the inner product $\\mathbf{u}'\\mathbf{v}$ and $\\mathbf{v}'\\mathbf{u}$ of\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{u} = \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} & & \\mathbf{v} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 3 \\end{pmatrix} \n\\end{aligned}\n$$\n\n\n\n\n* do by hand\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu <- c(2, -3, 1)\nv <- c(4, -2, 3)\n# u'v\nsum(u*v)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17\n```\n:::\n\n```{.r .cell-code}\nt(u) %*% v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]   17\n```\n:::\n\n```{.r .cell-code}\n# v'u\nsum(v*u)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17\n```\n:::\n\n```{.r .cell-code}\nt(v) %*% u\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]   17\n```\n:::\n:::\n\n\n\n:::\n\nThe properties of inner products are defined with the following theorem.\n\n:::{#thm-}\nLet $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{w}$ be vectors in $\\mathcal{R}^n$ and let $c$ be a scalar. Then\n\na) $\\mathbf{u}'\\mathbf{v} = \\mathbf{v}'\\mathbf{u}$\n\nb) $(\\mathbf{u} + \\mathbf{v})' \\mathbf{w} = \\mathbf{u}' \\mathbf{w} + \\mathbf{v}' \\mathbf{w}$\n\nc) $( c \\mathbf{u} )' \\mathbf{v} = c ( \\mathbf{v}'\\mathbf{u} )$\n\nd) $\\mathbf{u}'\\mathbf{u} \\geq 0$ with $\\mathbf{u}'\\mathbf{u} = 0$ only when $\\mathbf{u} = \\mathbf{0}$\n\n:::\n\n\nBased on the theorem above, the inner product of a vector with itself ($\\mathbf{u}'\\mathbf{u}$) is strictly non-negative. Thus, we can define the length of the vector $\\mathbf{u}$ (also called the **norm** of the vector $\\mathbf{u}$).\n\n\n::: {#def-}\nThe length of a vector $\\mathbf{v} \\in \\mathcal{R}^n$, also called the vector **norm** $\\| \\mathbf{v} \\|$ is defined as\n\n\n\n$$\n\\begin{aligned}\n\\| \\mathbf{v} \\| & = \\sqrt{\\mathbf{v}'\\mathbf{v}} = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}\n\\end{aligned}\n$$\n\n\n\n:::\n\n::: {#exm-}\nLet $\\mathbf{v} = \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\in \\mathcal{R}^2$. Show that the definition of the norm satisfies the Pythagorean theorem.\n:::\n\n\nAnother property of the norm is how the norm changes based on scalar multiplication. Let $\\mathbf{v} \\in \\mathcal{R}^n$ be a vector and let $c$ be a scalar. Then $\\|c \\mathbf{v}\\| = |c|\\|\\mathbf{v}\\|$ \n\n::: {#def-}\nA vector $\\mathbf{v} \\in \\mathcal{R}^n$ whose length/norm is 1 is called a **unit** vector. Any vector can be made into a unit vector through **normalization** by multiplying the vector $\\mathbf{v}$ by $\\frac{1}{\\|\\mathbf{v}\\|}$ to get a unit vector $\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}$ in the same direction as $\\mathbf{v}$.\n:::\n\n\n## Distance\n\nIn two dimensions, the Euclidean distance between the points $(x_1, y_1)$ and $(x_2, y_2)$ is defined as $\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$. In higher dimensions, a similar definition holds.\n\n::: {#def-}\nLet $\\mathbf{u}$ and $\\mathbf{v}$ be vectors in $\\mathcal{R}^n$. Then the distance $dist(\\mathbf{u}, \\mathbf{v})$ between $\\mathbf{u}$ and $\\mathbf{v}$ is \n\n\n\n\n$$\n\\begin{aligned}\ndist(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|\n\\end{aligned}\n$$\n\n\n\n:::\n\n::: {#exm-}\nDistance between two 3-dimensional vectors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu <- c(3, -5, 1)\nv <- c(4, 3, -2)\nsqrt(sum((u-v)^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.602325\n```\n:::\n:::\n\n\n\n:::\n\n## Orthogonal vectors\n\nThe equivalent of perpendicular lines in $\\mathcal{R}^n$ are known as orthogonal vectors.\n<!-- Geometrically, two vectors are defined as orthogonal if the distance between $\\mathbf{u}$ and $\\mathbf{v}$ is the same as the distance between $\\mathbf{u}$ and $-\\mathbf{v}$. **Draw picture -- perpendicular triangle** -->\n<!-- $$\\begin{aligned} -->\n<!-- dist(\\mathbf{u}, - \\mathbf{v}) & =  -->\n<!-- \\end{aligned}$$ -->\n\n::: {#def-}\nThe two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathcal{R}^n$ are orthogonal if \n\n\n\n$$\n\\begin{aligned}\n\\mathbf{u}' \\mathbf{v} = 0\n\\end{aligned}\n$$\n\n\n\n:::\n\n\n## Angles between vectors\n\nLet $\\mathbf{u}$ and $\\mathbf{v}$ be vectors $\\mathcal{R}^n$. Then, the angle between the vectors $\\mathbf{u}$ and $\\mathbf{v}$ is defined as the angle $\\theta$ in the relationship\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{u}' \\mathbf{v} = \\| \\mathbf{u} \\| \\| \\mathbf{v} \\| cos(\\theta)\n\\end{aligned}\n$$\n\n\n\nSolving for the angle $\\theta$ results in the equation\n\n\n\n$$\n\\begin{aligned}\n\\theta = arccos \\left( \\frac{\\mathbf{u}' \\mathbf{v}}{\\| \\mathbf{u} \\| \\| \\mathbf{v} \\|} \\right)\n\\end{aligned}\n$$\n\n\n\nwhere $arccos(\\cdot)$ is inverse cosine function, which is `acos()` in `R`.\n\n\n**see example: `angles-as-n-gets-large.R`**\n\n::: {#exm-}\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nLet $\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 4 \\\\ 6 \\end{pmatrix}$ and $\\mathbf{v} = \\begin{pmatrix} -5 \\\\ 2 \\\\ 4 \\end{pmatrix}$. What is the angle between these two vectors?\n:::\n\n\n::: {.callout-note icon=false collapse=\"true\" appearance=\"simple\"} \n## Solution\n\nThe angle between the vectors $\\mathbf{u}$ and $\\mathbf{v}$ depends on the dot product between the two vectors and the norms (lengths) of the two vectors. The inner product of $\\mathbf{u}$ and $\\mathbf{v}$ is\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{u}'\\mathbf{v} = \\begin{pmatrix} 1 & 4 & 6 \\end{pmatrix} \\begin{pmatrix} -5 \\\\ 2 \\\\ 4 \\end{pmatrix} = 1*-5 +4*2 + 6*4 = 27\n\\end{aligned}\n$$\n\n\n\nwhere the vector $\\mathbf{u}$ has length\n\n\n\n$$\n\\|\\mathbf{u}\\| = \\sqrt{u_1^2 + u_2^2 + u_3^2} = \\sqrt{1^2 + 4^2 + 6^2} = \\sqrt{53} = 7.2801099\n$$\n\n\n\nand the vector $\\mathbf{v}$ has length\n\n\n\n$$\n\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + v_3^2} = \\sqrt{-5^2 + 2^2 + 4^2} = \\sqrt{45} = 6.7082039\n$$\n\n\n\nPlugging these into the equation for the angle $\\theta$ gives\n\n\n\n$$\n\\begin{aligned}\n\\theta = arccos \\left( \\frac{\\mathbf{u}' \\mathbf{v}}{\\| \\mathbf{u} \\| \\| \\mathbf{v} \\|} \\right) \\\\\n= arccos \\left( \\frac{27}{ 7.2801099 * 6.7082039} \\right) \\\\\n= 0.984997\n\\end{aligned}\n$$\n\n\n\nwhich gives an angle of $\\theta$ = 0.984997 radians between the vector $\\mathbf{u}$ and $\\mathbf{v}$. In degrees, this angle is $\\theta$ = 0.984997 * $\\frac{180}{\\pi}$ = 56.4361716 degrees.\n\nIn `R`, this angle can be found by finding the dot product of $\\mathbf{u}$ and $\\mathbf{v}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu <- c(1, 4, 6)\nv <- c(-5, 2, 4)\nsum(u * v) # dot product of u and v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27\n```\n:::\n:::\n\n\n\nas well as the lengths of these two vectors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(sum(u^2)) # length of u\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.28011\n```\n:::\n\n```{.r .cell-code}\nsqrt(sum(v^2)) # length of v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.708204\n```\n:::\n:::\n\n\n\nCombining these, the angle $\\theta$ can be calculate in radians as\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- acos(sum(u * v) / (sqrt(sum(u^2)) * sqrt(sum(v^2))))\ntheta\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.984997\n```\n:::\n:::\n\n\n\nand in degrees this is \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta * 180 / pi\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 56.43617\n```\n:::\n:::\n\n\n\n\n<!-- # cosine angle -->\n<!-- u <- c(1, 1) -->\n<!-- v <- c(-1, 2) -->\n<!-- sum(u * v) -->\n<!-- # euclidean norm -->\n<!-- norm(u, type = '2')  -->\n<!-- sqrt(sum(u^2)) -->\n<!-- norm(v, type = '2')  -->\n<!-- sqrt(sum(v^2)) -->\n\n<!-- theta <- acos(sum(u * v) / (norm(u, \"2\") * norm(v, \"2\"))) -->\n<!-- theta -->\n<!-- theta / (pi / 2) -->\n<!-- ``` -->\n:::\n\n\n\n\n\n\n## Orthogonal sets \n\nThe set of vectors $\\mathcal{S} = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$ in $\\mathcal{R}^n$ is said to be an **orthogonal set** if every pair of vectors is orthogonal. In other words, for all $i \\neq j$, $\\mathbf{v}_i' \\mathbf{v}_j = 0$. The set is called an **orthonormal set** if the set of vectors are orthogonal and for $i = 1, \\ldots, p$, each vector $\\mathbf{v}_i$ in the set has length $\\| \\mathbf{v}_i \\| = 1$.\n\n\n\n::: {#exm-}\nShow the set of vectors $\\left\\{ \\mathbf{v}_1 = \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\mathbf{v}_2 = \\begin{pmatrix} -\\frac{1}{2} \\\\ -2 \\\\ \\frac{7}{2} \\end{pmatrix}, \\mathbf{v}_3 = \\begin{pmatrix} -1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\right\\}$ is orthogonal\n\n* Show these are orthogonal using `R`\n:::\n\n\nIf the set of vectors $\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$ are an orthogonal set, then the set of vectors $\\left\\{ \\frac{\\mathbf{v}_1}{\\|\\mathbf{v}_1\\|}, \\ldots, \\frac{\\mathbf{v}_p}{\\|\\mathbf{v}_p\\|} \\right\\}$ is an orthonormal set. Note that for each $i$, the length of the vector $\\frac{\\mathbf{v}_i} {\\|\\mathbf{v}_i \\|} = 1$\n\n:::{#thm-}\nLet the set $\\mathcal{S} = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$ be an orthogonal set of nonzero vectors in $\\mathcal{R}^n$. Then, the set of vectors in $\\mathcal{S}$ are linearly independent and therefore are a basis for the space spanned by $\\mathcal{S}$.\n:::\n\n::: {.callout-tip icon=false collapse=\"true\" appearance=\"simple\"} \n## Proof\n\nAssume the set of vectors $\\mathbf{v}_1, \\ldots, \\mathbf{v}_p$ are linearly dependent. Then, there exist coefficients $c_1, \\ldots, c_p$ such that\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{0} & = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \n\\end{aligned}\n$$\n\n\n\nThen, multiplying both equations on the left by $\\mathbf{v}_1'$ gives\n\n\n\n$$\n\\begin{aligned}\n0 = \\mathbf{v}_1' \\mathbf{0} & = \\mathbf{v}_1' (c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p) \\\\\n& = c_1 \\mathbf{v}_1' \\mathbf{v}_1 + c_2  \\mathbf{v}_1' \\mathbf{v}_2 + \\cdots + c_p  \\mathbf{v}_1' \\mathbf{v}_p \\\\\n& = c_1 \\mathbf{v}_1' \\mathbf{v}_1 + c_2  0 + \\cdots + c_p 0 \\\\\n& = c_1 \\mathbf{v}_1' \\mathbf{v}_1\n\\end{aligned}\n$$\n\n\n\nwhich is only equal to 0 when $c_1$ is equal to 0 because $\\mathbf{v}_1$ is a nonzero vector. The above left multiplication could be repeated for each vector $\\mathbf{v}_i$ which gives all $c_i$ must equal 0. As the only solution to the starting equation has all 0 coefficients, the set of vectors $\\mathcal{S}$ must be linearly independent.\n:::\n\nA set of orthogonal vectors is called an **orthogonal basis**.\n\n\n:::{#thm-}\nLet $\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$ be an orthogonal basis of the subspace $\\mathcal{W}$ of $\\mathcal{R}^n$. Then for each $\\mathbf{x} \\in \\mathcal{W}$, the coefficients for the linear combination of basis vectors $\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$ for the vector $\\mathbf{x}$ are\n\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{x} & = \\frac{\\mathbf{x}'\\mathbf{v}_1}{\\mathbf{v}_1'\\mathbf{v}_1} \\mathbf{v}_1 + \\frac{\\mathbf{x}'\\mathbf{v}_2}{\\mathbf{v}_2'\\mathbf{v}_2} \\mathbf{v}_2 + \\cdots +  \\frac{\\mathbf{x}'\\mathbf{v}_p}{\\mathbf{v}_p'\\mathbf{v}_p} \\mathbf{v}_p \\\\\n& = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\\\\n\\end{aligned}\n$$\n\n\n\nwhere $c_j = \\frac{\\mathbf{x}'\\mathbf{v}_j}{\\mathbf{v}_j'\\mathbf{v}_j}$. In other words, the coordinates of the vector $\\mathbf{x}$ with respect to the orthogonal basis $\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$  are the linear projection of the vector $\\mathbf{x}$ on the respective vectors $\\mathbf{v}_j$.\n:::\n\n\n::: {.callout-tip icon=false collapse=\"true\" appearance=\"simple\"} \n## Proof\n\nThe orthogonality of the basis $\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$ gives\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{x}'\\mathbf{v}_j & = \\left(c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\right)' \\mathbf{v}_j \\\\\n& = c_j \\mathbf{v}_j' \\mathbf{v}_j\n\\end{aligned}\n$$\n\n\n\nBecause we know that $\\mathbf{v}_j'\\mathbf{v}_j$ is not zero (a vector can't be orthogonal to itself), we can divide the above equality by $\\mathbf{v}_j' \\mathbf{v}_j$ and solve for $c_j = \\frac{\\mathbf{x}'\\mathbf{v}_j}{\\mathbf{v}_j'\\mathbf{v}_j}$\n:::\n\n\nThus, for a vector $\\mathbf{x}$ in the standard basis, the coordinates of $\\mathbf{x}$ with respect to an orthogonal basis can be easily calculated using dot products (rather than matrix inverses) which is an easier computation.\n\nIn fact, this is exactly the idea of using **least squares** estimation (linear regression, spline regression, etc.).\n\n\n## Orthogonal projections\n\n::: {#def-}\nLet $\\mathbf{x}$ be a vector in $\\mathcal{R}^n$ and let $\\mathcal{W}$ be a subspace of $\\mathcal{R}^n$. Then the vector $\\mathbf{x}$ can be written as the **orthogonal decomposition** \n\n\n\n$$\n\\begin{aligned}\n\\mathbf{x} = \\mathbf{x}_{\\mathcal{W}} + \\mathbf{x}_{\\mathcal{W}^\\perp}\n\\end{aligned}\n$$\n\n\n\nwhere $\\mathbf{x}_{\\mathcal{W}}$ is the vector in $\\mathcal{W}$ that is closest to $\\mathbf{x}$ and is called the **orthogonal projection of $\\mathbf{x}$ onto $\\mathcal{W}$** and $\\mathbf{x}_{\\mathcal{W}^\\perp}$ is the orthogonal projection of $\\mathbf{x}$ onto $\\mathcal{W}^{\\perp}$, the subspace $\\mathcal{W}^\\perp$ of $\\mathcal{R}^n$ that is complementary to $\\mathcal{W}$ and is called the **orthogonal complement**.\n:::\n\n**Draw picture in class - W is a plane, orthogonal projection of a vector onto the plane**\n\nThis leads to the projection theorem that decomposes a vector $\\mathbf{x} \\in \\mathcal{R}^n$ into components that are \n\n:::{#thm-}\nLet $\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$ be an orthogonal basis of the subspace $\\mathcal{W}$ of $\\mathcal{R}^n$. Then for each $\\mathbf{x} \\in \\mathcal{R}^n$, the orthogonal projection of $\\mathbf{x}$ onto $\\mathcal{W}$ is given by\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{x}_{\\mathcal{W}} & = \\frac{\\mathbf{x}'\\mathbf{v}_1}{\\mathbf{v}_1'\\mathbf{v}_1} \\mathbf{v}_1 + \\frac{\\mathbf{x}'\\mathbf{v}_2}{\\mathbf{v}_2'\\mathbf{v}_2} \\mathbf{v}_2 + \\cdots +  \\frac{\\mathbf{x}'\\mathbf{v}_p}{\\mathbf{v}_p'\\mathbf{v}_p} \\mathbf{v}_p \\\\\n& = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\\\\n\\end{aligned}\n$$\n\n\n\nwhere the coefficient $c_j$ corresponding to the vector $\\mathbf{v}_j$ of the linear combination of vectors $\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$ is given by $c_j = \\frac{\\mathbf{x}'\\mathbf{v}_j}{\\mathbf{v}_j'\\mathbf{v}_j}$. In other words, the coordinates of the vector $\\mathbf{x}$ with respect to the orthogonal basis $\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}$  are the linear projection of the vector $\\mathbf{x}$ on the respective vectors $\\mathbf{v}_j$.\n:::\n\n\nYou might be wondering what use orthogonal projections are. In fact, linear regression (and most common regression models) use orthogonal projections to fit a line (or surface) of best fit. This leads to the important theorem that allows us to project a vector $\\mathbf{y} \\in \\mathcal{R}^n$ onto the column space of a $n \\times p$ matrix $\\mathbf{X}$ (which is exactly the linear regression of $\\mathbf{y}$ onto $\\mathbf{X}$).\n\n:::{#thm-orthogonalmatrixprojection}\n## Orthogonal Projection Theorem\n\nLet $\\mathbf{X}$ be a $n \\times p$ matrix, let $\\mathcal{W} =$ col($\\mathbf{X}$), and let $\\mathbf{y}$ be a vector in $\\mathcal{R}^n$. Then the matrix equation\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{X}'\\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}' \\mathbf{y}\n\\end{aligned}\n$$\n\n\n\nwith respect to the unknown coefficients $\\boldsymbol{\\beta}$ is consistent and $\\mathbf{y}_{\\mathcal{W}} = \\mathbf{X}\\boldsymbol{\\beta}$ for any solution $\\boldsymbol{\\beta}$.\n:::\n\n\n\n::: {.callout-tip icon=false collapse=\"true\" appearance=\"simple\"} \n## Proof\n\nShow this in class\n:::\n\n\nIn addition, if the columns of $\\mathbf{X}$ are linearly independent, then the coefficients $\\boldsymbol{\\beta}$ are given by \n\n\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\beta} = \\left( \\mathbf{X}'\\mathbf{X} \\right)^{-1} \\mathbf{X}' \\mathbf{y}\n\\end{aligned}\n$$\n\n\n\nwhich is the least squares solution to the linear regression problem. For example, let `X` and `y` be defined as below\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- cbind(1, c(2, -1, 3, -4, 5, 7, -2, 3))\ny <- c(5, 3, 4, -9, 11, 12, -5, 6)\n```\n:::\n\n\n\nPlotting this data shows the strong positive linear relationship\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The first column is a basis for a constant term (the intercept)\ndata.frame(x = X[, 2], y = y) %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](22-orthogonal_files/figure-epub/example-least-squares-2-1.png)\n:::\n:::\n\n\n\nWe can solve for the coefficients `beta` using the linear project theorem\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta <- solve(t(X) %*% X) %*% t(X) %*% y\n```\n:::\n\n\n\nand using this solution, solve for the projection $\\mathbf{y}_{\\mathcal{W}}$ of $\\mathbf{y}$ onto $\\mathbf{X}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_W <- X %*% beta\n```\n:::\n\n\n\nPlotting the projection $\\mathbf{y}_{\\mathcal{W}}$ gives\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The first column is a basis for a constant term (the intercept)\ndata.frame(x = X[, 2], y = y, y_W = y_W) %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x, y = y_W))\n```\n\n::: {.cell-output-display}\n![](22-orthogonal_files/figure-epub/example-least-squares-5-1.png)\n:::\n:::\n\n\n\nThe complement of the project (called the **residuals** in statistics) is given by $\\mathbf{y}_{\\mathcal{W}^\\perp} = \\mathbf{y} - \\mathbf{y}_{\\mathcal{W}}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_W_perp <- y - y_W\n```\n:::\n\n\n\nand can be visualized as the orthogonal projection using segments\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The first column is a basis for a constant term (the intercept)\ndata.frame(x = X[, 2], y = y, y_W = y_W) %>% \n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x, y = y_W)) +\n    geom_segment(aes(x = x, y = y_W, xend = x, yend = y_W + y_W_perp), color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](22-orthogonal_files/figure-epub/example-least-squares-7-1.png)\n:::\n:::\n\n\n\nRecall that the orthogonal projection gives the \"closest\" vector $\\mathbf{y}_W$ to $\\mathbf{y}$ that is in the subspace $\\mathcal{W}$ that is the span of the column space of $\\mathbf{X}$. See [https://www.enchufa2.es/archives/least-squares-as-springs-the-shiny-app.html](https://www.enchufa2.es/archives/least-squares-as-springs-the-shiny-app.html) for an example.\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "22-orthogonal_files/figure-epub"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": {},
    "postProcess": true
  }
}