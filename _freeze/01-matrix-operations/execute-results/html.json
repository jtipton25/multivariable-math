{
  "hash": "f72ed9e4fcdd4dcfe8ff7e8383f51fcd",
  "result": {
    "markdown": "# Matrix operations {#matrix-operations}\n\n## ToDo \n\n- [3 Blue 1 Brown -- Matrix Multiplication](https://www.3blue1brown.com/lessons/matrix-multiplication)\n\n* **Note: add examples:**\n\n## The dot/inner product\n\n\n::: {#def-}\nLet $\\mathbf{u}$ and $\\mathbf{v}$ be vectors in $\\mathcal{R}^n$. Then, the **dot product** (also called the **inner product**) of $\\mathbf{u}$ and $\\mathbf{v}$ is $\\mathbf{u}' \\mathbf{v}$. The vectors $\\mathbf{u}$ and $\\mathbf{v}$ are $n \\times 1$ matrices ($n$ rows and one column) where $\\mathbf{u}'$ is a $1 \\times n$ matrix and the inner product $\\mathbf{u}' \\mathbf{v}$ is a scalar ($1 \\times 1$ matrix). The inner product is also sometimes called the dot product and written as $\\mathbf{u} \\cdot \\mathbf{v}$. \n\nIf the vectors\n\n$$\n\\begin{aligned}\n\\mathbf{u} = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{pmatrix} & & \\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix} \n\\end{aligned}\n$$\n\nthen $\\mathbf{u}' \\mathbf{v} = u_1 v_1 + u_2 v_2 + \\cdots u_n v_n$\n:::\n\n::: {#exm-}\nFind the inner product $\\mathbf{u}'\\mathbf{v}$ and $\\mathbf{v}'\\mathbf{u}$ of\n\n$$\n\\begin{aligned}\n\\mathbf{u} = \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} & & \\mathbf{v} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 3 \\end{pmatrix} \n\\end{aligned}\n$$\n\n\n* do by hand\n\n::: {.cell}\n\n```{.r .cell-code}\nu <- c(2, -3, 1)\nv <- c(4, -2, 3)\n# u'v\nsum(u*v)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17\n```\n:::\n\n```{.r .cell-code}\nt(u) %*% v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]   17\n```\n:::\n\n```{.r .cell-code}\n# v'u\nsum(v*u)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17\n```\n:::\n\n```{.r .cell-code}\nt(v) %*% u\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]   17\n```\n:::\n:::\n\n:::\n\nThe properties of inner products are defined with the following theorem.\n\n::: {#thm-innerprod}\n## Inner Product\nLet $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{w}$ be vectors in $\\mathcal{R}^n$ and let $c$ be a scalar. Then\n\na) $\\mathbf{u}'\\mathbf{v} = \\mathbf{v}'\\mathbf{u}$\n\nb) $(\\mathbf{u} + \\mathbf{v})' \\mathbf{w} = \\mathbf{u}' \\mathbf{w} + \\mathbf{v}' \\mathbf{w}$\n\nc) $( c \\mathbf{u} )' \\mathbf{v} = c ( \\mathbf{v}'\\mathbf{u} )$\n\nd) $\\mathbf{u}'\\mathbf{u} \\geq 0$ with $\\mathbf{u}'\\mathbf{u} = 0$ only when $\\mathbf{u} = \\mathbf{0}$\n\n:::\n\n\n## Properties of matrices\n\n### Matrix Addition\n\n**Matrix Addition:** If the matrices $\\mathbf{A}$ and $\\mathbf{B}$ are of the same dimension (e.g., both $\\mathbf{A}$ and $\\mathbf{B}$ have the same number of rows $m$ and the same number of columns $n$), then\n\n\n$$\n\\begin{aligned}\n\\mathbf{A} + \\mathbf{B} & = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{pmatrix} + \n\\begin{pmatrix} b_{11} & b_{12} & \\cdots & b_{1n} \\\\\nb_{21} & b_{22} & \\cdots & b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{m1} & b_{m2} & \\cdots & b_{mn}\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix} a_{11} + b_{11} & b_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn}\n\\end{pmatrix} \\\\\n& = \\left\\{ a_{ij} + b_{ij} \\right\\}\n\\end{aligned}\n$${#eq-matrixaddition}\n\n\n**... Another way to **\n\nIf $\\mathbf{A}$ and $\\mathbf{B}$ are of the same dimension (same number of rows and columns) you can add the matrices together\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 1, 33, 2, 0, -4), 3, 2)\nB <- matrix(c(7, -24, 3, 9, 11, -9), 3, 2)\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    1    0\n[3,]   33   -4\n```\n:::\n\n```{.r .cell-code}\nB\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    7    9\n[2,]  -24   11\n[3,]    3   -9\n```\n:::\n\n```{.r .cell-code}\nA + B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]   11   11\n[2,]  -23   11\n[3,]   36  -13\n```\n:::\n:::\n\n\n\nWe can also write this using `for` loops\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# initialize an empty matrix to fill\nC <- matrix(0, 3, 2)\n\nfor (i in 1:nrow(A)) {         # loop over the rows\n    for (j in 1:ncol(A)) {     # loop over the columns\n        C[i, j] <- A[i, j] + B[i, j]\n    }\n}\nC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]   11   11\n[2,]  -23   11\n[3,]   36  -13\n```\n:::\n:::\n\n\nIf $\\mathbf{A}$ and $\\mathbf{B}$ are of different dimensions (they differ in either the number of rows or columns), `R` will return an error warning you that the matrices are of different sizes and can't be added\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 1, 33, 2, 0, -4), 3, 2)\nB <- matrix(c(7, -24, 3, 9), 2, 2)\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    4    2\n[2,]    1    0\n[3,]   33   -4\n```\n:::\n\n```{.r .cell-code}\nB\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    7    3\n[2,]  -24    9\n```\n:::\n\n```{.r .cell-code}\nA + B\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in A + B: non-conformable arrays\n```\n:::\n:::\n\n\n\n::: {#thm-matrixproperties}\nLet $\\mathbf{A}$, $\\mathbf{B}$, and $\\mathbf{C}$ be $m \\times n$ matrices and let $a$ and $b$ be scalars, then:\n\n1) $\\mathbf{A} + \\mathbf{B}  = \\mathbf{B} + \\mathbf{A}$ \\hfill [(commutivity of addition)]{style=\"float:right\"}\n\n2)  $(\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})$ \\hfill [(commutivity of addition)]{style=\"float:right\"}\n\n3) $\\mathbf{A} + \\mathbf{0}  = \\mathbf{A}$  \\hfill [(additive identity)]{style=\"float:right\"}\n\n4) $a (\\mathbf{A} + \\mathbf{B}) = a \\mathbf{A} + a \\mathbf{B}$ \\hfill [(scalar ...)]{style=\"float:right\"}\n\n5) $(a + b)\\mathbf{A} = a \\mathbf{A} + b \\mathbf{A}$ \\hfill [(scalar ...)]{style=\"float:right\"} \n\n6) $(ab)\\mathbf{A} = a (b\\mathbf{A})$ \\hfill [(scalar ...)]{style=\"float:right\"} \n:::\n\n### Matrix Multipliation\n\n\n**Matrix Multiplication:** If $\\mathbf{A} = \\left\\{ a_{ij} \\right\\}$ is an $m \\times n$ matrix and $\\mathbf{B} = \\left\\{ a_{jk} \\right\\}$ is a $n \\times p$ matrix, then the matrix product $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$ is an $m \\times p$ matrix where $\\mathbf{C} = \\left\\{ \\sum_{j=1}^p a_{ij} b{jk} \\right\\}$\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{A} \\mathbf{B} & = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{pmatrix} \n\\begin{pmatrix} b_{11} & b_{12} & \\cdots & b_{1p} \\\\\nb_{21} & b_{22} & \\cdots & b_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{np}\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix} \\sum_{j=1}^n a_{1j} b_{j1} & \\sum_{j=1}^n a_{1j} b_{j2} & \\cdots & \\sum_{j=1}^n a_{1j} b_{jp} \\\\\n\\sum_{j=1}^n a_{2j} b_{j1} &\\sum_{j=1}^n a_{2j} b_{j2} & \\cdots & \\sum_{j=1}^n a_{2j} b_{jp} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sum_{j=1}^n a_{mj} b_{j1} &\\sum_{j=1}^n a_{nj} b_{j2} & \\cdots & \\sum_{j=1}^n a_{mj} b_{jp}\n\\end{pmatrix} \n\\end{aligned}\n$${#eq-matrixmultiplication}\n\n\n\nAnother way to define matrix multiplication is through inner product notation. Define the $m \\times n$ matrix $\\mathbf{A}$ and the $n \\times p$ matrix $\\mathbf{B}$ as the partition\n\n\n$$\n\\begin{aligned}\n\\mathbf{A} & = \\begin{pmatrix} \\mathbf{a}_{1}' \\\\\n\\mathbf{a}_{2}' \\\\ \n\\vdots \\\\\n\\mathbf{a}_{m}' \\end{pmatrix}\n& \\mbox{ and } \n&& \\mathbf{B} & = \\begin{pmatrix} \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{p} \\end{pmatrix} \n\\end{aligned}\n$$\n\n\nwhere $\\mathbf{a}_i$ and $\\mathbf{b}_k$ are both $n$-vectors. Then, we have $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$ can be written as\n\n\n$$\n\\begin{aligned}\n\\mathbf{A} \\mathbf{B}  = \\mathbf{A} \\begin{pmatrix} \\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p\n\\end{pmatrix}  = \\begin{pmatrix} \\mathbf{A} \\mathbf{b}_1 & \\mathbf{A} \\mathbf{b}_2 & \\cdots & \\mathbf{A} \\mathbf{b}_p\n\\end{pmatrix} \n\\end{aligned}\n$$\n\n \nNote that in this representation, each column of the matrix $\\mathbf{A}\\mathbf{B}$ is a linear combination the the columns of $\\mathbf{A}$ with coefficients given by the corresponding column of $\\mathbf{B}$.\n \n\n$$\n\\begin{aligned}\n\\mathbf{A} \\mathbf{B} & = \\begin{pmatrix} \\mathbf{a}_1' \\mathbf{b}_1 & \\mathbf{a}_1' \\mathbf{b}_2 & \\cdots & \\mathbf{a}_1' \\mathbf{b}_q \\\\\n\\mathbf{a}_2' \\mathbf{b}_1 & \\mathbf{a}_2' \\mathbf{b}_2 & \\cdots & \\mathbf{a}_2' \\mathbf{b}_q \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{a}_n' \\mathbf{b}_1 & \\mathbf{a}_n' \\mathbf{b}_2  & \\cdots & \\mathbf{a}_n' \\mathbf{b}_q\n\\end{pmatrix} \\\\\n& = \\left\\{ \\mathbf{a}_i' \\mathbf{b}_k  \\right\\}.\n\\end{aligned}\n$$\n\n \nWritten in this notation, we arrive at the multiplication rule for $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$ -- the $ik$th element $c_{ik}$ of $\\mathbf{C}$ is the inner product of the $i$th row of $\\mathbf{A}$ and the $j$th column of $\\mathbf{B}$.\n\n\n### Properties of Matrix Multiplication\n\nDefine the $m \\times m$ **identity matrix** $\\mathbf{I}_m$ with ones on the diagonal and zeros off diagonal as\n\n$$\n\\mathbf{I}_m = \\begin{pmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ 0 & 0 & \\ddots & 0 \\\\ 0 & 0 & \\cdots & 1\\end{pmatrix}\n$$\n\nLet $\\mathbf{A}$ be an $m \\times n$ matrix, then:\n\n1) Let $\\mathbf{B}$ be an $n \\times p$ matrix and $\\mathbf{C}$ a $p \\times q$ matrix. Then $\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}$ is an $m \\times q$ matrix.\n\n2) Let $\\mathbf{B}$ and $\\mathbf{C}$ be $n \\times p$ matrices. Then $\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}$ is an $m \\times p$ matrix.\n\n3) Let $\\mathbf{B}$ and $\\mathbf{C}$ be $p \\times m$ matrices. Then $(\\mathbf{B} + \\mathbf{C})\\mathbf{A} = \\mathbf{B}\\mathbf{A} + \\mathbf{C}\\mathbf{A}$ is an $p \\times m$ matrix.\n\n4) Let $\\mathbf{B}$ be an $p \\times m$ matrix and $c$ a scalar. Then $c(\\mathbf{A} \\mathbf{B}) = (c \\mathbf{A}) \\mathbf{B} = \\mathbf{A}(c\\mathbf{B})$ is an $p \\times m$ matrix.\n\n5) $\\mathbf{I}_m \\mathbf{A} = \\mathbf{A} \\mathbf{I}_n = \\mathbf{A}$\n\n\n**Examples: in class**\n\n**Note:** Matrix multiplication violates some of the rules of multiplication that you might be used to. Pay attention for the following:\n\n1) In general $\\mathbf{A} \\mathbf{B} \\neq \\mathbf{B} \\mathbf{A}$ (sometimes these are equal, but usually are not)\n\n2) $\\mathbf{A}\\mathbf{B} = \\mathbf{A} \\mathbf{C}$ **does not** imply $\\mathbf{B} = \\mathbf{C}$\n\n3) $\\mathbf{A}\\mathbf{B} = \\mathbf{0}$ does not imply that $\\mathbf{A} = \\mathbf{0}$ or $\\mathbf{B} = \\mathbf{0}$\n\n\n\n### Matrix Multiplication complexity (Big O notation)\n\nIn the study of algorithms, the notation $O(n)$ is used to describe the number of calculations that need to be done to evaluate the equation. As an example, consider $\\mathbf{A} = \\begin{pmatrix}3 & 1 \\\\ 2 & -3 \\end{pmatrix}$, $\\mathbf{B} = \\begin{pmatrix} -2 & 4 \\\\ -1 & 2 \\end{pmatrix}$, and $\\mathbf{x} = \\begin{pmatrix} -3 \\\\ 1 \\end{pmatrix}$. \n\n**By hand:** Calculate \n\n1) $(\\mathbf{A} \\mathbf{B}) \\mathbf{x}$\n\n2) $\\mathbf{A} (\\mathbf{B} \\mathbf{x})$\n\nWhich was easier? Which required less calculation?\n\n* Matrix-matrix multiplication of and $m \\times n$ matrix $\\mathbf{A}$ and an $m \\times p$ matrix $\\mathbf{B}$ has complexity $O(m n p)$.\n\n* Matrix-vector multiplication of and $m \\times n$ matrix $\\mathbf{A}$ and an $p$-vector $\\mathbf{x}$ has complexity $O(n m)$.\n\nFrom example above:\n\n1) $O(m n p)$ matrix-matrix multiplication $(\\mathbf{A} \\mathbf{B})$ followed by $O(m n)$ matrix-vector multiplication $(\\mathbf{A} \\mathbf{B}) \\mathbf{x}$ which has computational complexity $O(m n p) + O(m n)$\n\n2) $O(m n)$ matrix-vector multiplication $(\\mathbf{B} \\mathbf{x})$ followed by $O(m n)$ matrix-vector multiplication $\\mathbf{A} (\\mathbf{B} \\mathbf{x})$ which has computational complexity $O(m n) + O(m n)$\n\n\n### Matrix powers\n\nPowers of a $n \\times n$ (square) matrix are defined as the product of $\\mathbf{A}$ multiplied $k$ times\n\n$$\n\\mathbf{A}^k = \\underbrace{\\mathbf{A} \\cdots \\mathbf{A}}_k\n$$\n\n\n### Matrix Transpose\n\n\nThe matrix transpose is an operator that swaps the rows and columns of a matrix. If $\\mathbf{A}$ is an $m \\times n$ matrix ($m$ rows and $n$ columns), then $\\mathbf{A}'$ is a $n \\times m$ matrix ($n$ rows and $m$ columns. Note: some use $\\mathbf{A}^T$ to denote a transpose; I prefer the $'$ notation as it is much simpler and cleaner notation). The matrix \n\n\n$$\n\\begin{aligned}\n\\mathbf{A} & = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{pmatrix}\n\\end{aligned}\n$$\n\nhas transpose\n\n\n$$\n\\begin{aligned}\n\\mathbf{A}' & = \\begin{pmatrix} a_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn}\n\\end{pmatrix},\n\\end{aligned}\n$$\n\n\n:::{#thm-matrixtransposes}\nLet $\\mathbf{A}$ be an $m \\times n$ matrix, then\n\n1) $(\\mathbf{A}')' = \\mathbf{A}$.\n\n2) Let $\\mathbf{B}$ be an $m \\times n$ matrix, then $(\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}'$.\n\n3) For any scalar $c$, $(c \\mathbf{A})' = c \\mathbf{A}'$.\n\n4) Let $\\mathbf{B}$ be an $n \\times p$ matrix, then $( \\mathbf{A} \\mathbf{B})' = \\mathbf{B}' \\mathbf{A}'$ is an $p \\times m$ matrix.\n\n:::\n\n**Note:** The power of video games: GPUs and modern CPUs are becoming more and more parallelized. Because the $ij$th element of $\\mathbf{A}\\mathbf{B}$ requires only the $i$th row of $\\mathbf{A}$ and the $j$th column of $\\mathbf{B}$, matrix multiplication is easily parallelized under modern computing architectures. Thanks to video games, this parallelization has been made faster than ever.\n\n\n**Examples: in class**\n\n\n\n\n\n\n\n\n<!-- ## Direct Sums -->\n\n<!-- **Direct Sums:** For the $n \\times p$ matrix $\\mathbf{A}$ and the $m \\times q$ matrix $\\mathbf{B}$, the direct sum is -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\mathbf{C} & = \\mathbf{A} \\bigoplus \\mathbf{B} \\\\ -->\n<!-- & = \\begin{pmatrix} \\mathbf{A} & \\mathbf{0}_{n \\times q} \\\\ -->\n<!-- \\mathbf{0}_{m \\times p} & \\mathbf{B} -->\n<!-- \\end{pmatrix} \\\\ -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- where $\\mathbf{0}_{n \\times q}$ and $\\mathbf{0}_{m \\times p}$ is an are $n \\times q$ and $m \\times p$ matrices of all 0s, respectively.  -->\n\n<!-- The direct sum can be generalized to any arbitrary number of matrices as  -->\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\mathbf{C} & = \\bigoplus_{i=1}^K \\mathbf{A}_i \\\\ -->\n<!-- & = \\begin{pmatrix} \\mathbf{A}_1 & \\mathbf{0} & \\cdots & \\mathbf{0} \\\\ -->\n<!-- \\mathbf{0} & \\mathbf{A}_2 &  \\cdots & \\mathbf{0} \\\\ -->\n<!-- \\vdots & \\vdots & \\ddots & \\vdots \\\\ -->\n<!-- \\mathbf{0} & \\mathbf{0} & \\cdots & \\mathbf{A}_K \\\\ -->\n<!-- \\end{pmatrix} \\\\ -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- where the matrices $\\mathbf{0}$ are all zeros of the appropriate dimension. -->\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}