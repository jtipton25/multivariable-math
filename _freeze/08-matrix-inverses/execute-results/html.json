{
  "hash": "d18a3fada55e48218737c53bb72dcd98",
  "result": {
    "markdown": "# Matrix Inverses {#matrix-inverse}\n\n- [3 Blue 1 Brown -- Inverse Matrices, column space, and null space](https://www.3blue1brown.com/lessons/inverse-matrices)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dasc2594)\nlibrary(tidyverse)\n```\n:::\n\n\nFor scalars, the multiplicative identity is \n$$\na \\frac{1}{a} = a a^{-1} = a^{-1} a = 1\n$$\nwhere $a^{-1}$ is the inverse of $a$.\n\n::: {#def-matrixinverse}\n## Matrix Inverse\n\nThe $n \\times n$ **square** matrix $\\mathbf{A}$ is said to be **invertible** if there exists a $n \\times n$ matrix $\\mathbf{C}$( which we call $\\mathbf{A}^{-1}$ once we verify the inverse exists) such that \n$$\n\\begin{aligned}\n\\mathbf{C}\\mathbf{A} = \\mathbf{A} \\mathbf{C} & = \\mathbf{I} \\\\\n\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{-1} & = \\mathbf{I}\n\\end{aligned}\n$$\nwhere $\\mathbf{I}$ is the $n \\times n$ identity matrix (the matrix with 1s on the diagonal and zeros everywhere else).\n:::\n\n\nIn `R`, an identity matrix is easy to construct. An $n \\times n$ identity matrix can be constructed using the `diag()` function\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 4\nI <- diag(n)\nI\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n```\n:::\n:::\n\n\n::: {#exm-}\n\n::: {.cell}\n\n:::\n\n\n\n$$\n\\begin{aligned}\n\\mathbf{A} = \\begin{pmatrix} 1 & -1 \\\\ 2 & -3 \\end{pmatrix} && \\mathbf{B} = \\begin{pmatrix} 3 & -1 \\\\ 2 & -1 \\end{pmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check if B is the inverse of A\nA %*% B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n:::\n\n```{.r .cell-code}\n# check if B is the inverse of A\nB %*% A\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n:::\n:::\n\n\nBecause $\\mathbf{A} \\mathbf{B} = \\mathbf{B} \\mathbf{A} = \\mathbf{I}$, we have $\\mathbf{A}$ is an invertible matrix with inverse $\\mathbf{B} = \\mathbf{A}^{-1}$.\n:::\n\n\n:::{#thm-matrix2by2}\n## Matrix Inverse for 2 by 2 matrix\n\nLet $\\mathbf{A} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. If $ad - bc \\neq 0$ then $\\mathbf{A}$ is invertible and \n$$\n\\begin{aligned}\n\\mathbf{A}^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\n\\end{aligned}\n$$\nIf $ad - bc = 0$, then the matrix is not invertible.\n:::\n\n* **Question:** why is the matrix not invertible when $ad - bc = 0$? \n    * Have you heard of \"singular\" or \"singularity\" before?\n    * Black holes are called singularities. Why is this?\n    * Square matrices that are not invertible are call \"singular\"\n    \n::: {#def-}\nFor the $2 \\times 2$ matrix $\\mathbf{A} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the term $ad - bc$ is called the **determinant** of the matrix $\\mathbf{A}$ and is written as $\\operatorname{det}(\\mathbf{A})$. Sometimes the determinant is written as $| \\mathbf{A}|$\n:::\n\nA consequence of the above theorem is that a $2 \\times 2$ matrix is invertible only if its determinant is nonzero.\n    \n<!-- Example -->\n::: {#exm-}\nDetermine if the following $2 \\times 2$ matrix is invertible\n\n\n::: {.cell}\n\n:::\n\n$\\mathbf{A} = \\begin{pmatrix} 4 & -4 \\\\ -1 & 2 \\end{pmatrix}$\n:::\n\n\n:::{#thm-}\nIf the $n \\times n$ matrix $\\mathbf{A}$ is invertible, then for each $\\mathbf{b} \\in \\mathcal{R}^n$, the matrix equation\n$$\n\\mathbf{A} \\mathbf{x} = \\mathbf{b}\n$$\nhas the unique solution $\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}$. \n:::\n\n::: {.callout-tip icon=false collapse=\"true\" appearance=\"simple\"} \n## Proof\n\nThere are two things to show ...\n\n1) show there is a solution\n\n2) show the solution is unique\n:::\n\n\n::: {#exm-}\n\n\n::: {.cell}\n\n:::\n\nLet $\\mathbf{A} = \\begin{pmatrix} 4 & -4 & -2 \\\\ 5 & 2 & -5 \\\\ -4 & 6 & 1 \\end{pmatrix}$ and $\\mathbf{b} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix}$\n\nFind the solution to $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$\n:::\n\n\n\n:::{#thm-invertiblematrix1}\n## Invertible Matrix Theorem Again\n\nAdding onto the Invertible Matrix Theorem  @thm-invertiblematrix we have\n\n1) If $\\mathbf{A}$ is an invertible matrix, then $\\mathbf{A}^{-1}$ is invertible and $(\\mathbf{A}^{-1})^{-1} = \\mathbf{A}$\n\n2) If $\\mathbf{A}$ and $\\mathbf{B}$ are $n \\times n$ invertible matrices, then $\\mathbf{A} \\mathbf{B}$ is also an invertible matrix whose inverse is \n$$\n(\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\n$$\nwhich is the inverse of the matrices in reverse order.\n\n3) If $\\mathbf{A}$ is an invertible matrix, then the transpose $\\mathbf{A}'$ is also invertible and the inverse of $\\mathbf{A}'$ is the transpose of $\\mathbf{A}^{-1}$. Equivalently,\n$$\n(\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'\n$$\n\n:::\n    \n\n\n::: {.callout-tip icon=false collapse=\"true\" appearance=\"simple\"} \n## Proof\n\nHere we prove the three statements from the theorem above. All three statements rely on the definition of an invertible matrix in Definition \\@ref(def:matrix-inverse)\n\n\n1) If $\\mathbf{A}^{-1}$ is invertible, then, there exists a matrix $\\mathbf{C}$ such that $\\mathbf{C} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{C} = \\mathbf{I}$. Let $\\mathbf{C} = \\mathbf{A}$. Then, we have $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}$ which shows that $\\left(\\mathbf{A}^{-1}\\right)^{-1} = \\mathbf{A}$\n\n2) First, consider multiplying $\\mathbf{A}\\mathbf{B}$ on the left by $\\mathbf{B}^{-1} \\mathbf{A}^{-1}$ where $(\\mathbf{A}\\mathbf{B}) (\\mathbf{B}^{-1} \\mathbf{A}^{-1}) = \\mathbf{A} (\\mathbf{B} \\mathbf{B}^{-1}) \\mathbf{A}^{-1} = \\mathbf{A} \\mathbf{I} \\mathbf{A}^{-1} = \\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$. Then multiply $\\mathbf{A}\\mathbf{B}$ on the right by $\\mathbf{B}^{-1} \\mathbf{A}^{-1}$ where $(\\mathbf{B}^{-1} \\mathbf{A}^{-1}) (\\mathbf{A}\\mathbf{B}) = \\mathbf{B} (\\mathbf{A} \\mathbf{A}^{-1}) \\mathbf{B}^{-1} = \\mathbf{B} \\mathbf{I} \\mathbf{B}^{-1} = \\mathbf{B} \\mathbf{B}^{-1} = \\mathbf{I}$.\n\n\n3) Use the fact that $(\\mathbf{A} \\mathbf{B})' = \\mathbf{B}' \\mathbf{A}'$. Then, $(\\mathbf{A}^{-1})' \\mathbf{A}' = (\\mathbf{A}\\mathbf{A}^{-1})' = \\mathbf{I}' = \\mathbf{I}$. Similarly $\\mathbf{A}'(\\mathbf{A}^{-1})' = (\\mathbf{A}^{-1}\\mathbf{A})' = \\mathbf{I}' = \\mathbf{I}$. Thus $\\mathbf{A}'$ is invertible with inverse $(\\mathbf{A}^{-1})'$\n:::\n\n\n\n* **Note:** A consequence of @thm-invertiblematrix2 (2) is that the product of $k$ invertible $n \\times n$ matrices $\\mathbf{A}_1 \\mathbf{A}_2 \\cdots \\mathbf{A}_k$ has inverse $\\mathbf{A}_k^{-1} \\mathbf{A}_{k-1}^{-1} \\cdots \\mathbf{A}_1^{-1}$\n\n## Elementary matrices\n\n* Elementary matrices are matrices that perform basic row operations (i.e., we can write the reduced row echelon algorithm as a produce of elementary matrices).\n\nRecall the elementary row operations:\n\n1) swaps: swapping two rows. \n2) sums: replacing a row by the sum itself and a multiple of another row.\n3) scalar multiplication: replacing a row by a scalar multiple times itself.\n\n::: {#exm-row-operations}\nConsider the $3 \\times 3$ matrix \n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 5, 9, -2, -4, 1, 4, 6, -2), 3, 3)\n```\n:::\n\n    \n$\\mathbf{A} = \\begin{pmatrix} 4 & -2 & 4 \\\\ 5 & -4 & 6 \\\\ 9 & 1 & -2 \\end{pmatrix}$\n    \n    \n1) What is the elementary matrix (let's call it $\\mathbf{E}_1$ that swaps the first and second rows of $\\mathbf{A}$?\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nE_1 <- matrix(c(0, 1, 0, 1, 0, 0,  0, 0, 1), 3, 3)\n```\n:::\n\n$\\mathbf{E}_1 = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]    5   -4    6\n[3,]    9    1   -2\n```\n:::\n\n```{.r .cell-code}\n## left multiple A by E_1\nE_1 %*% A\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    5   -4    6\n[2,]    4   -2    4\n[3,]    9    1   -2\n```\n:::\n:::\n\n\nThus, the matrix $\\mathbf{E}_1 = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$ is the matrix that swaps the first and second row.\n\n2) What is the elementary matrix (let's call it $\\mathbf{E}_2$ that adds two times the first of $\\mathbf{A}$ to the third row of $\\mathbf{A}$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nE_2 <- matrix(c(1, 0, 2, 0, 1, 0, 0, 0, 1), 3, 3)\n```\n:::\n\n$\\mathbf{E}_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 1 \\end{pmatrix}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]    5   -4    6\n[3,]    9    1   -2\n```\n:::\n\n```{.r .cell-code}\n## left multiple A by E_2\nE_2 %*% A\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]    5   -4    6\n[3,]   17   -3    6\n```\n:::\n:::\n\n\nThus, the matrix $\\mathbf{E}_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 1 \\end{pmatrix}$ is the matrix that adds two times the first of $\\mathbf{A}$ to the third row of $\\mathbf{A}$\n\n3) What is the elementary matrix (let's call it $\\mathbf{E}_3$ that mutliples the second row of $\\mathbf{A}$ by 3?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nE_3 <- matrix(c(1, 0, 0, 0, 3, 0, 0, 0, 1), 3, 3)\n```\n:::\n\n$\\mathbf{E}_3 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]    5   -4    6\n[3,]    9    1   -2\n```\n:::\n\n```{.r .cell-code}\n## left multiple A by E_3\nE_3 %*% A\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    4   -2    4\n[2,]   15  -12   18\n[3,]    9    1   -2\n```\n:::\n:::\n\n\nThus, the matrix $\\mathbf{E}_3 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$ is the matrix that  multiples the second row of $\\mathbf{A}$ by 3.\n\n:::\n    \n* **Question:** Do you see any patterns with how the example elementary matrices look?\n\n$$\n\\begin{aligned}\n\\mathbf{E_1} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} && \\mathbf{E_2} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 1 \\end{pmatrix} && \\mathbf{E_3} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n\\end{aligned}\n$$    \n    \n* The elementary matrices look like the identity matrix $\\mathbf{I}$ with an elementary row operation applied to $\\mathbf{I}$. In fact, this leads us to this general fact:\n\n**Fact:** If an elementary row matrix is applied to the $m \\times n$ matrix $\\mathbf{A}$, the result of this elementary row operation applied to $\\mathbf{A}$ can be written as $\\mathbf{E} \\mathbf{A}$ where $\\mathbf{E}$ is the $m \\times m$ identity matrix $\\mathbf{I}$ with the respective elementary row operation applied to $\\mathbf{I}$.\n\n**Fact:** Each elementary matrix $\\mathbf{E}$ is invertible\n\n:::{#exm-}\n**In class**\n:::\n\nThe next theorem is quite important as the result gives an algorithm for calculating the inverse of a $n \\times n$ matrix $\\mathbf{A}$ which also makes it possible to solve matrix equations $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$\n\n:::{#thm-}\nIf an $n \\times n$ matrix $\\mathbf{A}$ is invertible, then $\\mathbf{A}$ is row-equivalent to $\\mathbf{I}$ ($\\mathbf{A} \\sim \\mathbf{I}$; row-equivalent means $\\mathbf{A}$ can be reduced to $\\mathbf{I}$ using elementary row operations). The row-equivalency implies that there is a series of elementary row operations (e.g., elementary matrices $\\mathbf{E}_1, \\ldots, \\mathbf{E}_k$) that converts $\\mathbf{A}$ to $\\mathbf{I}$. In addition, the application of these row matrices to $\\mathbf{I}$ transforms $\\mathbf{I}$ to the matrix inverse $\\mathbf{A}^{-1}$.\n:::\n\n* **Proof: in class**\n\n## Finding the inverse of $\\mathbf{A}$\n\nThe previous theorem states that for a $n \\times n$ invertible matrix $\\mathbf{A}$, the elementary row operations that covert $\\mathbf{A}$ to $\\mathbf{I}$ also convert $\\mathbf{I}$ to $\\mathbf{A}^{-1}$. This suggests an algorithm for finding the inverse $\\mathbf{A}^{-1}$ of $\\mathbf{A}$:\n\nCreate the augmented matrix $\\begin{pmatrix} \\mathbf{A} & \\mathbf{I} \\end{pmatrix}$ and row reduce the augmented matrix. If the row-reduced augmented matrix is of the form $\\begin{pmatrix} \\mathbf{I} & \\mathbf{A}^{-1} \\end{pmatrix}$ then $\\mathbf{A}^{-1}$ is the inverse of $\\mathbf{A}$. If the leading matrix in the augmented matrix is not the identity matrix $\\mathbf{I}$, then $\\mathbf{A}$ is not row equivalent to $\\mathbf{I}$ and is therefore not invertible.\n\n::: {#exm-}\n\n\n\nLet $\\mathbf{A} = \\begin{pmatrix} -3 & -3 & -4 \\\\ -4 & 2 & -4 \\\\ 4 & -4 & 4 \\end{pmatrix}$. Does $\\mathbf{A}$ have an inverse, and if so, what is it?\n\nUsing `R`\n:::\n\n\n\n## The Invertible Matrix Theorem\n\n\n:::{#thm-invertiblematrix}\n## The Invertible Matrix Theorem\nLet $\\mathbf{A}$ be an $n \\times n$ matrix. Then the following statements are equivalent (i.e., they are all either simultaneously true or false).\n\n1) $\\mathbf{A}$ is an invertible matrix.\n\n2) $\\mathbf{A}$ is row equivalent to the $n \\times n$ identity matrix $\\mathbf{I}$ ($\\mathbf{A} \\sim \\mathbf{I}$).\n\n3) $\\mathbf{A}$ and $n$ pivot columns.\n\n4) The homogeneous matrix equation $\\mathbf{A} \\mathbf{x} = \\mathbf{0}$ has only the trivial solution $\\mathbf{x} = \\mathbf{0}$.\n\n5) The columns of $\\mathbf{A}$ are linearly independent.\n\n6) The linear transformation $T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n$ given by the matrix transformation $\\mathbf{x} \\rightarrow \\mathbf{A}\\mathbf{x}$ is one-to-one.\n\n7) The inhomogeneous matrix equation $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ has a unique solution for all $\\mathbf{b} \\in \\mathcal{R}^n$.\n\n8) The columns of $\\mathbf{A}$ span $\\mathcal{R}^n$.\n\n9) The linear transformation $\\mathbf{x} \\rightarrow \\mathbf{A} \\mathbf{x}$ maps $\\mathcal{R}^n$ onto $\\mathcal{R}^n$.\n\n10) There is an $n \\times n$ matrix $\\mathbf{C}$ such that $\\mathbf{C}\\mathbf{A} = \\mathbf{I}$.\n\n11) There is an $n \\times n$ matrix $\\mathbf{D}$ such that $\\mathbf{A}\\mathbf{D} = \\mathbf{I}$.\n\n12) $\\mathbf{A}'$ is an invertible matrix.\n:::\n\n::: {.callout-tip icon=false collapse=\"true\" appearance=\"simple\"} \n## Proof\n\n**In class**\n:::\n\n\n\n\nA result of the invertible matrix theorem is that if $\\mathbf{A}$ and $\\mathbf{B}$ are $n \\times n$ matrices with $\\mathbf{A} \\mathbf{B} = \\mathbf{I}$ then $\\mathbf{A} = \\mathbf{B}^{-1}$ and $\\mathbf{B} = \\mathbf{A}^{-1}$.\n\n\n## Invertible Linear Transformations\n\n::: {#def-}\nA linear transformation $T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n$ is said to be invertible if there exists a transformation $S:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n$ such that\n\n$$\n\\begin{aligned}\nS(T(\\mathbf{x})) = \\mathbf{x} && \\mbox{for all } \\mathbf{x} \\in \\mathcal{R}^n\nT(S(\\mathbf{x})) = \\mathbf{x} && \\mbox{for all } \\mathbf{x} \\in \\mathcal{R}^n \\\\\n\\end{aligned}\n$$\n:::\n\n* **Draw figure in class**\n\n:::{#thm-}\nLet $T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n$ be a linear transformation and let $\\mathbf{A}$ be the matrix representing the transformation $T$. Then the transformation $T$ is invertible if and only if the matrix $\\mathbf{A}$ is invertible. Therefore, the matrix that represents $S:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n$, the inverse transformation of $T$, is unique and is represented by the matrix $\\mathbf{A}^{-1}$.\n:::\n\n",
    "supporting": [
      "08-matrix-inverses_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}