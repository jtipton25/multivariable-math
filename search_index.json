[["index.html", "Multivariable Mathematics for Data Science Chapter 1 Preface 1.1 Outline", " Multivariable Mathematics for Data Science John Tipton 2021-01-06 Chapter 1 Preface This book will introduce students to multivariable Calculus and linear algebra methods and techniques to be successful in data science, statistics, computer science, and other data-driven, computational disciplines. The motiviation for this text is to provide both a theoretical understanding of important multivariable methods used in data science as well as giving a hands-on experience using software. Throughout this text, we assume the reader has a solid foundation in univariate calculus (typically two semesters) as well as familiarity with a scripting language (e.g., R or python). 1.1 Outline Note: This is going to be revised and reformated throughout the class Introduction to vectors and matrices and vector and matrix operations vector spaces and subspaces dot products, cross products, projections linear combinations, linear independence, bases, coordinate systems planes, surfaces, and lines in space linear transformations, matrix arithmetic, and matrix rank solving linear equations \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) innner products, outer products, and norms projections - orthogonal projections and least squares matrix decompositoins: Eigen, Cholesky, principal components, singular value decomposition, pre-whitening matrix decompositoins: Eigen, Cholesky, principal components, singular value decomposition, pre-whitening limits, continuity, and partial derivatives chain rule, directional derivatives tangent planes, linear approximations, Taylor Series double/triple integrals, change of variables, Jacobian transformation double/triple integrals, change of variables, Jacobian transformation "],["section-linear-systems-of-equations.html", "Chapter 2 Linear Systems of Equations 2.1 Liner Systems of equations 2.2 Reduce row echelon form", " Chapter 2 Linear Systems of Equations library(tidyverse) # For 3-d plotting # if devtools package not installed, install the package if (!require(devtools)) { install.packages(&quot;devtools&quot;) } # if gg3D package not installed, install the package if (!require(gg3D)) { devtools::install_github(&quot;AckerDWM/gg3D&quot;) library(gg3D) } ## Warning in fun(libname, pkgname): no display name and no $DISPLAY environment ## variable # if dasc2594 package not installed, install the package if (!require(dasc2594)) { devtools::install_github(&quot;jtipton25/dasc2594&quot;) library(dasc2594) } 2.1 Liner Systems of equations 2.1.1 Linear equations Let \\(x_1, x_2, \\ldots, x_n\\) be variables with coefficients \\(a_1, a_2, \\ldots, a_n\\), and \\(b\\) are fixed and known numbers. Then, we say \\[ \\begin{align} \\label{eq:linear} a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n &amp; = b \\end{align} \\] is a linear equation. For example, the equation for a line with slope \\(m\\) and \\(y\\)-intercept \\(b\\) is \\[ \\begin{align*} y &amp; = m x + b, \\end{align*} \\] is a linear equation because it can be re-written as \\[ \\begin{align*} y - m x &amp; = b, \\end{align*} \\] where \\(a_1 = 1\\), \\(a_2 = m\\), \\(x_1 = y\\) and \\(x_2 = x\\). The equations \\[ \\begin{align*} \\sqrt{19} x_1 &amp; = (4 + \\sqrt{2}) x_2 - x_3 - 9 &amp; \\mbox{ and } &amp;&amp; -4 x_1 + 5 x_2 - 11 &amp; = x_3 \\end{align*} \\] are both linear equations because they can be written as \\[ \\begin{align*} \\sqrt{19} x_1 - (4 + \\sqrt{2}) x_2 + x_3 &amp; = - 9 &amp; \\mbox{ and } &amp;&amp; -4 x_1 + 5 x_2 - x_3 &amp; = 11, \\end{align*} \\] respectively. The equations \\[ \\begin{align*} x_1 &amp; = x_2^2 + 3 &amp; \\mbox{ and } &amp;&amp; x_1 + x_2 - x_1 x_2 &amp; = 16 \\end{align*} \\] are not linear equations because they do not meet the form of (The first equation above has a quadratic power of \\(x_2\\) and the second equation has a product of \\(x_1\\) and \\(x_2\\)). 2.1.2 Systems of linear equations A set of two or more linear equations that each contain the same set of variables is called a system of linear equations. The equations \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; &amp;&amp; &amp;&amp; = &amp; 9 \\end{align*} \\] are a system of equations. Note that in the second equation, the coefficient for \\(x_3\\) is 0, meaning we could re-write the above example as \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; + &amp;&amp; 0 x_3 &amp;&amp; = &amp; 9. \\end{align*} \\] Exercises: For the following, are these linear equations? \\(x_1 + 3 x_1 x_2 = 5\\) \\(5x + 7y + 8z = 11.2\\) \\(y / 4 + \\sqrt{2} z = 2^6\\) \\(x + 4 y^2 = 9\\) 2.1.3 Solutions of linear systems A fundamental question when presented with a linear system of equations is whether the system has a solution. A solution to a system means that there are numbers \\((s_1, s_2, \\ldots, s_n)\\) that each of the variables \\(x_1, x_2, \\ldots, x_n\\) take that allow for all the equations to simultaneously be true. For example, consider the system of equations \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 5 y &amp;&amp; = &amp; 7 \\end{align*} \\] To find if a solution to this equation exists, we can do some algebra and take 4 times the top equation and then subtract the bottom equation, replacing the bottom equation with this new sum like \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x - 4 * (x) &amp;&amp; + &amp;&amp; 5 y - 4 * (4y) &amp;&amp; = &amp; 7 - 4 * (8) \\end{align*} \\] where the part of the equations in () is the top equation. This system of equations now simplifies to \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 0 &amp;&amp; + &amp;&amp; -11 y &amp;&amp; = &amp; -25 \\end{align*} \\] which gives \\(y = \\frac{25}{11}\\). Plugging this value into the top equation gives \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 * \\frac{25}{11} &amp;&amp; = &amp; 8 \\\\ 0 &amp;&amp; + &amp;&amp; y &amp;&amp; = &amp; \\frac{25}{11} \\end{align*} \\] where we can solve \\(x = 8 - \\frac{100}{11} = -\\frac{12}{11}\\) giving the solution of the form \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 0 &amp;&amp; = &amp; - \\frac{12}{11} \\\\ 0 &amp;&amp; + &amp;&amp; y &amp;&amp; = &amp; \\frac{25}{11}. \\end{align*} \\] In this case, the system of equation has the solution \\(x = -\\frac{12}{11}\\) and \\(y = \\frac{25}{11}\\). While finding the solution can be done algebraically, what does this mean visually (geometrically)? The original equations were \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 5 y &amp;&amp; = &amp; 7 \\end{align*} \\] which define two lines: \\(y = -\\frac{x}{4} + 2\\) \\(y = -\\frac{4x}{5} + \\frac{7}{5}\\) Let’s plot these equations in R and see what they look like # define some grid points to evaluate the line x &lt;- seq(-2, 2, length = 1000) dat &lt;- data.frame( x = c(x, x), y = c(-x / 4 + 2, - 4 / 5 * x + 7/5), equation = factor(rep(c(1, 2), each = 1000)) ) glimpse(dat) ## Rows: 2,000 ## Columns: 3 ## $ x [3m[90m&lt;dbl&gt;[39m[23m -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.97… ## $ y [3m[90m&lt;dbl&gt;[39m[23m 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, … ## $ equation [3m[90m&lt;fct&gt;[39m[23m 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … dat %&gt;% ggplot(aes(x = x, y = y, color = equation, group = equation)) + geom_line() + scale_color_viridis_d(end = 0.8) + # solution x = -12/11, y = 25/11 geom_point(aes(x = -12/11, y = 25/11), color = &quot;red&quot;, size = 2) + ggtitle(&quot;Linear system of equations&quot;) Figure 2.1: Linear system of equations with one solution From this plot, it is clear that the solution to the system of equations is the location where the two lines intersect! 2.1.4 Types of solutions Typically, there are 3 cases for the solutions to a system of linear equations There are no solutions There is one solution (Figure 2.1) There are infinitely many solutions Definition: A linear system of equations is called consistent if the system has either one or infinitely many solutions and is called inconsistent if the system has no solution. There are no solutions: Consider the system of linear equations \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 16 y &amp;&amp; = &amp; 18 \\end{align*} \\] # define some grid points to evaluate the line x &lt;- seq(-2, 2, length = 1000) dat &lt;- data.frame( x = c(x, x), y = c(-x / 4 + 8 / 4, - x / 4 + 18 / 4), equation = factor(rep(c(1, 2), each = 1000)) ) glimpse(dat) ## Rows: 2,000 ## Columns: 3 ## $ x [3m[90m&lt;dbl&gt;[39m[23m -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.97… ## $ y [3m[90m&lt;dbl&gt;[39m[23m 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, … ## $ equation [3m[90m&lt;fct&gt;[39m[23m 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … dat %&gt;% ggplot(aes(x = x, y = y, color = equation, group = equation)) + geom_line() + scale_color_viridis_d(end = 0.8) + # solution x = -12/11, y = 25/11 ggtitle(&quot;Linear system of equations&quot;) Figure 2.2: Linear system of equations with no solution In this case, the linear equations are parallel lines and will never intersect so therefore there is no solution. There is one solution: We have seen this example in Figure 2.1 There are infinitely many solutions: Consider the system of linear equations \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 16 y &amp;&amp; = &amp; 32 \\end{align*} \\] # define some grid points to evaluate the line x &lt;- seq(-2, 2, length = 1000) dat &lt;- data.frame( x = c(x, x), y = c(-x / 4 + 8 / 4, - 4 * x / 16 + 32 / 16), equation = factor(rep(c(1, 2), each = 1000)) ) glimpse(dat) ## Rows: 2,000 ## Columns: 3 ## $ x [3m[90m&lt;dbl&gt;[39m[23m -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.97… ## $ y [3m[90m&lt;dbl&gt;[39m[23m 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, … ## $ equation [3m[90m&lt;fct&gt;[39m[23m 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … dat %&gt;% ggplot(aes(x = x, y = y, color = equation, group = equation)) + geom_line() + scale_color_viridis_d(end = 0.8) + # solution x = -12/11, y = 25/11 ggtitle(&quot;Linear system of equations&quot;) Figure 2.3: Linear system of equations with no solution In this case, the linear equations are perfectly overlapping lines and always intersect so therefore there are infinitely many solutions (all points on the line). Definition: Two linear systems of equations are called equivalent if both systems share the same solution set. For example, the system of equations \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; + &amp;&amp; 2 x_3 &amp;&amp; = &amp; 9. \\end{align*} \\] and the system of equations \\[ \\begin{align*} 2x_1 &amp;&amp; + &amp;&amp; 8 x_2 &amp;&amp; - &amp;&amp; 2 x_3 &amp;&amp; = &amp; 22 \\\\ 8 x_1 &amp;&amp; + &amp;&amp; 10 x_2 &amp;&amp; + &amp;&amp; 4 x_3 &amp;&amp; = &amp; 18. \\end{align*} \\] have the same solution set (the second set of equations is just 2 times the first set of equations). Exercise/Lab: generate some equations, plot them, and determine if there is a solution. Then try to solve these using algebra. Exercises: for the following systems of equations, determine if a solution(s) exist and if so, solve for the solution \\[\\begin{align*} 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; = 8\\\\ 9 x_1 &amp;&amp; - &amp;&amp; 3 x_2 &amp;&amp; = 4 \\end{align*}\\] \\[\\begin{align*} 7 x_1 &amp;&amp; + &amp;&amp; 3 x_2 &amp;&amp; + &amp;&amp; 4 x_3 &amp;&amp; = 5\\\\ 4 x_1 &amp;&amp; - &amp;&amp; 5 x_2 &amp;&amp; &amp;&amp; &amp;&amp; = -2 \\end{align*}\\] \\[\\begin{align*} 4 x_1 &amp;&amp; - &amp;&amp; 2 x_2 &amp;&amp; = 8\\\\ 2 x_1 &amp;&amp; + &amp;&amp; x_2 &amp;&amp; = 7 \\\\ -3 x_1 &amp;&amp; + &amp;&amp; 6 x_2 &amp;&amp; = 11 \\end{align*}\\] 2.1.5 Elementary row and column operations on matrices The elementary row (column) operations include swaps: swapping two rows (columns), sums: replacing a row (column) by the sum itself and a multiple of another row (column) scalar multiplication: replacing a row (column) by a scalar multiple times itself Note that these operations are exactly what we used to solve the equation using algebra above (except for swapping rows). Add in examples in class here 2.1.6 The Augmented matrix form of a system of equations Consider the linear system of equations \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; + &amp;&amp; 2 x_3 &amp;&amp; = &amp; 9. \\end{align*} \\] The augmented matrix representation of this system of linear equations is given by the matrix \\[ \\begin{align*} \\begin{pmatrix} 1 &amp; 4 &amp; - 1 &amp; 11 \\\\ 4 &amp; 5 &amp; 2 &amp; 9 \\end{pmatrix}, \\end{align*} \\] where the first column of the matrix represents the variable \\(x_1\\), the second column of the matrix represents the variable \\(x_2\\), the third column of the matrix represents the variable \\(x_3\\), and the fourth column of the matrix represents the constant terms. We can express the augmented form in R using a matrix augmented_matrix &lt;- matrix(c(1, 4, 4, 5, -1, 2, 11, 9), 2, 4) augmented_matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 4 -1 11 ## [2,] 4 5 2 9 and to make clear the respective variables, we can add in column names as a matrix attribute using the colnames() function colnames(augmented_matrix) &lt;- c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;constants&quot;) augmented_matrix ## x1 x2 x3 constants ## [1,] 1 4 -1 11 ## [2,] 4 5 2 9 which adds labels to each of the columns. Now, using elementary row operations on the matrix, we can attempt to find solutions to the system of equations. First, we multiply the first row by -4 and add it to the second row of the matrix and replace the second row with this sum augmented_matrix[2, ] &lt;- -4 * augmented_matrix[1, ] + augmented_matrix[2, ] augmented_matrix ## x1 x2 x3 constants ## [1,] 1 4 -1 11 ## [2,] 0 -11 6 -35 Next, scale the second row to have a leading value of 1 by dividing by -11 augmented_matrix[2, ] &lt;- augmented_matrix[2, ] / (-11) augmented_matrix ## x1 x2 x3 constants ## [1,] 1 4 -1.0000000 11.000000 ## [2,] 0 1 -0.5454545 3.181818 We can then multiply the second row by -4 and add it to the first row and replace the first row with this value. augmented_matrix[1, ] &lt;- augmented_matrix[1, ] - 4 * augmented_matrix[2, ] augmented_matrix ## x1 x2 x3 constants ## [1,] 1 0 1.1818182 -1.727273 ## [2,] 0 1 -0.5454545 3.181818 Notice how the matrix has a “triangular” form (The lower part of the “triangle” is made of 0s and the upper part has numbers). The triangular form tells us that There are infinitely many solutions to this system of equation. The infinite solutions are subject to the requirements that \\[x_1 = - \\frac{19}{11} - \\frac{13}{11} x_3\\] and \\[x_2 = \\frac{35}{11} + \\frac{6}{11} x_3.\\] To get this into a reasonable form, we will solve these equations as a function of \\(x_1\\). Solving the first equation for \\(x_3\\) gives \\[x_3 = - \\frac{19}{13} -\\frac{11}{13} x_1.\\] Then, plugging this into \\(x_3\\) in the second equation gives \\[ \\begin{align*} x_2 &amp; = \\frac{35}{11} + \\frac{6}{11} \\left( - \\frac{19}{13} -\\frac{11}{13} x_1 \\right) \\\\ &amp; = \\frac{341}{143} - \\frac{6}{13} x_1 \\end{align*} \\] which defines a linear relationship between \\(x_1\\) and \\(x_2\\). Notice that in these last two solutions, \\(x_1\\) is a “free variable” and \\(x_2\\) and \\(x_3\\) are “determined” by \\(x_1\\). In the plot below, the two planes (red and blue) are the geometric plots of the linear equations in the system of equations (the red plane is the top equation and the blue plane is the bottom equation). The purple line is the equation for the solution given the free variable \\(x_3\\) and lies at the intersection of the two planes, much like the point in the two lines in figure linking reference here lies at the intersection of the two points. # uses gg3D library n &lt;- 60 x1 &lt;- x2 &lt;- seq(-10, 10, length = n) region &lt;- expand.grid(x1 = x1, x2 = x2) df &lt;- data.frame( x1 = region$x1, x2 = region$x2, x3 = - 11 + (region$x1 + 4 * region$x2) ) df2 &lt;- data.frame( x1 = region$x1, x2 = region$x2, x3 = (9 - 4 * region$x1 - 5 * region$x2) / 2 ) df_solution &lt;- data.frame( x1 = x1, x2 = 341 / 143 - 6 / 13 * x1, x3 = -19/13 - 11/13 * x1 ) # theta and phi set up the &quot;perspective/viewing angle&quot; of the 3D plot theta &lt;- 63 phi &lt;- -12 ggplot(df, aes(x1, x2, z = x3)) + axes_3D(theta = theta, phi = phi) + stat_wireframe(alpha = 0.25, color = &quot;red&quot;, theta = theta, phi = phi) + stat_wireframe(data = df2, aes(x = x1, y = x2, z = x3), alpha = 0.25, color = &quot;blue&quot;, theta = theta, phi = phi) + stat_3D(data = df_solution, aes(x1, x2, z = x3), geom = &quot;line&quot;, theta = theta, phi = phi, color = &quot;purple&quot;) + theme_void() + theme(legend.position = &quot;none&quot;) + labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), angle=c(0, 0, 90), theta = theta, phi = phi) ## Warning: Removed 2 row(s) containing missing values (geom_path). ## Warning: Removed 2 row(s) containing missing values (geom_path). 2.1.7 Existence and Uniqueness Definition: A system of linear equations is said to be consistent if at least one solution exists. The linear system of equations is said to have a unique solution if only one solution exists. Example: (do in class) Is the system of linear equations consistent? IF the system is consistent, does it have a unique solution? \\[ \\begin{align*} 16 x_1 &amp;&amp; + &amp;&amp; 2 x_2 &amp;&amp; + &amp;&amp; 3 x_3 &amp;&amp; = &amp; 13 \\\\ 5 x_1 &amp;&amp; + &amp;&amp; 11 x_2 &amp;&amp; + &amp;&amp; 10 x_3 &amp;&amp; = &amp; 8 \\\\ 9 x_1 &amp;&amp; + &amp;&amp; 7 x_2 &amp;&amp; + &amp;&amp; 6 x_3 &amp;&amp; = &amp; 12 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 14 x_2 &amp;&amp; + &amp;&amp; 15 x_3 &amp;&amp; = &amp; 1 \\\\ \\end{align*} \\] Example: (do in class) Is the system of linear equations consistent? IF the system is consistent, does it have a unique solution? \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 2 x_2 &amp;&amp; + &amp;&amp; 3 x_3 = &amp; 5 \\\\ x_1 &amp;&amp; + &amp;&amp; 3 x_2 &amp;&amp; + &amp;&amp; 2 x_3= &amp; 2 \\\\ 3 x_1 &amp;&amp; + &amp;&amp; 2 x_2 &amp;&amp; + &amp;&amp; x_3 = &amp; 7 \\end{align*} \\] 2.2 Reduce row echelon form Reducing a matrix to row echelon form is a useful technique for working with matrices. The row echelon form can be used to solve systems of equations, as well as determine other properties of a matrix that are yet to be discussed, including rank, invertibility, column/row spaces, etc. Definition: A matrix is said to be in echelon form if 1) all nonzero rows are above any rows of zeros (all rows consisting entirely of zeros are at the bottom) 2) the leading entry/coefficient of a nonzero row (called the pivot) is always strictly to the right of the leading entry/coefficient of the row above Example: echelon matrix example in class Definition: A matrix is in reduced row echelon form if it is in echelon form and 1) the leading entry/coefficient of each row is 1 2) The leading entry/coefficient of 1 is the only nonzero entry in its column. Example: rref matrix example in class Definition: Echelon matrices have the property of being upper diagonal. A matrix is said to be upper diagonal if all entries of the matrix at or above the diagonal are nonzero. Example: **lower and non-lower diagonal matrices Definition: Two matrices are row-equivalent if one matrix can be transformed to the other through elementary row operations. Theorem: A nonzero matrix can be transformed into more than one echelon forms. However, the reduced row echelon form of a nonzero matrix is unique. Exercise: using elementary row operations, calculate the reduced row echelon form of the following matrices fill in later fill in later fill in later 2.2.1 Pivot positions The leading entry/coefficients of a row echelon form matrix are called pivots. The positions of the pivot positions are the same for any row echelon form of a matrix. In reduced row echelon form, these pivot positions take the value 1. Definition: In a matrix that is in reduced echelon form, the pivot position is the first nonzero element of each row. The column in which the pivot position occurs is called a pivot column. Example: pivot position and pivot columns 2.2.2 Finding the reduced row echelon form Calculating the reduced row echelon form is known as Gaussian elimination, which is named after Johann Carl Friedrich Gauss. This algorithm uses elementary row operations to calculate the reduced row echelon form. The following steps perform the Gaussian elimination algorithm. Start with the left-most nonzero column, which is a pivot column If the top row is zero, swap rows so that the top row is nonzero so that the top row has a nonzero element in the pivot position. Use row multiplication and addition to zero out all positions in the pivot column below the top row (pivot position). Ignore this top row and repeat steps 1-3 until there are no more nonzero rows to apply steps 1-3 on. At the end of this step, the matrix is in row echelon form. Starting at the right-most pivot column, use elementary row operations to zero out all positions above each pivot and to make each pivot position 1. At the end of this step, the matrix is in reduced row echelon form. Example: in class # pracma library # rref example in class 2.2.3 Using reduced row echelon forms to solve systems of linear equations When a system of linear equations is expressed as an augmented matrix, the reduced row echelon form can be used to find solutions to those systems of equations. Consider the systems of equations \\[ \\begin{align*} 3x_1 &amp;&amp; + &amp;&amp; 8 x_2 &amp;&amp; - &amp;&amp; 4 x_3 &amp;&amp; = &amp; 6 \\\\ 2 x_1 &amp;&amp; - &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 8 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; &amp;&amp; &amp;&amp; = &amp; 9 \\end{align*} \\] which can be written in the augmented matrix form \\[ \\begin{pmatrix} 3 &amp; 8 &amp; -4 &amp; 6 \\\\ 2 &amp; -4 &amp; -1 &amp; 8 \\\\ 4 &amp; 5 &amp; 0 &amp; 9 \\end{pmatrix} \\] In R, this is the matrix # define matrix Calculating the reduced row echelon form, gives # calculate rref of augmented matrix which gives the solution … Exercise: calculate the RREF for the augmented matrix in the example above by hand Another example where we find a solution is \\[ \\begin{align*} 5 x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; 2 x_3 &amp;&amp; = &amp; 0 \\\\ -3 x_1 &amp;&amp; - &amp;&amp; 2 x_2 &amp;&amp; - &amp;&amp; 4 x_3 &amp;&amp; = &amp; 1 \\\\ \\end{align*} \\] Do same steps Definition: In a system of linear equations that is underdetermined (fewer equations than unknowns), the determined/basic variables are those variable that have a 1 in the respective columns when in reduced row echelon form (i.e., variables in a pivot position). The variables that are not in a pivot position are called free variable. Example in class 2.2.4 Existence and uniquenss from reduced row echelon form The row echelon form is useful to determine if a system of linear equations is consistent (the system of equations has a solution). To check if a solution to a linear system of equations exists, convert the system of equations to an augmented matrix form. Then, reduce the augmented matrix to row echelon form using elementary matrix operations. As long as there is not an equation of the form \\[ 0 = \\mbox{constant} \\] for some constant number not equal to 0, the system of linear equations is consistent. If the augmented matrix can be written in reduced row echelon form with no free variables, the solution to the linear system of equations is unique. These results give rise to the theorem Theorem: A linear system of equations is consistent (has a solution) if the furthest right column (the constant column) is not a pivot column. If the system of equations is consistent, (i.e., the furthest right column is not a pivot column), the solution is unique if there are no free variables and there are infinitely many solutions if there is at least one free variable. Example: consistent system of equations \\[\\begin{pmatrix} -7 &amp; -9 &amp; 7 &amp; 8 \\\\ -4 &amp; 0 &amp; 6 &amp; -6 \\\\ -10 &amp; 3 &amp; -8 &amp; 5 \\end{pmatrix}\\] Example: inconsistent system of equations \\[\\begin{pmatrix} -7 &amp; 0 &amp; -8 &amp; -5 \\\\ -4 &amp; 3 &amp; 8 &amp; -2 \\\\ -10 &amp; 7 &amp; -6 &amp; 4 \\\\ -9 &amp; 6 &amp; 5 &amp; 1 \\end{pmatrix}\\] "],["section-vectors-and-matrices.html", "Chapter 3 Vectors and matrices 3.1 Exercises", " Chapter 3 Vectors and matrices This can produce an error, so we wrap the function array_to_latex in a call to cat() A &lt;- matrix(c(3,4,5,6,7,9,4,5,122), ncol=3, byrow=TRUE) array_to_latex(A) [1] “\\begin{pmatrix} 3 &amp; 4 &amp; 5 \\\\ 6 &amp; 7 &amp; 9 \\\\ 4 &amp; 5 &amp; 122 \\end{pmatrix}” A &lt;- matrix(c(3,4,5,6,7,9,4,5,122), ncol=3, byrow=TRUE) cat(array_to_latex(A)) \\[\\begin{pmatrix} 3 &amp; 4 &amp; 5 \\\\ 6 &amp; 7 &amp; 9 \\\\ 4 &amp; 5 &amp; 122 \\end{pmatrix}\\] The fundamental objects in this text are scalars, vectors, and matrices. Transpose Transivity: The transpose operator is transitive \\[ \\begin{align*} (\\mathbf{x} + \\mathbf{y})&#39; &amp; = \\mathbf{x}&#39; + \\mathbf{y}&#39; \\end{align*} \\] x &lt;- c(4, 3, 5) y &lt;- c(-4, 8, 7) t(x + y) ## [,1] [,2] [,3] ## [1,] 0 11 12 t(x) + t(y) ## [,1] [,2] [,3] ## [1,] 0 11 12 In addition, the transpose of a transpose is the identity operator \\(\\left( \\mathbf{x}&#39; \\right)&#39; = \\mathbf{x}\\). x ## [1] 4 3 5 t(x) ## [,1] [,2] [,3] ## [1,] 4 3 5 t(t(x)) ## [,1] ## [1,] 4 ## [2,] 3 ## [3,] 5 Note: R can be finicky about vectors and matrices. The transpose converts the vector \\(\\mathbf{x}\\) to a matrix \\(\\mathbf{x}&#39;\\) with one row. Thus, t(t(x)) gives a matrix with one column (which is a not quite the same as a vector). To force t(t(x)) to be a vector, you can drop the dimension with drop() drop(t(t(x))) ## [1] 4 3 5 3.0.1 Matrices We let uppercase bold letters \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), etc., represent matrices. We define the matrix \\(\\mathbf{A}\\) with \\(n\\) rows and \\(p\\) columns as \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1p} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{np} \\end{pmatrix}, \\end{align*} \\] with \\(a_{ij}\\) being the value of the matrix \\(\\mathbf{A}\\) in the \\(i\\)th row and the \\(j\\)th column. If the matrix \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} 5 &amp; 7 &amp; 1 \\\\ 5 &amp; -22 &amp; 2 \\\\ -14 &amp; 5 &amp; 99 \\\\ 42 &amp; -3 &amp; 0\\end{pmatrix}, \\end{align*} \\] the elements \\(a_{11}\\) = 5, \\(a_{12}\\) = 7, \\(a_{21}\\) = 5, and \\(a_{33}\\) = 99, etc. In R, we can define the matrix A using the matrix() function A &lt;- matrix( data = c(5, 5, -14, 42, 7, -22, 5, -3, 1, 2, 99, 0), nrow = 4, ncol = 3 ) A ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 Notice in the above creation of \\(\\mathbf{A}\\), we wrote defined the elements of the \\(\\mathbf{A}\\) using the columns stacked on top of one another. If we want to fill in the elements of \\(\\mathbf{A}\\) using the rows, we can add the option byrow = TRUE to the matrix() function A &lt;- matrix( data = c(5, 7, 1, 5, -22, 2, -14, 5, 99, 42, -3, 0), nrow = 4, ncol = 3, byrow = TRUE ) A ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 To select the \\(ij\\)th elements of \\(\\mathbf{A}\\), we use the subset operator [ to select the element. For example, to get the element \\(a_{11} = 5\\) in the first row and first column of \\(\\mathbf{A}\\), we use A[1, 1] ## [1] 5 The element \\(a_{3, 3} = 99\\) in the third row and third column can be selected using A[3, 3] ## [1] 99 The matrix \\(\\mathbf{A}\\) can also be represented as a set of either column vectors \\(\\{\\mathbf{c}_j \\}_{j=1}^p\\) or row vectors \\(\\{\\mathbf{r}_i \\}_{i=1}^n\\). For example, the column vector representation is \\[ \\begin{align*} \\mathbf{A} &amp; = \\left( \\mathbf{c}_{1} \\middle| \\mathbf{c}_{2} \\middle| \\cdots \\middle| \\mathbf{c}_{p} \\right), \\end{align*} \\] where the notation \\(|\\) is used to separate the vectors \\[ \\begin{align*} \\mathbf{c}_1 &amp; = \\begin{pmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{n1} \\end{pmatrix}, &amp; \\mathbf{c}_2 &amp; = \\begin{pmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{n2} \\end{pmatrix}, &amp; \\cdots, &amp; &amp; \\mathbf{c}_p &amp; = \\begin{pmatrix} a_{1p} \\\\ a_{2p} \\\\ \\vdots \\\\ a_{np} \\end{pmatrix} \\end{align*} \\] In R you can extract the columns using the [ selection operator c1 &lt;- A[, 1] # first column c2 &lt;- A[, 2] # second column c3 &lt;- A[, 3] # third column and you can give the column representation of the matrix A with with column bind function cbind() cbind(c1, c2, c3) ## c1 c2 c3 ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 The row vector representation of \\(\\mathbf{A}\\) is \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{r}_{1} \\\\ \\mathbf{r}_{2} \\\\ \\vdots \\\\ \\mathbf{r}_{n} \\end{pmatrix}, \\end{align*} \\] where the row vectors \\(\\mathbf{r}_i\\) are \\[ \\begin{align*} \\mathbf{r}_1 &amp; = \\left( a_{11}, a_{21}, \\ldots, a_{n1} \\right) \\\\ \\mathbf{r}_2 &amp; = \\left( a_{12}, a_{22}, \\ldots, a_{n2} \\right) \\\\ &amp; \\vdots \\\\ \\mathbf{r}_n &amp; = \\left( a_{1p}, a_{2p}, \\ldots, a_{np} \\right) \\end{align*} \\] In R you can extract the rows using the [ selection operator r1 &lt;- A[1, ] # first row r2 &lt;- A[2, ] # second row r3 &lt;- A[3, ] # third row r4 &lt;- A[4, ] # fourth row and you can give the row representation of the matrix A with with row bind function rbind() rbind(r1, r2, r3, r4) ## [,1] [,2] [,3] ## r1 5 7 1 ## r2 5 -22 2 ## r3 -14 5 99 ## r4 42 -3 0 Another way to represent matrices is using a block form. A block-representation of a matrix arises when the \\(n \\times p\\) matrix \\(\\mathbf{A}\\) is represented using smaller blocks as follows: \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} &amp; \\cdots &amp; \\mathbf{A}_{1K} \\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} &amp; \\cdots &amp; \\mathbf{A}_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{J1} &amp; \\mathbf{A}_{J2} &amp; \\cdots &amp; \\mathbf{A}_{JK} \\\\ \\end{pmatrix} \\\\ \\end{align*} \\] where \\(\\mathbf{A}_{ij}\\) is a \\(n_j \\times p_k\\) matrix where \\(\\sum_{j=1}^J n_j = n\\) and \\(\\sum_{k=1}^K p_k = p\\). For example, the matrix \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} 5 &amp; 7 &amp; 1 \\\\ 5 &amp; -22 &amp; 2 \\\\ -14 &amp; 5 &amp; 99 \\\\ 42 &amp; -3 &amp; 0\\end{pmatrix}, \\end{align*} \\] can be written in block diagonal form with \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} \\begin{bmatrix} 5 &amp; 7 \\\\ 5 &amp; -22 \\end{bmatrix} &amp; \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\\\ \\begin{bmatrix} -14 &amp; 5 \\\\ 42 &amp; -3 \\end{bmatrix} &amp; \\begin{bmatrix} 99 \\\\ 0 \\end{bmatrix} \\end{pmatrix}, \\end{align*} \\] where \\(\\mathbf{A}_{11} = \\begin{bmatrix} 5 &amp; 7 \\\\ 5 &amp; -22 \\end{bmatrix}\\) is a \\(2 \\times 2\\) matrix, \\(\\mathbf{A}_{12} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) is a \\(1 \\times 2\\) matrix, etc. A_11 &lt;- matrix(c(5, 5, 7, -22), 2, 2) A_12 &lt;- c(1, 2) A_21 &lt;- matrix(c(-14, 42, 5, -3), 2, 2) A_22 &lt;- c(99, 0) ## bind columns then rows rbind( cbind(A_11, A_12), cbind(A_21, A_22) ) ## A_12 ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 ## bind rows then columns cbind( rbind(A_11, A_21), c(A_12, A_22) ## rbind on vectors is different than c() ) ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 ## bind rows then columns cbind( rbind(A_11, A_21), ## convert the vectors to matrices for rbind rbind(as.matrix(A_12), as.matrix(A_22)) ) ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 3.0.1.1 Properties of matrices Matrix Addition: If the matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of the same dimension (e.g., both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) have the same number of rows \\(n\\) and the same number of columns \\(p\\)), then \\[ \\begin{align*} \\mathbf{A} + \\mathbf{B} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1p} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{np} \\end{pmatrix} + \\begin{pmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1p} \\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{n1} &amp; b_{n2} &amp; \\cdots &amp; b_{np} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} a_{11} + b_{11} &amp; b_{12} + b_{12} &amp; \\cdots &amp; a_{1p} + b_{1p} \\\\ a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \\cdots &amp; a_{2p} + b_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} + b_{n1} &amp; a_{n2} + b_{n2} &amp; \\cdots &amp; a_{np} + b_{np} \\end{pmatrix} \\\\ &amp; = \\left\\{ a_{ij} + b_{ij} \\right\\} \\end{align*} \\] … Another way to If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of the same dimension (same number of rows and columns) you can add the matrices together A &lt;- matrix(c(4, 1, 33, 2, 0, -4), 3, 2) B &lt;- matrix(c(7, -24, 3, 9, 11, -9), 3, 2) A ## [,1] [,2] ## [1,] 4 2 ## [2,] 1 0 ## [3,] 33 -4 B ## [,1] [,2] ## [1,] 7 9 ## [2,] -24 11 ## [3,] 3 -9 A + B ## [,1] [,2] ## [1,] 11 11 ## [2,] -23 11 ## [3,] 36 -13 We can also write this using for loops # initialize an empty matrix to fill C &lt;- matrix(0, 3, 2) for (i in 1:nrow(A)) { # loop over the rows for (j in 1:ncol(A)) { # loop over the columns C[i, j] &lt;- A[i, j] + B[i, j] } } C ## [,1] [,2] ## [1,] 11 11 ## [2,] -23 11 ## [3,] 36 -13 If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of different dimensions (they differ in either the number of rows or columns), R will return an error warning you that the matrices are of different sizes and can’t be added A &lt;- matrix(c(4, 1, 33, 2, 0, -4), 3, 2) B &lt;- matrix(c(7, -24, 3, 9), 2, 2) A ## [,1] [,2] ## [1,] 4 2 ## [2,] 1 0 ## [3,] 33 -4 B ## [,1] [,2] ## [1,] 7 3 ## [2,] -24 9 A + B ## Error in A + B: non-conformable arrays Matrix Multiplication: If \\(\\mathbf{A} = \\left\\{ a_{ij} \\right\\}\\) is an \\(n \\times p\\) matrix and \\(\\mathbf{B} = \\left\\{ a_{jk} \\right\\}\\) is a \\(p \\times q\\) matrix, then the matrix product \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) is an \\(n \\times q\\) matrix where \\(\\mathbf{C} = \\left\\{ \\sum_{j=1}^p a_{ij} b{jk} \\right\\}\\) \\[ \\begin{align*} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1p} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{np} \\end{pmatrix} \\begin{pmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1q} \\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2q} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{p1} &amp; b_{p2} &amp; \\cdots &amp; b_{pq} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} \\sum_{j=1}^p a_{1j} b_{j1} &amp; \\sum_{j=1}^p a_{1j} b_{j2} &amp; \\cdots &amp; \\sum_{j=1}^p a_{1j} b_{jq} \\\\ \\sum_{j=1}^p a_{2j} b_{j1} &amp;\\sum_{j=1}^p a_{2j} b_{j2} &amp; \\cdots &amp; \\sum_{j=1}^p a_{2j} b_{jq} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum_{j=1}^p a_{nj} b_{j1} &amp;\\sum_{j=1}^p a_{nj} b_{j2} &amp; \\cdots &amp; \\sum_{j=1}^p a_{nj} b_{jq} \\end{pmatrix} \\\\ &amp; = \\left\\{ \\sum_{j=1}^p a_{ij} b_{jk} \\right\\} \\end{align*} \\] Another way to define matrix multiplication is through inner product notation. Define the \\(n \\times p\\) matrix \\(\\mathbf{A}\\) and the \\(p \\times q\\) matrix \\(\\mathbf{B}\\) as the partition \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{a}_{1}&#39; \\\\ \\mathbf{a}_{2}&#39; \\\\ \\vdots \\\\ \\mathbf{a}_{n} \\end{pmatrix} &amp; \\mbox{ and } &amp;&amp; \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{b}_{1} &amp; \\mathbf{b}_{2} &amp; \\cdots &amp; \\mathbf{b}_{p} \\end{pmatrix} \\end{align*} \\] where \\(\\mathbf{a}\\) and \\(\\mathbf{b}_k\\) are both \\(p\\)-vectors. Then, we have \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B} = \\left\\{ c_{ik} \\right\\} = \\left\\{ \\mathbf{a}_i&#39; \\mathbf{b}_k \\right\\}\\) which has the matrix form \\[ \\begin{align*} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{a}_1&#39; \\mathbf{b}_1 &amp; \\mathbf{a}_1&#39; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}_1&#39; \\mathbf{b}_q \\\\ \\mathbf{a}_2&#39; \\mathbf{b}_1 &amp; \\mathbf{a}_2&#39; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}_2&#39; \\mathbf{b}_q \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{a}_n&#39; \\mathbf{b}_1 &amp; \\mathbf{a}_n&#39; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}_n&#39; \\mathbf{b}_q \\end{pmatrix} \\\\ &amp; = \\left\\{ \\mathbf{a}_i&#39; \\mathbf{b}_k \\right\\}. \\end{align*} \\] Written in this notation, we arrive at the multiplication rule for \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) – the \\(ik\\)th element \\(c_{ik}\\) of \\(\\mathbf{C}\\) is the inner product of the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). Direct Sums: For the \\(n \\times p\\) matrix \\(\\mathbf{A}\\) and the \\(m \\times q\\) matrix \\(\\mathbf{B}\\), the direct sum is \\[ \\begin{align*} \\mathbf{C} &amp; = \\mathbf{A} \\bigoplus \\mathbf{B} \\\\ &amp; = \\begin{pmatrix} \\mathbf{A} &amp; \\mathbf{0}_{n \\times q} \\\\ \\mathbf{0}_{m \\times p} &amp; \\mathbf{B} \\end{pmatrix} \\\\ \\end{align*} \\] where \\(\\mathbf{0}_{n \\times q}\\) and \\(\\mathbf{0}_{m \\times p}\\) is an are \\(n \\times q\\) and \\(m \\times p\\) matrices of all 0s, respectively. The direct sum can be generalized to any arbitrary number of matrices as \\[ \\begin{align*} \\mathbf{C} &amp; = \\bigoplus_{i=1}^K \\mathbf{A}_i \\\\ &amp; = \\begin{pmatrix} \\mathbf{A}_1 &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{A}_2 &amp; \\cdots &amp; \\mathbf{0} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{A}_K \\\\ \\end{pmatrix} \\\\ \\end{align*} \\] where the matrices \\(\\mathbf{0}\\) are all zeros of the appropriate dimension. 3.0.2 Arrays Higher order arrays (for example, tensors in the tensorflow library) can be represented using subscript notation where \\([\\mathbf{A}_1 | \\mathbf{A}_2 | \\cdots \\mathbf{A}_n]\\) is a 3-dimensional array. Higher order arrays can be represented using additional subscripts. 3.0.3 Lists To add: vector addition, multiplication To add: matrix addition, multiplication To add: determinants 3.1 Exercises What is 3 + \\(\\begin{pmatrix} 4 \\\\ 7 \\\\ 3 \\end{pmatrix}\\)? Why can’t you add the following two vectors: \\[ \\begin{align*} \\mathbf{x} = \\begin{pmatrix} 14 \\\\ 3 \\\\ 3 \\\\ -5 \\end{pmatrix} &amp; &amp; \\mathbf{y} = \\begin{pmatrix} 4 \\\\ 7 \\\\ 3 \\end{pmatrix} \\end{align*} \\] Based on the notation, what type of object is \\(\\mathbf{x}&#39;\\)? \\(\\mathbf{x}&#39; \\mathbf{y}\\)? \\(\\mathbf{x}&#39; \\mathbf{A}\\)? \\(\\mathbf{A}&#39; \\mathbf{y}\\)? \\(\\mathbf{X}&#39; \\mathbf{Z}\\)? "],["section-vector-spaces.html", "Chapter 4 Vectors spaces 4.1 Vectors 4.2 Vector addition 4.3 Span", " Chapter 4 Vectors spaces library(shiny) library(patchwork) library(tidyverse) # if gg3D package not installed, install the package library(gg3D) library(dasc2594) 4.1 Vectors For notation, we let lowercase Roman letters represent scalar numbers (e.g., n = 5, d = 7), lowercase bold letters represent vectors \\[ \\begin{align*} \\textbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\end{align*} \\] where the elements \\(x_1, \\ldots, x_n\\) are scalars written in lowercase Roman. Note that vectors are assumed to follow a vertical notation where the elements of the vector (the \\(x_i\\)s are stacked on top of one another) and the order matters. For example, the vector \\[ \\begin{align*} \\mathbf{x} &amp; = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix} \\end{align*} \\] has the first element \\(x_1 = 5\\), second element \\(x_2 = 2\\) and third element \\(x_3 = 8\\). Note that the vector \\(\\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) is not the same as the vector \\(\\begin{pmatrix} 8 \\\\ 2 \\\\ 5 \\end{pmatrix}\\) because the order of the elements matters. We can also write the vector as \\[ \\begin{align*} \\textbf{x} = \\left( x_1, x_2, \\ldots, x_n \\right)&#39;, \\end{align*} \\] where the \\(&#39;\\) symbol represents the transpose function. For our example matrix, we have \\(\\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}&#39; = \\begin{pmatrix} 5 &amp; 2 &amp; 8 \\end{pmatrix}\\) which is the original vector but arranged in a row rather than a column. Likewise, the transpose of a row vector \\(\\begin{pmatrix} 5 &amp; 2 &amp; 8 \\end{pmatrix}&#39; = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) is a column vector. If \\(\\mathbf{x}\\) is a column vector, we say that \\(\\mathbf{x}&#39;\\) is a row vector and if \\(\\mathbf{x}\\) is a row vector, the \\(\\mathbf{x}&#39;\\) is a column vector. To create a vector we can use the concatenate function c(). For example, the vector \\(\\mathbf{x} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) can be created as the R object using x &lt;- c(5, 2, 8) where the &lt;- assigns the values in the vector c(5, 2, 8) to the object named x. To print the values of x, we can use x ## [1] 5 2 8 which prints the elements of x. Notice that R prints the elements of \\(\\mathbf{x}\\) in a row; however, \\(\\mathbf{x}\\) is a column vector. This inconsistency is present to allow the output to be printed in a manner easier to read (more numbers fit on a row). If we put the column vector into a data.frame, then the vector will be presented as a column vector data.frame(x) ## x ## 1 5 ## 2 2 ## 3 8 One can use the index operator \\([\\hspace{2mm}]\\) to select specific elements of the vector \\(\\mathbf{x}\\). For example, the first element of \\(\\mathbf{x}\\), \\(x_1\\), is x[1] ## [1] 5 and the third element of \\(\\mathbf{x}\\), \\(x_3\\), is x[3] ## [1] 8 The transpose function t() turns a column vector into a row vector (or a row vector into a column vector). For example the transpose \\(\\mathbf{x}&#39;\\) of \\(\\mathbf{x}\\) is tx &lt;- t(x) tx ## [,1] [,2] [,3] ## [1,] 5 2 8 where tx is R object storing the transpose of \\(\\mathbf{x}\\) and is a row vector. The transpose of tx. Notice the indices on the output of the row vector tx. The index operator [1, ] selects the first row to tx and the index operator [, 1] gives the first column tx. Taking the transpose again gives us back the original column vector t(tx) ## [,1] ## [1,] 5 ## [2,] 2 ## [3,] 8 4.1.1 Properties of Vectors For any real valued scalars \\(a, b \\in \\mathcal{R}\\) and any vectors \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathcal{R}^n\\) (vectors of real numbers of length \\(n\\)), scalar multiplication \\[ \\begin{align*} a \\mathbf{x} &amp; = a \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} a x_1 \\\\ a x_2 \\\\ \\vdots \\\\ a x_n \\end{pmatrix} \\end{align*} \\] where the scalar \\(a\\) is multiplied by each element of the vector. For example, \\[ \\begin{align*} 4 \\begin{pmatrix} 4 \\\\ 6 \\\\ 7 \\\\ 12 \\end{pmatrix} &amp; = \\begin{pmatrix} 4 * 4 \\\\ 4 * 6 \\\\ 4 * 7 \\\\ 4 * 12 \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} 16 \\\\ 24 \\\\ 28 \\\\ 48 \\end{pmatrix} \\end{align*} \\] In R, we can multiply the vector by a scalar as 4 * c(4, 6, 7, 12) ## [1] 16 24 28 48 or if the vector \\(\\mathbf{x} = \\left( 4, 6, 7, 12 \\right)&#39;\\) we can write this as x &lt;- c(4, 6, 7, 12) 4 * x ## [1] 16 24 28 48 scalar multiplicative commutivity \\[ \\begin{align*} a (b \\mathbf{x}) &amp; = (ab) \\mathbf{x} &amp; = b (a \\mathbf{x}) \\end{align*} \\] 4 * (6 * x) ## [1] 96 144 168 288 (4 * 6) * x ## [1] 96 144 168 288 scalar additive associativity \\[ \\begin{align*} a \\mathbf{x} + b \\mathbf{x} &amp; = (a + b) \\mathbf{x} \\end{align*} \\] vector additive associativity \\[ \\begin{align*} a \\mathbf{x} + a \\mathbf{y} &amp; = a (\\mathbf{x} + \\mathbf{y}) \\end{align*} \\] vector associativity \\[ \\begin{align*} \\mathbf{x} + \\mathbf{y} &amp; = \\mathbf{y} + \\mathbf{x} \\end{align*} \\] \\[ \\begin{align*} (\\mathbf{x} + \\mathbf{y}) + \\mathbf{z} &amp; = \\mathbf{x} + (\\mathbf{y} + \\mathbf{z}) \\end{align*} \\] x &lt;- c(1, 2, 3, 4) y &lt;- c(4, 3, 5, 1) z &lt;- c(5, 2, 4, 6) x + y ## [1] 5 5 8 5 y + x ## [1] 5 5 8 5 (x + y) + z ## [1] 10 7 12 11 x + (y + z) ## [1] 10 7 12 11 Identity Element of Addition: For any vector \\(\\mathbf{x}\\) of length \\(n\\), there exists a vector \\(\\mathbf{0}\\), known as the zero vector, such that \\[ \\begin{align*} \\mathbf{x} + \\mathbf{0} &amp; = \\mathbf{x} \\end{align*} \\] x + 0 ## [1] 1 2 3 4 x + rep(0, 4) ## [1] 1 2 3 4 Inverse Element of Addition: For any vector \\(\\mathbf{x}\\) of length \\(n\\), there exists a vector \\(-\\mathbf{x}\\), known as the additive inverse vector, such that \\[ \\begin{align*} \\mathbf{x} + (- \\mathbf{x}) &amp; = \\mathbf{0} \\end{align*} \\] x + (-x) ## [1] 0 0 0 0 4.2 Vector addition Two vectors of length \\(n\\) can be added elementwise \\[ \\begin{align*} \\mathbf{x} + \\mathbf{y} &amp; = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} + \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix} \\end{align*} \\] For example, \\[ \\begin{align*} \\begin{pmatrix} 3 \\\\ 1 \\\\ -4 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} -3 \\\\ 17 \\\\ -39 \\\\ 4 \\end{pmatrix} &amp; = \\begin{pmatrix} 3 + (-3) \\\\ 1 + 17 \\\\ -4 + (-39) \\\\ 3 + 4 \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} 0 \\\\ 18 \\\\ -43 \\\\ 7 \\end{pmatrix} \\end{align*} \\] In R, we have x &lt;- c(3, 1, -4, 3) y &lt;- c(-3, 17, -39, 4) x + y ## [1] 0 18 -43 7 If two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are of different lengths, then they cannot be added together. Using R, we get the following error: x &lt;- c(1, 2, 3) y &lt;- c(1, 2, 3, 4) x + y ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 2 4 6 5 The error is telling us that the vector \\(\\mathbf{x}\\) and the vector \\(\\mathbf{y}\\) do not have the same length. Be careful when adding vectors in R. R uses “recycling” which means two vectors of different lengths can be added together if one vector is of a length that is a multiple of the other vector. For example, if \\(\\mathbf{x} = (1, 2)&#39;\\) is a vector of length 2 and \\(\\mathbf{y} = (1, 2, 3, 4)\\) is a vector of length 4, R will add \\(\\mathbf{x} + \\mathbf{y}\\) by replicating the vector \\(\\mathbf{x}\\) twice (i.e., \\(\\mathbf{x} + \\mathbf{y} = \\left( \\mathbf{x}&#39;, \\mathbf{x}&#39; \\right)&#39; = \\left(1, 2, 1, 2 \\right)&#39; + \\mathbf{y}\\)) x &lt;- c(1, 2) y &lt;- c(1, 2, 3, 4) x + y ## [1] 2 4 4 6 # replicated x = c(1, 2, 1, 2) c(1, 2, 1, 2) + y ## [1] 2 4 4 6 4.2.1 The geometric interpretation of vectors in \\(\\mathcal{R}^2\\) Let \\(\\mathcal{R}^2\\) be a real coordinate space of \\(2\\) dimensions. You are already familiar with the Cartesian plane that consists of ordered pairs \\((x, y)\\). The Cartesian plane defines the real coordinate space \\(\\mathbf{R}^2\\) of two dimensions. In \\(\\mathbf{R}^2\\), the location of any point of interest can be defined using the \\(x\\) and \\(y\\). For example, the plot below shows the location of the point (2, 3) dat &lt;- data.frame( x = 2, y = 3 ) ggplot(data = dat, aes(x = x, y = y)) + geom_point() + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) A vector space is a generalization of this representation. In \\(\\mathcal{R}^2\\), we say that the vector \\(\\mathbf{z} = c(2, 3)\\) is centered at the origin (0, 0) and has length 2 in the \\(x\\)-axis and length 3 in the \\(y\\)-axis. The plot below shows this vector We can also decompose the vector \\(\\mathbf{z}\\) into its \\(x\\) and \\(y\\) components. The \\(x\\) component of \\(\\mathbf{z}\\) is (2, 0) and the \\(y\\) component of \\(\\mathbf{z}\\) is (0, 3). The following plot shows the \\(x\\) component (2, 0) in blue and the \\(y\\) component (0, 3) in red. The below Shiny app allows you to plot the vector for any \\((x, y)\\) pair of your choosing. The shiny app can be downloaded and run on your own computer using library(shiny) runGitHub(rep = &quot;multivariable-math&quot;, username = &quot;jtipton25&quot;, subdir = &quot;shiny-apps/chapter-03/vector-space&quot;) 4.2.1.1 Addition of vectors We can represent the addition of vectors geometrically as well. Consider the two vectors \\(\\mathbf{u}\\) = (3, 2) and \\(\\mathbf{v}\\) = (-2, 1) where \\(\\mathbf{u} + \\mathbf{v}\\) = (1, 3). data.frame(x = c(3, -2, 1), y = c(2, 1, 3), vector = c(&quot;u&quot;, &quot;v&quot;, &quot;u+v&quot;)) %&gt;% ggplot() + geom_point(aes(x = x, y = y, color = vector)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) We can represent the sum using vectors by adding \\(\\mathbf{u}\\) first then adding \\(\\mathbf{v}\\) to \\(\\mathbf{u}\\) or by adding \\(\\mathbf{v}\\) first and then \\(\\mathbf{u}\\) to get df &lt;- data.frame(x = c(0, 3, 1, -2), y = c(0, 2, 3, 1)) p1 &lt;- ggplot() + geom_segment(aes(x = 0, xend = 3, y = 0, yend = 2), arrow = arrow(), color = &quot;blue&quot;) + geom_segment(aes(x = 3, xend = 3 - 2, y = 2, yend = 2 + 1), arrow = arrow(), color = &quot;red&quot;) + geom_segment(aes(x = 0, xend = 3 - 2, y = 0, yend = 2 + 1), arrow = arrow(), color = &quot;black&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_polygon(data = df, aes(x = x, y = y), fill = &quot;grey&quot;, alpha = 0.5) + ggtitle(&quot;u + v&quot;) p2 &lt;- ggplot() + geom_segment(aes(x = 0, xend = -2, y = 0, yend = 1), arrow = arrow(), color = &quot;red&quot;) + geom_segment(aes(x = -2, xend = -2 + 3, y = 1, yend = 1 + 2), arrow = arrow(), color = &quot;blue&quot;) + geom_segment(aes(x = 0, xend = 3 - 2, y = 0, yend = 2 + 1), arrow = arrow(), color = &quot;black&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_polygon(data = df, aes(x = x, y = y), fill = &quot;grey&quot;, alpha = 0.5) + ggtitle(&quot;v + u&quot;) p1 + p2 Notice that the sum of these vectors defines a parallelogram where the sum \\(\\mathbf{u} + \\mathbf{v}\\) is the diagonal of the shaded parallelogram. This geometric interpretation will serve as a basis for interpreting vector equations in higher dimensions where typical visualization methods fail. 4.2.2 Scalar multiplication of vectors We can represent the multiplication of a vector by a scalar geometrically as well. Consider the vector \\(\\mathbf{u}\\) = (3, 2) and the scalars \\(a = 2\\) and \\(b = -1\\). Then, we can plot \\(\\mathbf{u}\\), \\(a\\mathbf{u}\\), and \\(b\\mathbf{u}\\). data.frame(x = c(3, 2 * 3, -1 * 3), y = c(2, 2 * 2, -1 * 2), vector = c(&quot;u&quot;, &quot;a*u&quot;, &quot;b*u&quot;)) %&gt;% ggplot() + geom_point(aes(x = x, y = y, color = vector)) + geom_segment(aes(x = 0, xend = x, y = 0, yend = y, color = vector), arrow = arrow(), alpha = 0.75) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-6, 6), ylim = c(-6, 6)) In fact, if \\(a\\) is allowed to take on any values, then the set of all possible values of \\(a \\mathbf{u}\\) for all values of \\(a\\) defines an infinite line ggplot() + geom_abline(slope = 2/3, intercept = 0) + geom_point(aes(x = 3, y = 2)) + geom_segment(aes(x = 0, xend = 3, y = 0, yend = 2), arrow = arrow(), color = &quot;black&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) 4.2.3 The geometric interpretation of vectors in \\(\\mathcal{R}^3\\) Let the vector \\(\\mathbf{u} = c(-2, 3, 5)\\). Then, the figure below shows the vector in 3 dimensions. Draw picture by hand 4.2.4 The geometric interpretation of vectors in \\(\\mathcal{R}^n\\) As the number of dimensions increases, the same interpretation can be used, but the ability to visualize higher dimensions becomes more difficult. 4.2.5 Linear Combinations of Vectors We say that for any two scalars \\(a\\) and \\(b\\) and any two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of length \\(n\\), the sum \\[ \\begin{align*} a \\mathbf{x} + b \\mathbf{y} &amp; = \\begin{pmatrix} a x_1 + b y_1 \\\\ a x_2 + b y_2 \\\\ \\vdots \\\\ a x_n + b y_n \\\\ \\end{pmatrix} \\end{align*} \\] is called a linear combination. The idea of a linear combination can be extended to \\(K\\) different scalars \\(\\{ a_1, \\ldots, a_K \\}\\) and \\(K\\) different vectors \\(\\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_K\\}\\) each of length \\(n\\) as \\[ \\begin{align*} a_1 \\mathbf{x}_1 + a_2 + \\mathbf{x}_2 + \\ldots + a_K \\mathbf{x}_K = \\sum_{k=1}^K a_k \\mathbf{x}_k &amp; = \\begin{pmatrix} \\sum_{k=1}^K a_k x_{k1} \\\\ \\sum_{k=1}^K a_k x_{k2} \\\\ \\vdots \\\\ \\sum_{k=1}^K a_k x_{kn} \\\\ \\end{pmatrix} \\end{align*} \\] The scalars \\(a_k\\) are called coefficients (sometimes also called weights). Example: Consider the linear combination \\(a \\mathbf{u} + b \\mathbf{v}\\) where \\[ \\begin{align*} \\mathbf{u} = \\begin{pmatrix} 3 \\\\ 6\\end{pmatrix} &amp;&amp; \\mathbf{v} = \\begin{pmatrix} -2 \\\\ 1\\end{pmatrix}. \\end{align*} \\] Are there values of \\(a\\) and \\(b\\) such \\(a \\mathbf{x}_1 + b \\mathbf{x}_2 = \\begin{pmatrix} 9 \\\\ - 4 \\end{pmatrix}\\)? To answer this question, we can write the linear combination as \\[ \\begin{align*} a \\begin{pmatrix} 3 \\\\ 6\\end{pmatrix} + b \\begin{pmatrix} -2 \\\\ 1\\end{pmatrix} &amp; = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix} \\end{align*} \\] which can be written using the property of scalar multiplication as \\[ \\begin{align*} \\begin{pmatrix} 3a \\\\ 6a \\end{pmatrix} + \\begin{pmatrix} -2b \\\\ b \\end{pmatrix} &amp; = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix} \\end{align*} \\] and using properties of vector addition can be written as \\[ \\begin{align*} \\begin{pmatrix} 3a - 2b \\\\ 6a + b \\end{pmatrix} &amp; = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix} \\end{align*} \\] Recognizing this as a system of linear equations \\[ \\begin{align*} 3a - 2b &amp; = 9 \\\\ 6a + b &amp; = -4, \\end{align*} \\] the system of equations can be written in an augmented matrix form as \\[ \\begin{align*} \\begin{pmatrix} 3 &amp; - 2 &amp; 9\\\\ 6 &amp; 1 &amp; -4 \\end{pmatrix} \\end{align*} \\] Reducing the augmented matrix to reduced row echelon form gives rref(matrix(c(3, 6, -2, 1, 9, -4), 2, 3)) ## [,1] [,2] [,3] ## [1,] 1 0 0.06666667 ## [2,] 0 1 -4.40000000 which has solutions \\(a = 0.0667\\) and \\(b = -4.4\\). Result: Any vector equation \\(a_1 \\mathbf{x}_1 + a_2 + \\mathbf{x}_2 + \\ldots + a_K \\mathbf{x}_K = \\mathbf{c}\\) for a given constant vector \\(\\mathbf{b}\\) has the same solution set as the augmented matrix \\[ \\begin{align*} \\begin{pmatrix} \\mathbf{x}_1 &amp; \\mathbf{x}_2 &amp; \\cdots &amp; \\mathbf{x}_K &amp; \\mathbf{b} \\end{pmatrix} \\end{align*} \\] Equivalently, the set of vectors \\(\\{\\mathbf{x}_k\\}_{k=1}^K\\) can only be combined with linear coefficients \\(\\{a_k\\}_{k=1}^K\\) to equal the vector \\(\\mathbf{b}\\) if the linear system of equations is consistent. 4.2.6 The geometric interpretation of linear combinations of vectors Consider the vectors \\(\\mathbf{u} = \\begin{pmatrix} \\sqrt{2} \\\\ - \\sqrt{2} \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1\\end{pmatrix}\\) shown in the figure below on the left. Exercise: Given \\(\\mathbf{u} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\) and \\(\\mathbf{v} = begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\), estimate the linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) that gives the point \\(\\mathbf{w}\\) in the figure below. 4.3 Span Let \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) be vectors in \\(\\mathcal{R}^n\\). We say the vector \\(\\mathbf{w}\\) is in the span of \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) (\\(\\mathbf{w} \\in \\mbox{span}\\{ \\mathbf{a}_1, \\ldots, \\mathbf{a}_K \\}\\)) if there exists coefficients \\(x_1, \\ldots, x_K\\) such that \\(\\mathbf{w} = \\sum_{k=1}^K x_k \\mathbf{a}_k\\). Example: While not a vector notation, you already understand the span from polynomial functions. For example, assume you have the functions 1, \\(x\\), and \\(x^2\\). Then, the functions \\(-4 + 3x^2\\) (\\(a_1 = -4, a_2 = 0, a_3 = 3\\)) and \\(-3 + 4x - 2x^2\\) (\\(a_1 = -3, a_2 = 4, a_3 = -2\\)) are in the span of functions \\(\\{1, x, x^2\\}\\), but the functions \\(x^3\\), \\(x^4 - 2x^2\\), etc., are not in the span of \\(\\{1, x, x^2\\}\\) because you cannot write these as a linear combination of \\(a_1 1 + a_2 x + a_3 x^2\\). 4.3.1 Geometric example of the span Example: Consider the vector \\(\\mathbf{u} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\). Then, the vector \\(\\mathbf{w} = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}\\) is in the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) because \\(\\mathbf{w} = 2 \\mathbf{u}\\) but the vector \\(\\mathbf{v} = \\begin{pmatrix} 4 \\\\ -4 \\end{pmatrix}\\) is not in the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) because there is no coefficient \\(a\\) such that \\(\\mathbf{w} = a \\mathbf{u}\\). In this example, the vector \\(\\mathbf{u}\\) is a 2-dimensional vector (lives in \\(\\mathcal{R}^2\\)–a plane) but the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) lives in 1-dimension (a line). ggplot() + geom_abline(slope = 1/2, intercept = 0, color = &quot;blue&quot;, size = 2) + geom_segment(aes(x = 0, xend = 2, y = 0, yend = 1), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;red&quot;) + geom_segment(aes(x = 0, xend = 4, y = 0, yend = 2), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;red&quot;) + geom_segment(aes(x = 0, xend = 4, y = 0, yend = -4), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;orange&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_text(data = data.frame(x = c(2, 4, 4), y = c(1, 2, -4), text = c(&quot;u&quot;, &quot;w&quot;, &quot;v&quot;)), aes(x = x, y = y + 0.5, label = text), size = 5, inherit.aes = FALSE, color = c(&quot;red&quot;, &quot;red&quot;, &quot;orange&quot;)) + ggtitle(&quot;span{u} is the blue line \\nw is in span{u}\\nv is not in span{u}&quot;) From the example above, we can answer the question “Is the point (a, b) on the line defined by the vector \\(\\mathbf{u}\\)?” by asking whether the point (a, b) is in the \\(span\\{\\mathbf{u}\\}\\). While this is trivial for such a simple problem, the use of the span will make things easier in higher dimensions. Example: do in class 2 3-d vectors that are not scalar multiples of each other define a plane. Does a point lie within the plane? Use the span to answer this question. "],["section-matrix-equation.html", "Chapter 5 Matrix equations 5.1 Solutions of matrix equations 5.2 Existence of solutions 5.3 Matrix multiplication 5.4 Properties of matrix-vector multiplication 5.5 Solutions of linear systems 5.6 Solutions to nonhomogeneous systems 5.7 Finding solutions", " Chapter 5 Matrix equations library(tidyverse) Here we introduce the concept of the linear equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\). This equation is the most fundamental equation in all of statistics and data science. Given a matrix \\(\\mathbf{A}\\) and a vector of constants \\(\\mathbf{b}\\), the goal is to solve for the value (or values) of \\(\\mathbf{x}\\) that are a solution to this equation. The equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) is a matrix representation of the system of linear equations \\[ \\begin{align*} \\mathbf{A} \\mathbf{x} &amp; = \\mathbf{b} \\\\ \\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_K \\end{pmatrix} &amp; = \\mathbf{b} \\\\ x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K &amp; = \\mathbf{b} \\\\ \\end{align*} \\] as long as the matrix \\(\\mathbf{A}\\) has \\(n\\) rows and \\(K\\) columns and the vectors \\(\\mathbf{a}_k\\) are \\(n\\)-dimensional. Example: in class Example: in class 5.1 Solutions of matrix equations Because the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) is equivalent to a linear system of equations \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\), we can solve the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) by writing the equation in an augmented matrix form \\[ \\begin{align*} \\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K &amp; \\mathbf{b} \\end{pmatrix} \\end{align*} \\] and then reducing the matrix to reduced row echelon form. This gives rise to the theorem Theorem: The matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\), the vector equation \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\), and the augmented matrix \\(\\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K &amp; \\mathbf{b} \\end{pmatrix}\\) all have the same solution set. 5.2 Existence of solutions A solution to the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) exists if and only if \\(\\mathbf{b}\\) is a linear combination of the columns of \\(\\mathbf{A}\\). In other words, \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a solution if and only if \\(\\mathbf{b}\\) is in the \\(\\mbox{span}\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\}\\). Example: in class Let \\(\\mathbf{A} =\\ldots\\) and \\(\\mathbf{b} = \\ldots\\). Is the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) consistent? Theorem: For the \\(n \\times K\\) matrix \\(\\mathbf{A}\\), the following statements are equivalent: a) For each \\(\\mathbf{b} \\in \\mathcal{R}^K\\), the equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has at least one solution b) Each \\(\\mathbf{b} \\in \\mathcal{R}^K\\) is a linear combination of the columns of \\(\\mathbf{A}\\) c) The columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^K\\) d) \\(\\mathbf{A}\\) has a pivot in every row 5.3 Matrix multiplication To calculate \\(\\mathbf{A} \\mathbf{x}\\), we need to define matrix multiplication. The equivalence between the linear systems of equations \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\) and the matrix equation \\(\\mathbf{A} \\mathbf{x}\\) gives a hint in how to do this. First, recall the definition of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1K} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\ldots &amp; a_{nK} \\\\ \\end{pmatrix} &amp;&amp; \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\end{align*} \\] The matrix product \\(\\mathbf{A}\\mathbf{x}\\) is the linear system of equations \\[ \\begin{align*} \\mathbf{A} \\mathbf{x} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1K} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\ldots &amp; a_{nK} \\\\ \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\\\ &amp; = x_1\\begin{pmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{n1} \\end{pmatrix} + x_2 \\begin{pmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{n2} \\end{pmatrix} + \\cdots + x_K \\begin{pmatrix} a_{1K} \\\\ a_{nK} \\\\ \\vdots \\\\ a_{nK} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} a_{11} x_1 + a_{12} x_2 + \\ldots + a_{1K} x_K \\\\ a_{21} x_1 + a_{22} x_2 + \\ldots + a_{2K} x_K \\\\ \\vdots \\\\ a_{n1} x_1 + a_{n2} x_2 + \\ldots + a_{nK} x_K \\\\ \\end{pmatrix} \\end{align*} \\] Notice that the first row of the last matrix above has the sum first row of the matrix \\(\\mathbf{A}\\) multiplied by the corresponding elements in \\(\\mathbf{x}\\) (i.e., first element \\(a_{11}\\) of the first row of \\(\\mathbf{A}\\) times the first element \\(x_1\\) of \\(\\mathbf{x}\\) plus the second, third, fourth, etc.). Likewise, this pattern holds for the second row, and all the other rows. This gives an algorithm for evaluating the product \\(\\mathbf{A} \\mathbf{x}\\). Definition: The product \\(\\mathbf{A}\\mathbf{x}\\) of a \\(n \\times K\\) matrix \\(\\mathbf{A}\\) with a \\(K\\)-vector \\(\\mathbf{x}\\) is a \\(n\\)-vector where the \\(i\\)th element of \\(\\mathbf{A}\\mathbf{x}\\) is the sum of the \\(i\\)th row of \\(\\mathbf{A}\\) times the corresponding elements of the vector \\(\\mathbf{x}\\) Example: in class Example: in class Example: in R using loops Example: in R using %*% 5.4 Properties of matrix-vector multiplication If \\(\\mathbf{A}\\) is a \\(n \\times K\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^K\\) and \\(c\\) is a scalar, then \\(\\mathbf{A} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{A} \\mathbf{u} + \\mathbf{A} \\mathbf{v}\\) \\(\\mathbf{A} (c \\mathbf{u}) = (c \\mathbf{A}) \\mathbf{u}\\) Proof in class 5.5 Solutions of linear systems 5.5.1 Homogeneous linear systems of equations Definition: The matrix equation \\[ \\begin{align} \\tag{5.1} \\mathbf{A}\\mathbf{x} = \\mathbf{0} \\label{eq:homogeneous} \\end{align} \\] is called a homogeneous system of equations. The vector \\(\\mathbf{0}\\) is a vector of length \\(K\\) composed of all zeros. The trivial solution of the homogeneous equation is when \\(\\mathbf{x} = \\mathbf{0}\\) and is not a very useful solution. Typically one is interested in nontrivial solutions where \\(\\mathbf{x} \\neq \\mathbf{0}\\). The homogeneous linear system of equations can be written in augmented matrix form \\[ \\begin{align*} \\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K &amp; \\mathbf{0} \\end{pmatrix} \\end{align*} \\] which implies that a non-trivial solution only exists if there is a free variable. Another way of saying this is that at least one column must not be a pivot column. If every column were a pivot column, the reduced row echelon form of the augmented matrix would be \\[ \\begin{align*} \\begin{pmatrix} 1 &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\ldots &amp; 1 &amp; 0 \\end{pmatrix} \\end{align*} \\] which implies the only solution is the trivial solution \\(\\mathbf{0}\\). Example: in class \\[ \\begin{align*} 3 x_1 - 2 x_2 + 4 x_3 = 0 \\\\ - 2 x_1 + 4 x_2 - 2 x_3 = 0 \\\\ 5 x_1 - 6 x_2 + 6 x_3 = 0 \\end{align*} \\] * Example: in class Consider the equation \\[ \\begin{align*} 2x_1 + 4 x_2 - x_3 = 0. \\end{align*} \\] we can write this as \\[ \\begin{align*} x_1 = -2 x_2 + \\frac{1}{2} x_3 \\end{align*} \\] where \\(x_2\\) and \\(x_3\\) are free variables. Writing this as a solution \\(\\mathbf{x}\\) gives \\[ \\begin{align*} \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} -2 x_2 + \\frac{1}{2} x_3 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = x_2 \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} + x_3 \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{align*} \\] which is a linear combination of the vectors \\(\\mathbf{u} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix}\\). This implies that we can write the solution \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) for scalars \\(a\\) and \\(b\\). Therefore, the solution set \\(\\mathbf{x}\\) is contained in the \\(\\mbox{span}\\{\\mathbf{u}, \\mathbf{v}\\}\\). Because the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are linearly independent (they don’t point in the same direction), the set of all linear combinations of \\(c \\mathbf{u} + d \\mathbf{v}\\) defines a plane. Definition: A solution set of the form \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) is called a parametric vector solution. 5.6 Solutions to nonhomogeneous systems Recall the simple linear equation \\[ y = mx + b \\] where \\(m\\) is the slope and \\(b\\) is the y-intercept. Setting \\(b = 0\\) gives a simple homogenous linear equation where the y-intercept goes through the origin (0, 0). When \\(b\\) is nonzero, the line keeps the same slope but is shifted upward/downward by \\(b\\). ggplot(data = data.frame(x = 0, y = 0), aes(x, y)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_abline(slope = 2, intercept = 0, color = &quot;red&quot;) + geom_abline(slope = 2, intercept = 2, color = &quot;blue&quot;) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_text( data = data.frame(x = c(0, 0), y = c(0, 2), text = c(&quot;homogeneous\\nsolution&quot;, &quot;inhomogeneous\\nsolution&quot;)), aes(x = x + c(1.75, -1), y = y + 0.5, label = text), size = 5, inherit.aes = FALSE, color = c(&quot;red&quot;, &quot;blue&quot;)) + geom_segment( aes(x = 0, xend = 0, y = 0, yend = 2), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;orange&quot;) + geom_text( data = data.frame(x = 0, y = 2, text = &quot;b&quot;), aes(x = x + 0.5, y = y, label = text), size = 8, inherit.aes = FALSE, color = &quot;orange&quot;) This shift in location (but not in slope) is called a translation Example: in class Let’s revisit the example from before \\[ \\begin{align*} 3 x_1 - 2 x_2 + 4 x_3 = 0 \\\\ - 2 x_1 + 4 x_2 - 2 x_3 = 0 \\\\ 5 x_1 - 6 x_2 + 6 x_3 = 0 \\end{align*} \\] but change this so that \\(\\mathbf{b} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 1 \\end{pmatrix}\\) Write this as a parametric solution with a mean shift Example: Show this shift for a system of linear equations where the solution set defines a plane. From example above, \\[ \\begin{align*} 2x_1 + 4 x_2 - x_3 = 0. \\end{align*} \\] has the parametric solution \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) with \\[ \\begin{align*} \\mathbf{u} &amp; = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\mathbf{v} &amp; = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{align*} \\] Now, if we change the system of linear equations so that we have the inhomogeneous equation \\[ \\begin{align*} 2x_1 + 4 x_2 - x_3 = 20. \\end{align*} \\] we get the homogeneous solution set \\(x_1 = -2 x_2 + \\frac{1}{2} x_3 + 4\\) which can be written in parametric form as \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v} + \\mathbf{p}\\)$ with \\[ \\begin{align*} \\mathbf{u} &amp; = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\mathbf{v} &amp; = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} \\\\ \\mathbf{p} &amp; = \\begin{pmatrix} 20 \\\\ 0 \\\\ 0 \\end{pmatrix} \\end{align*} \\] A &lt;- matrix(c(3, -2, 5, -2, 4, -6, 4, -2, 6, 2, -6, 8), 3, 4) rref(A) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 1.50 -0.50 ## [2,] 0 1 0.25 -1.75 ## [3,] 0 0 0.00 0.00 \\[ \\begin{align*} x_1 = \\frac{3}{2} x_2 - \\frac{1}{2}\\\\ x_2 = \\frac{1}{4} x_3 - \\frac{7}{4} \\\\ \\end{align*} \\] which was the same solution set as the homoegenous solution plus the additional vector \\(\\begin{pmatrix} 20 \\\\ 0 \\\\ 0 \\end{pmatrix}\\). Thus, the inhomogenous solution is now \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v} + \\mathbf{p}\\) where \\(\\mathbf{u} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\\), \\(\\mathbf{v} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix}\\), and \\(\\mathbf{p} = \\begin{pmatrix} 20\\\\ 0 \\\\ 0 \\end{pmatrix}\\). For plotting, we will solve these equations for \\(x_3\\). Thus, the homogeneous equation has the solution \\(x_3 = 2x_1 + 4x_2\\) and the inhomogenous equation has the solution \\(x_3 = 2x_1 + 4x_2 - 20\\). # uses gg3D library n &lt;- 60 x1 &lt;- x2 &lt;- seq(-10, 10, length = n) region &lt;- expand.grid(x1 = x1, x2 = x2) df &lt;- data.frame( x1 = region$x1, x2 = region$x2, x3 = c( 2 * region$x1 + 4 * region$x2, 2 * region$x1 + 4 * region$x2 - 20), equation = rep(c(&quot;inhomogeneous&quot;, &quot;homogeneous&quot;), each = n^2)) # theta and phi set up the &quot;perspective/viewing angle&quot; of the 3D plot theta &lt;- 45 phi &lt;- 20 ggplot(df, aes(x = x1, y = x2, z = x3, color = equation)) + axes_3D(theta = theta, phi = phi) + stat_wireframe( alpha = 0.75, theta = theta, phi = phi) + scale_color_manual(values = c(&quot;inhomogeneous&quot; = &quot;blue&quot;, &quot;homogeneous&quot; = &quot;red&quot;)) + theme_void() + theme(legend.position = &quot;none&quot;) + labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), angle=c(0, 0, 90), theta = theta, phi = phi) ## Warning: Removed 4 row(s) containing missing values (geom_path). 5.7 Finding solutions The following algorithm describes how to solve a linear system of equations. 1) Put the system of equations in an augmented matrix form 2) Reduce the augmented matrix to reduced row echelon form 3) Express each determined variable as a function of the free variables. 4) Write the solution in a general form where the determined variables are a function of the independent variables 5) Decompose the solution \\(\\mathbf{x}\\) into a linear combination of free variables as parameters "],["section-linear-independence.html", "Chapter 6 Linear independence", " Chapter 6 Linear independence Recall the homogeneous equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\) can be written as a linear combination of coefficients \\(x_1, \\ldots, x_K\\) and vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) where \\[ \\begin{align*} \\sum_{k=1}^K x_k \\mathbf{a}_k = \\mathbf{0} \\end{align*} \\] Definition: The set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) are called linearly independent if the only solution to the vector equation \\(\\sum_{k=1}^K x_k \\mathbf{a}_k = \\mathbf{0}\\) is the trivial solution. The set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) are called linearly dependent if there are coefficients \\(x_1, \\ldots, x_K\\) that are not all zero. Example: in class What does it mean for a set of vectors to be linearly dependent? This means that there is at least one vector \\(\\mathbf{a}_k\\) that can be written as a sum of the other vectors with coefficients \\(z_k\\): \\[ \\begin{align*} \\mathbf{a}_k = \\sum_{k&#39; \\neq k} z_{k&#39;} \\mathbf{a}_{k&#39;} \\end{align*} \\] Note: this does not imply that all vectors \\(\\mathbf{a}_{k}\\) can be written as a linear combination of other vectors. Example: in class – determine if the vectors are linearly independent and solve the dependence relation Theorem: The matrix equation \\(\\mathbf{A}\\) has linearly independent columns if and only if the equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution. Example: in class A set of a single vector Example: in class A set of two vectors linearly independent if: linearly dependent if one vector is a scalar multiple of the other: Theorem: If an \\(n \\times K\\) matrix \\(\\mathbf{A}\\) has \\(K &gt; n\\), then the columns of \\(\\mathbf{A}\\) are linearly dependent. In other words, if a set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) contains more vectors than entries within vectors, the set of vectors is linearly dependent. Proof: If \\(K&gt;n\\), there are more variables (\\(K\\)) than equations (\\(n\\)). Therefore, there is at least one free variable and this implies that the homogeneous equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\) has a non-trivial solution (5.1) Theorem: If a set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) contains the \\(\\mathbf{0}\\) vector, then the the set of vectors is linearly dependent. Proof: in class Example: in class Determine whether the following sets of vectors are linearly dependent "],["section-linear-transformations.html", "Chapter 7 Linear Transformations 7.1 Linear Transformations 7.2 Types of matrix transformations 7.3 Properties of matrix transformations", " Chapter 7 Linear Transformations library(tidyverse) library(dasc2594) library(gifski) It is often useful to think of \\(\\mathbf{A}\\mathbf{x}\\) as a linear transformation defined by the matrix \\(\\mathbf{A}\\) applied to the vector \\(\\mathbf{x}\\). A linear transformation is mathematically defined as a function/mapping \\(T(\\cdot)\\) (\\(T\\) for transformation) from a domain in \\(\\mathcal{R}^n\\) (function input) to a codomain in \\(\\mathcal{R}^m\\) (function output). In shorthand, this is written as \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) which is read a \"\\(T\\) maps inputs from the domain \\(\\mathcal{R}^n\\) to the codomain \\(\\mathcal{R}^m\\). For each \\(\\mathbf{x} \\in \\mathcal{R}^n\\) (in the domain), \\(T(\\mathbf{x}) \\in \\mathcal{R}^m\\) is known as the image of \\(\\mathbf{x}\\). The set of all \\(T(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathcal{R}^n\\) is known as the range of \\(T(\\mathbf{x})\\). Note that it is possible that the range of \\(T(\\mathbf{x})\\) is not required to be the entire space \\(\\mathcal{R}^m\\) (i.e., the range of the transformation \\(T\\) might be a subset of \\(\\mathcal{R}^m\\)) Draw figure In the case of matrix transformations (linear transformations), the function \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x}\\) where \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix and \\(\\mathbf{x} \\in \\mathcal{R}^n\\) is a \\(n\\)-vector. Question: What kind of object is \\(\\mathbf{A} \\mathbf{x}\\)? scalar vector matrix array Question What are the dimensions of \\(\\mathbf{A} \\mathbf{x}\\)? Using the matrix transformation notation, the domain of the transformation \\(T\\) is \\(\\mathcal{R}^n\\), the codomain of \\(\\mathcal{T}\\) \\(\\mathcal{R}^m\\). The range of the transformation \\(T\\) is the set of all linear combinations of the columns of \\(\\mathbf{A}\\) (the \\(\\mbox{span}\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}\\)) because the transformation \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x}\\) is a linear combination \\(\\sum_{i=1}^n x_i \\mathbf{a}_i\\) of the columns \\(\\{\\mathbf{a}_i\\}_{i=1}^n\\) of \\(\\mathbf{A}\\) with coefficients \\(x_1, \\ldots, x_n\\) Example: \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} 2 &amp; 4 \\\\ -3 &amp; 1 \\\\ -1 &amp; 6 \\end{pmatrix} &amp;&amp; \\mathbf{u} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} &amp;&amp; \\mathbf{b} = \\begin{pmatrix} -2 \\\\ -11 \\\\ -15 \\end{pmatrix} &amp;&amp; \\mathbf{c} = \\begin{pmatrix} 2 \\\\ -2 \\\\ -1 \\end{pmatrix} \\end{align*} \\] ## [,1] ## [1,] 14 ## [2,] 0 ## [3,] 17 ## b ## [1,] 1 0 3 ## [2,] 0 1 -2 ## [3,] 0 0 0 ## c ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Find the image of \\(\\mathbf{u}\\) using the matrix transformation \\(T\\) (e.g., calculate \\(T(\\mathbf{u})\\)). Find a coefficient vector \\(\\mathbf{x} \\in \\mathcal{R}^2\\) such that \\(T(\\mathbf{x})\\). Is there more than one \\(\\mathbf{x}\\) whose image under \\(T\\) is \\(\\mathbf{b}?\\) In other words, is the solution \\(\\mathbf{A} \\mathbf{x}= \\mathbf{b}\\) unique? Determine if \\(\\mathbf{c}\\) is in the range of \\(T\\). In other words, does the solution \\(\\mathbf{A} \\mathbf{x}= \\mathbf{c}\\) exist? 7.1 Linear Transformations Definition: A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is linear if \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) for all \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in the domain of \\(T\\) \\(T(c \\mathbf{u}) = c T(\\mathbf{u})\\) for all scalars \\(c\\) and all vectors \\(\\mathbf{u}\\) in the domain of \\(T\\) Note: Because a linear transformation is equivalent to a matrix transformation, the definition above is equivalent to the following matrix-vector multiplication properties If \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^m\\) and \\(c\\) is a scalar, then \\(\\mathbf{A} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{A} \\mathbf{u} + \\mathbf{A} \\mathbf{v}\\) \\(\\mathbf{A} (c \\mathbf{u}) = (c \\mathbf{A}) \\mathbf{u}\\) As a consequence of the previous definition, the following properties hold for scalars \\(c\\) and \\(d\\) and vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v} \\in \\mathcal{R}^m\\) \\(T(\\mathbf{0}) = \\mathbf{0}\\) \\(T(c \\mathbf{u} + d \\mathbf{v}) = c T(\\mathbf{u}) + d T(\\mathbf{v})\\) Show why in class These properties give rise to the following statement for scalars \\(c_1, \\ldots, c_m\\) and vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_m \\in \\mathcal{R}^n\\) \\(T(c_1 \\mathbf{u}_1 + \\ldots + c_m \\mathbf{u}_m) = c_1 T(\\mathbf{u}_1) + \\ldots + c_m T(\\mathbf{u}_m)\\) The statements above for linear transformations are equivalent to the matrix statements where \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^m\\) and \\(c\\) is a scalar: \\(\\mathbf{A} \\mathbf{0} = \\mathbf{0}\\) \\(\\mathbf{A}(c \\mathbf{u} + d \\mathbf{v}) = c \\mathbf{A} \\mathbf{u} + d \\mathbf{A} \\mathbf{v}\\) And for a \\(m \\times n\\) matrix \\(\\mathbf{A}\\), scalars \\(c_1, \\ldots, c_m\\), and vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_m \\in \\mathcal{R}^n\\) \\(\\mathbf{A}(c_1 \\mathbf{u}_1 + \\ldots + c_m \\mathbf{u}_m) = c_1 \\mathbf{A}\\mathbf{u}_1 + \\ldots + c_m \\mathbf{A} \\mathbf{u}_m\\) 7.2 Types of matrix transformations The basic types of matrix transformations include contractions/expansions rotations reflections shears projections For the following examples, we will consider the unit vectors \\(\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\) and apply different linear transformations using the matrix \\(\\mathbf{A}\\). To build the matrix transformations, we use the dasc2594 package and build matrix transformations based on code from https://www.bryanshalloway.com/2020/02/20/visualizing-matrix-transformations-with-gganimate/. 7.2.1 Contractions/Expansions 7.2.1.1 Horizonal Expansion The matrix below gives a horizontal expansion when \\(x &gt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} x &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] In the example below, we set \\(x = 2\\) and generate the transformation. transformation_matrix &lt;- tribble( ~ x, ~ y, 2, 0, 0, 1) %&gt;% as.matrix() p &lt;- plot_transformation(transformation_matrix) 7.2.1.2 Horizonal Contraction The matrix below gives a horizontal contraction when \\(x &lt; 1\\) * Horizontal contraction when \\(x &lt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} x &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] In the example below, we set \\(x = 0.5\\) 7.2.1.3 Vertical Expansion The matrix below gives a vertical expansion when \\(x &gt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; x \\end{pmatrix} \\] * In the example below, we set \\(x = 2\\) 7.2.1.4 Vertical Contraction The matrix below gives a vertical contraction when \\(x &lt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; x \\end{pmatrix} \\] In the example below, we set \\(x = 0.5\\) 7.2.2 Rotations 7.2.2.1 Rotation by 90 degrees Rotations in 2D of an angle \\(\\theta \\in [0, 2\\pi]\\) take the form of \\[ \\mathbf{A} = \\begin{pmatrix} \\cos(\\theta) &amp; -\\sin(\\theta) \\\\ \\sin(\\theta) &amp; \\cos(\\theta) \\end{pmatrix} \\] For example, a rotation of 90 degrees counter-clockwise (\\(\\theta = \\frac{\\pi}{2}\\)) is given by the transformation matrix \\[ \\mathbf{A} = \\begin{pmatrix} \\cos(\\frac{\\pi}{2}) &amp; -\\sin(\\frac{\\pi}{2}) \\\\ \\sin(\\frac{\\pi}{2}) &amp; \\cos(\\frac{\\pi}{2}) \\end{pmatrix} = \\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] Another example is for a rotation of 45 degrees clockwise (\\(\\theta = -\\frac{\\pi}{4}\\)) is given by the transformation matrix \\[ = \\begin{pmatrix} \\cos(\\frac{\\pi}{4}) &amp; -\\sin(\\frac{\\pi}{4}) \\\\ \\sin(\\frac{\\pi}{4}) &amp; \\cos(\\frac{\\pi}{4}) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{2}}{2} &amp; -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} &amp; \\frac{\\sqrt{2}}{2} \\end{pmatrix} \\] 7.2.3 Reflections 7.2.3.1 Reflection across the x-axis The matrix below gives a reflection about the x-axis \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} \\] 7.2.3.2 Reflection across the y-axis The matrix below gives a reflection about the y-axis \\[ \\mathbf{A} = \\begin{pmatrix} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] 7.2.3.3 Reflection across the line y = x \\[ \\mathbf{A} = \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] In the example below, we set \\(x = 0.5\\) 7.2.3.4 Reflection across the line y = - x \\[ \\mathbf{A} = \\begin{pmatrix} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{pmatrix} \\] 7.2.3.5 Reflection across the origin (0, 0) \\[ \\mathbf{A} = \\begin{pmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} \\] 7.2.4 Shears A shear transformation is like stretching play-dough if it was possible to stretch all parts of the dough uniformly (rather than some sections getting stretched more than others). 7.2.4.1 Horizontal Shear \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; x \\\\ 0 &amp; 1 \\end{pmatrix} \\] For the example below, we plot a horizontal shear with \\(x = 2\\). 7.2.4.2 Vertical Shear \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; x \\\\ 0 &amp; 1 \\end{pmatrix} \\] For the example below, we plot a horizontal shear with \\(x = 2\\). 7.2.5 Projections 7.2.6 Identity 7.3 Properties of matrix transformations One-to-one Onto "],["section-solving-linear-equations-mathbfa-mathbfx-mathbfb.html", "Chapter 8 Solving linear equations \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\)", " Chapter 8 Solving linear equations \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) "],["section-inner-products-and-norms.html", "Chapter 9 Inner Products and Norms", " Chapter 9 Inner Products and Norms "],["section-projections-orthogonal-projections-and-least-squares.html", "Chapter 10 Projections: Orthogonal Projections and Least Squares", " Chapter 10 Projections: Orthogonal Projections and Least Squares "],["section-matrix-decompositoins-eigen-singular-value-qr-and-cholesky.html", "Chapter 11 Matrix Decompositoins: Eigen, Singular Value, QR, and Cholesky", " Chapter 11 Matrix Decompositoins: Eigen, Singular Value, QR, and Cholesky "],["section-limits-continuity-and-partial-derivatives.html", "Chapter 12 Limits, Continuity, and Partial Derivatives", " Chapter 12 Limits, Continuity, and Partial Derivatives "],["section-matrix-calculus-gradients-the-chain-rule-directional-derivatives-and-tangent-planes.html", "Chapter 13 Matrix Calculus: Gradients, the Chain Rule, Directional Derivatives, and Tangent Planes", " Chapter 13 Matrix Calculus: Gradients, the Chain Rule, Directional Derivatives, and Tangent Planes "],["section-tangent-planes-and-taylor-linearization.html", "Chapter 14 Tangent Planes and Taylor Linearization", " Chapter 14 Tangent Planes and Taylor Linearization "],["section-double-and-triple-integrals-and-change-of-variables-jacobian.html", "Chapter 15 Double and Triple Integrals and Change of Variables (Jacobian)", " Chapter 15 Double and Triple Integrals and Change of Variables (Jacobian) "],["section-references.html", "References", " References "]]
