[["index.html", "Multivariable Mathematics for Data Science Chapter 1 Preface 1.1 Getting started in R 1.2 Notation", " Multivariable Mathematics for Data Science John Tipton 2021-01-22 Chapter 1 Preface This book will introduce students to multivariable Calculus and linear algebra methods and techniques to be successful in data science, statistics, computer science, and other data-driven, computational disciplines. The motiviation for this text is to provide both a theoretical understanding of important multivariable methods used in data science as well as giving a hands-on experience using software. Throughout this text, we assume the reader has a solid foundation in univariate calculus (typically two semesters) as well as familiarity with a scripting language (e.g., R or python). 1.1 Getting started in R TBD 1.2 Notation For notation, we let lowercase Roman letters represent scalar numbers (e.g., n = 5, d = 7), lowercase bold letters represent vectors \\[ \\begin{align*} \\textbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\end{align*} \\] where the elements \\(x_1, \\ldots, x_n\\) are scalars written in lowercase Roman. Note that vectors are assumed to follow a vertical notation where the elements of the vector (the \\(x_i\\)s are stacked on top of one another) and the order matters. For example, the vector \\[ \\begin{align*} \\mathbf{x} &amp; = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix} \\end{align*} \\] has the first element \\(x_1 = 5\\), second element \\(x_2 = 2\\) and third element \\(x_3 = 8\\). Note that the vector \\(\\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) is not the same as the vector \\(\\begin{pmatrix} 8 \\\\ 2 \\\\ 5 \\end{pmatrix}\\) because the order of the elements matters. We can also write the vector as \\[ \\begin{align*} \\textbf{x} = \\left( x_1, x_2, \\ldots, x_n \\right)&#39;, \\end{align*} \\] where the \\(&#39;\\) symbol represents the transpose function. For our example matrix, we have \\(\\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}&#39; = \\begin{pmatrix} 5 &amp; 2 &amp; 8 \\end{pmatrix}\\) which is the original vector but arranged in a row rather than a column. Likewise, the transpose of a row vector \\(\\begin{pmatrix} 5 &amp; 2 &amp; 8 \\end{pmatrix}&#39; = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) is a column vector. If \\(\\mathbf{x}\\) is a column vector, we say that \\(\\mathbf{x}&#39;\\) is a row vector and if \\(\\mathbf{x}\\) is a row vector, the \\(\\mathbf{x}&#39;\\) is a column vector. To create a vector we can use the concatenate function c(). For example, the vector \\(\\mathbf{x} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) can be created as the R object using x &lt;- c(5, 2, 8) where the &lt;- assigns the values in the vector c(5, 2, 8) to the object named x. To print the values of x, we can use x ## [1] 5 2 8 which prints the elements of x. Notice that R prints the elements of \\(\\mathbf{x}\\) in a row; however, \\(\\mathbf{x}\\) is a column vector. This inconsistency is present to allow the output to be printed in a manner easier to read (more numbers fit on a row). If we put the column vector into a data.frame, then the vector will be presented as a column vector data.frame(x) ## x ## 1 5 ## 2 2 ## 3 8 One can use the index operator \\([\\hspace{2mm}]\\) to select specific elements of the vector \\(\\mathbf{x}\\). For example, the first element of \\(\\mathbf{x}\\), \\(x_1\\), is x[1] ## [1] 5 and the third element of \\(\\mathbf{x}\\), \\(x_3\\), is x[3] ## [1] 8 The transpose function t() turns a column vector into a row vector (or a row vector into a column vector). For example the transpose \\(\\mathbf{x}&#39;\\) of \\(\\mathbf{x}\\) is tx &lt;- t(x) tx ## [,1] [,2] [,3] ## [1,] 5 2 8 where tx is R object storing the transpose of \\(\\mathbf{x}\\) and is a row vector. The transpose of tx. Notice the indices on the output of the row vector tx. The index operator [1, ] selects the first row to tx and the index operator [, 1] gives the first column tx. Taking the transpose again gives us back the original column vector t(tx) ## [,1] ## [1,] 5 ## [2,] 2 ## [3,] 8 1.2.1 Matrices We let uppercase bold letters \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), etc., represent matrices. We define the matrix \\(\\mathbf{A}\\) with \\(m\\) rows and \\(n\\) columns as \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}, \\end{align*} \\] with \\(a_{ij}\\) being the value of the matrix \\(\\mathbf{A}\\) in the \\(i\\)th row and the \\(j\\)th column. If the matrix \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} 5 &amp; 7 &amp; 1 \\\\ 5 &amp; -22 &amp; 2 \\\\ -14 &amp; 5 &amp; 99 \\\\ 42 &amp; -3 &amp; 0\\end{pmatrix}, \\end{align*} \\] the elements \\(a_{11}\\) = 5, \\(a_{12}\\) = 7, \\(a_{21}\\) = 5, and \\(a_{33}\\) = 99, etc. In R, we can define the matrix A using the matrix() function A &lt;- matrix( data = c(5, 5, -14, 42, 7, -22, 5, -3, 1, 2, 99, 0), nrow = 4, ncol = 3 ) A ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 Notice in the above creation of \\(\\mathbf{A}\\), we wrote defined the elements of the \\(\\mathbf{A}\\) using the columns stacked on top of one another. If we want to fill in the elements of \\(\\mathbf{A}\\) using the rows, we can add the option byrow = TRUE to the matrix() function A &lt;- matrix( data = c(5, 7, 1, 5, -22, 2, -14, 5, 99, 42, -3, 0), nrow = 4, ncol = 3, byrow = TRUE ) A ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 To select the \\(ij\\)th elements of \\(\\mathbf{A}\\), we use the subset operator [ to select the element. For example, to get the element \\(a_{11} = 5\\) in the first row and first column of \\(\\mathbf{A}\\), we use A[1, 1] ## [1] 5 The element \\(a_{3, 3} = 99\\) in the third row and third column can be selected using A[3, 3] ## [1] 99 The matrix \\(\\mathbf{A}\\) can also be represented as a set of either column vectors \\(\\{\\mathbf{c}_j \\}_{j=1}^n\\) or row vectors \\(\\{\\mathbf{r}_i \\}_{i=1}^m\\). For example, the column vector representation is \\[ \\begin{align*} \\mathbf{A} &amp; = \\left( \\mathbf{c}_{1} \\middle| \\mathbf{c}_{2} \\middle| \\cdots \\middle| \\mathbf{c}_{n} \\right), \\end{align*} \\] where the notation \\(|\\) is used to separate the vectors \\[ \\begin{align*} \\mathbf{c}_1 &amp; = \\begin{pmatrix} a_{11} \\\\ a_{12} \\\\ \\vdots \\\\ a_{1m} \\end{pmatrix}, &amp; \\mathbf{c}_2 &amp; = \\begin{pmatrix} a_{21} \\\\ a_{22} \\\\ \\vdots \\\\ a_{2m} \\end{pmatrix}, &amp; \\cdots, &amp; &amp; \\mathbf{c}_n &amp; = \\begin{pmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\end{pmatrix} \\end{align*} \\] In R you can extract the columns using the [ selection operator c1 &lt;- A[, 1] # first column c2 &lt;- A[, 2] # second column c3 &lt;- A[, 3] # third column and you can give the column representation of the matrix A with with column bind function cbind() cbind(c1, c2, c3) ## c1 c2 c3 ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 The row vector representation of \\(\\mathbf{A}\\) is \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{r}_{1} \\\\ \\mathbf{r}_{2} \\\\ \\vdots \\\\ \\mathbf{r}_{m} \\end{pmatrix}, \\end{align*} \\] where the row vectors \\(\\mathbf{r}_i\\) are \\[ \\begin{align*} \\mathbf{r}_1 &amp; = \\left( a_{11}, a_{12}, \\ldots, a_{1n} \\right) \\\\ \\mathbf{r}_2 &amp; = \\left( a_{21}, a_{22}, \\ldots, a_{2n} \\right) \\\\ &amp; \\vdots \\\\ \\mathbf{r}_m &amp; = \\left( a_{m1}, a_{m2}, \\ldots, a_{mn} \\right) \\end{align*} \\] In R you can extract the rows using the [ selection operator r1 &lt;- A[1, ] # first row r2 &lt;- A[2, ] # second row r3 &lt;- A[3, ] # third row r4 &lt;- A[4, ] # fourth row and you can give the row representation of the matrix A with with row bind function rbind() rbind(r1, r2, r3, r4) ## [,1] [,2] [,3] ## r1 5 7 1 ## r2 5 -22 2 ## r3 -14 5 99 ## r4 42 -3 0 "],["section-linear-systems-of-equations.html", "Chapter 2 Linear Systems of Equations 2.1 Linear Systems of equations 2.2 Reduce row echelon form", " Chapter 2 Linear Systems of Equations library(tidyverse) # For 3-d plotting # if devtools package not installed, install the package if (!require(devtools)) { install.packages(&quot;devtools&quot;) } # if gg3D package not installed, install the package if (!require(gg3D)) { devtools::install_github(&quot;AckerDWM/gg3D&quot;) library(gg3D) } ## Warning in fun(libname, pkgname): no display name and no $DISPLAY environment ## variable # if dasc2594 package not installed, install the package if (!require(dasc2594)) { devtools::install_github(&quot;jtipton25/dasc2594&quot;) library(dasc2594) } 2.1 Linear Systems of equations 2.1.1 Linear equations Let \\(x_1, x_2, \\ldots, x_n\\) be variables with coefficients \\(a_1, a_2, \\ldots, a_n\\), and \\(b\\) are fixed and known numbers. Then, we say \\[ \\begin{align} \\tag{2.1} a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n &amp; = b \\end{align} \\] is a linear equation. For example, the equation for a line with slope \\(m\\) and \\(y\\)-intercept \\(b\\) is \\[ \\begin{align*} y &amp; = m x + b, \\end{align*} \\] is a linear equation because it can be re-written as \\[ \\begin{align*} y - m x &amp; = b, \\end{align*} \\] where \\(a_1 = 1\\), \\(a_2 = m\\), \\(x_1 = y\\) and \\(x_2 = x\\). The equations \\[ \\begin{align*} \\sqrt{19} x_1 &amp; = (4 + \\sqrt{2}) x_2 - x_3 - 9 &amp; \\mbox{ and } &amp;&amp; -4 x_1 + 5 x_2 - 11 &amp; = x_3 \\end{align*} \\] are both linear equations because they can be written as \\[ \\begin{align*} \\sqrt{19} x_1 - (4 + \\sqrt{2}) x_2 + x_3 &amp; = - 9 &amp; \\mbox{ and } &amp;&amp; -4 x_1 + 5 x_2 - x_3 &amp; = 11, \\end{align*} \\] respectively. The equations \\[ \\begin{align*} x_1 &amp; = x_2^2 + 3 &amp; \\mbox{ and } &amp;&amp; x_1 + x_2 - x_1 x_2 &amp; = 16 \\end{align*} \\] are not linear equations because they do not meet the form of (2.1) (The first equation above has a quadratic power of \\(x_2\\) and the second equation has a product of \\(x_1\\) and \\(x_2\\)). 2.1.2 Systems of linear equations A set of two or more linear equations that each contain the same set of variables is called a system of linear equations. The equations \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; &amp;&amp; &amp;&amp; = &amp; 9 \\end{align*} \\] are a system of equations. Note that in the second equation, the coefficient for \\(x_3\\) is 0, meaning we could re-write the above example as \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; + &amp;&amp; 0 x_3 &amp;&amp; = &amp; 9. \\end{align*} \\] Exercises: For the following, are these linear equations? \\(x_1 + 3 x_1 x_2 = 5\\) \\(5x + 7y + 8z = 11.2\\) \\(y / 4 + \\sqrt{2} z = 2^6\\) \\(x + 4 y^2 = 9\\) 2.1.3 Solutions of linear systems A fundamental question when presented with a linear system of equations is whether the system has a solution. A solution to a system means that there are numbers \\((s_1, s_2, \\ldots, s_n)\\) that each of the variables \\(x_1, x_2, \\ldots, x_n\\) take that allow for all the equations to simultaneously be true. For example, consider the system of equations \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 5 y &amp;&amp; = &amp; 7 \\end{align*} \\] To find if a solution to this equation exists, we can do some algebra and take 4 times the top equation and then subtract the bottom equation, replacing the bottom equation with this new sum like \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x - 4 * (x) &amp;&amp; + &amp;&amp; 5 y - 4 * (4y) &amp;&amp; = &amp; 7 - 4 * (8) \\end{align*} \\] where the part of the equations in () is the top equation. This system of equations now simplifies to \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 0 &amp;&amp; + &amp;&amp; -11 y &amp;&amp; = &amp; -25 \\end{align*} \\] which gives \\(y = \\frac{25}{11}\\). Plugging this value into the top equation gives \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 * \\frac{25}{11} &amp;&amp; = &amp; 8 \\\\ 0 &amp;&amp; + &amp;&amp; y &amp;&amp; = &amp; \\frac{25}{11} \\end{align*} \\] where we can solve \\(x = 8 - \\frac{100}{11} = -\\frac{12}{11}\\) giving the solution of the form \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 0 &amp;&amp; = &amp; - \\frac{12}{11} \\\\ 0 &amp;&amp; + &amp;&amp; y &amp;&amp; = &amp; \\frac{25}{11}. \\end{align*} \\] In this case, the system of equation has the solution \\(x = -\\frac{12}{11}\\) and \\(y = \\frac{25}{11}\\). While finding the solution can be done algebraically, what does this mean visually (geometrically)? The original equations were \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 5 y &amp;&amp; = &amp; 7 \\end{align*} \\] which define two lines: \\(y = -\\frac{x}{4} + 2\\) \\(y = -\\frac{4x}{5} + \\frac{7}{5}\\) Let’s plot these equations in R and see what they look like # define some grid points to evaluate the line x &lt;- seq(-2, 2, length = 1000) dat &lt;- data.frame( x = c(x, x), y = c(-x / 4 + 2, - 4 / 5 * x + 7/5), equation = factor(rep(c(1, 2), each = 1000)) ) glimpse(dat) ## Rows: 2,000 ## Columns: 3 ## $ x [3m[90m&lt;dbl&gt;[39m[23m -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.97… ## $ y [3m[90m&lt;dbl&gt;[39m[23m 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, … ## $ equation [3m[90m&lt;fct&gt;[39m[23m 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … dat %&gt;% ggplot(aes(x = x, y = y, color = equation, group = equation)) + geom_line() + scale_color_viridis_d(end = 0.8) + # solution x = -12/11, y = 25/11 geom_point(aes(x = -12/11, y = 25/11), color = &quot;red&quot;, size = 2) + ggtitle(&quot;Linear system of equations&quot;) Figure 2.1: Linear system of equations with one solution From this plot, it is clear that the solution to the system of equations is the location where the two lines intersect! 2.1.4 Types of solutions Typically, there are 3 cases for the solutions to a system of linear equations There are no solutions There is one solution (Figure 2.1) There are infinitely many solutions Definition 2.1 A linear system of equations is called consistent if the system has either one or infinitely many solutions and is called inconsistent if the system has no solution. 2.1.4.1 There are no solutions: Consider the system of linear equations \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 16 y &amp;&amp; = &amp; 18 \\end{align*} \\] # define some grid points to evaluate the line x &lt;- seq(-2, 2, length = 1000) dat &lt;- data.frame( x = c(x, x), y = c(-x / 4 + 8 / 4, - x / 4 + 18 / 4), equation = factor(rep(c(1, 2), each = 1000)) ) glimpse(dat) ## Rows: 2,000 ## Columns: 3 ## $ x [3m[90m&lt;dbl&gt;[39m[23m -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.97… ## $ y [3m[90m&lt;dbl&gt;[39m[23m 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, … ## $ equation [3m[90m&lt;fct&gt;[39m[23m 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … dat %&gt;% ggplot(aes(x = x, y = y, color = equation, group = equation)) + geom_line() + scale_color_viridis_d(end = 0.8) + # solution x = -12/11, y = 25/11 ggtitle(&quot;Linear system of equations&quot;) Figure 2.2: Linear system of equations with no solution In this case, the linear equations are parallel lines and will never intersect so therefore there is no solution. 2.1.4.2 There is one solution: We have seen this example in Figure 2.1 2.1.4.3 There are infinitely many solutions: Consider the system of linear equations \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 16 y &amp;&amp; = &amp; 32 \\end{align*} \\] # define some grid points to evaluate the line x &lt;- seq(-2, 2, length = 1000) dat &lt;- data.frame( x = c(x, x), y = c(-x / 4 + 8 / 4, - 4 * x / 16 + 32 / 16), equation = factor(rep(c(1, 2), each = 1000)) ) glimpse(dat) ## Rows: 2,000 ## Columns: 3 ## $ x [3m[90m&lt;dbl&gt;[39m[23m -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.97… ## $ y [3m[90m&lt;dbl&gt;[39m[23m 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, … ## $ equation [3m[90m&lt;fct&gt;[39m[23m 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … dat %&gt;% ggplot(aes(x = x, y = y, color = equation, group = equation)) + geom_line() + scale_color_viridis_d(end = 0.8) + # solution x = -12/11, y = 25/11 ggtitle(&quot;Linear system of equations&quot;) Figure 2.3: Linear system of equations with no solution In this case, the linear equations are perfectly overlapping lines and always intersect so therefore there are infinitely many solutions (all points on the line). Definition 2.2 Two linear systems of equations are called equivalent if both systems share the same solution set. For example, the system of equations \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; + &amp;&amp; 2 x_3 &amp;&amp; = &amp; 9. \\end{align*} \\] and the system of equations \\[ \\begin{align*} 2x_1 &amp;&amp; + &amp;&amp; 8 x_2 &amp;&amp; - &amp;&amp; 2 x_3 &amp;&amp; = &amp; 22 \\\\ 8 x_1 &amp;&amp; + &amp;&amp; 10 x_2 &amp;&amp; + &amp;&amp; 4 x_3 &amp;&amp; = &amp; 18. \\end{align*} \\] have the same solution set (the second set of equations is just 2 times the first set of equations). Exercise/Lab: generate some equations, plot them, and determine if there is a solution. Then try to solve these using algebra. Exercises: for the following systems of equations, determine if a solution(s) exist and if so, solve for the solution \\[\\begin{align*} 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; = 8\\\\ 9 x_1 &amp;&amp; - &amp;&amp; 3 x_2 &amp;&amp; = 4 \\end{align*}\\] \\[\\begin{align*} 7 x_1 &amp;&amp; + &amp;&amp; 3 x_2 &amp;&amp; + &amp;&amp; 4 x_3 &amp;&amp; = 5\\\\ 4 x_1 &amp;&amp; - &amp;&amp; 5 x_2 &amp;&amp; &amp;&amp; &amp;&amp; = -2 \\end{align*}\\] \\[\\begin{align*} 4 x_1 &amp;&amp; - &amp;&amp; 2 x_2 &amp;&amp; = 8\\\\ 2 x_1 &amp;&amp; + &amp;&amp; x_2 &amp;&amp; = 7 \\\\ -3 x_1 &amp;&amp; + &amp;&amp; 6 x_2 &amp;&amp; = 11 \\end{align*}\\] 2.1.5 Elementary row and column operations on matrices The elementary row (column) operations include swaps: swapping two rows (columns), sums: replacing a row (column) by the sum itself and a multiple of another row (column) scalar multiplication: replacing a row (column) by a scalar multiple times itself Note that these operations are exactly what we used to solve the equation using algebra above (except for swapping rows). Add in examples in class here 2.1.6 The Augmented matrix form of a system of equations Consider the linear system of equations \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; + &amp;&amp; 2 x_3 &amp;&amp; = &amp; 9. \\end{align*} \\] The augmented matrix representation of this system of linear equations is given by the matrix \\[ \\begin{align*} \\begin{pmatrix} 1 &amp; 4 &amp; - 1 &amp; 11 \\\\ 4 &amp; 5 &amp; 2 &amp; 9 \\end{pmatrix}, \\end{align*} \\] where the first column of the matrix represents the variable \\(x_1\\), the second column of the matrix represents the variable \\(x_2\\), the third column of the matrix represents the variable \\(x_3\\), and the fourth column of the matrix represents the constant terms. We can express the augmented form in R using a matrix augmented_matrix &lt;- matrix(c(1, 4, 4, 5, -1, 2, 11, 9), 2, 4) augmented_matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 4 -1 11 ## [2,] 4 5 2 9 and to make clear the respective variables, we can add in column names as a matrix attribute using the colnames() function colnames(augmented_matrix) &lt;- c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;constants&quot;) augmented_matrix ## x1 x2 x3 constants ## [1,] 1 4 -1 11 ## [2,] 4 5 2 9 which adds labels to each of the columns. Now, using elementary row operations on the matrix, we can attempt to find solutions to the system of equations. First, we multiply the first row by -4 and add it to the second row of the matrix and replace the second row with this sum augmented_matrix[2, ] &lt;- -4 * augmented_matrix[1, ] + augmented_matrix[2, ] augmented_matrix ## x1 x2 x3 constants ## [1,] 1 4 -1 11 ## [2,] 0 -11 6 -35 Next, scale the second row to have a leading value of 1 by dividing by -11 augmented_matrix[2, ] &lt;- augmented_matrix[2, ] / (-11) augmented_matrix ## x1 x2 x3 constants ## [1,] 1 4 -1.0000000 11.000000 ## [2,] 0 1 -0.5454545 3.181818 We can then multiply the second row by -4 and add it to the first row and replace the first row with this value. augmented_matrix[1, ] &lt;- augmented_matrix[1, ] - 4 * augmented_matrix[2, ] augmented_matrix ## x1 x2 x3 constants ## [1,] 1 0 1.1818182 -1.727273 ## [2,] 0 1 -0.5454545 3.181818 Notice how the matrix has a “triangular” form (The lower part of the “triangle” is made of 0s and the upper part has numbers). The triangular form tells us that There are infinitely many solutions to this system of equation. The infinite solutions are subject to the requirements that \\[x_1 = - \\frac{19}{11} - \\frac{13}{11} x_3\\] and \\[x_2 = \\frac{35}{11} + \\frac{6}{11} x_3.\\] To get this into a reasonable form, we will solve these equations as a function of \\(x_1\\). Solving the first equation for \\(x_3\\) gives \\[x_3 = - \\frac{19}{13} -\\frac{11}{13} x_1.\\] Then, plugging this into \\(x_3\\) in the second equation gives \\[ \\begin{align*} x_2 &amp; = \\frac{35}{11} + \\frac{6}{11} \\left( - \\frac{19}{13} -\\frac{11}{13} x_1 \\right) \\\\ &amp; = \\frac{341}{143} - \\frac{6}{13} x_1 \\end{align*} \\] which defines a linear relationship between \\(x_1\\) and \\(x_2\\). Notice that in these last two solutions, \\(x_1\\) is a “free variable” and \\(x_2\\) and \\(x_3\\) are “determined” by \\(x_1\\). In the plot below, the two planes (red and blue) are the geometric plots of the linear equations in the system of equations (the red plane is the top equation and the blue plane is the bottom equation). The purple line is the equation for the solution given the free variable \\(x_3\\) and lies at the intersection of the two planes, much like the point in the two lines in figure linking reference here lies at the intersection of the two points. # uses gg3D library n &lt;- 60 x1 &lt;- x2 &lt;- seq(-10, 10, length = n) region &lt;- expand.grid(x1 = x1, x2 = x2) df &lt;- data.frame( x1 = region$x1, x2 = region$x2, x3 = - 11 + (region$x1 + 4 * region$x2) ) df2 &lt;- data.frame( x1 = region$x1, x2 = region$x2, x3 = (9 - 4 * region$x1 - 5 * region$x2) / 2 ) df_solution &lt;- data.frame( x1 = x1, x2 = 341 / 143 - 6 / 13 * x1, x3 = -19/13 - 11/13 * x1 ) # theta and phi set up the &quot;perspective/viewing angle&quot; of the 3D plot theta &lt;- 63 phi &lt;- -12 ggplot(df, aes(x1, x2, z = x3)) + axes_3D(theta = theta, phi = phi) + stat_wireframe(alpha = 0.25, color = &quot;red&quot;, theta = theta, phi = phi) + stat_wireframe(data = df2, aes(x = x1, y = x2, z = x3), alpha = 0.25, color = &quot;blue&quot;, theta = theta, phi = phi) + stat_3D(data = df_solution, aes(x1, x2, z = x3), geom = &quot;line&quot;, theta = theta, phi = phi, color = &quot;purple&quot;) + theme_void() + theme(legend.position = &quot;none&quot;) + labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), angle=c(0, 0, 90), theta = theta, phi = phi) ## Warning: Removed 2 row(s) containing missing values (geom_path). ## Warning: Removed 2 row(s) containing missing values (geom_path). 2.1.7 Existence and Uniqueness Definition 2.3 A system of linear equations is said to be consistent if at least one solution exists. The linear system of equations is said to have a unique solution if only one solution exists. Example: (do in class) Is the system of linear equations consistent? IF the system is consistent, does it have a unique solution? \\[ \\begin{align*} 16 x_1 &amp;&amp; + &amp;&amp; 2 x_2 &amp;&amp; + &amp;&amp; 3 x_3 &amp;&amp; = &amp; 13 \\\\ 5 x_1 &amp;&amp; + &amp;&amp; 11 x_2 &amp;&amp; + &amp;&amp; 10 x_3 &amp;&amp; = &amp; 8 \\\\ 9 x_1 &amp;&amp; + &amp;&amp; 7 x_2 &amp;&amp; + &amp;&amp; 6 x_3 &amp;&amp; = &amp; 12 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 14 x_2 &amp;&amp; + &amp;&amp; 15 x_3 &amp;&amp; = &amp; 1 \\\\ \\end{align*} \\] Example: (do in class) Is the system of linear equations consistent? IF the system is consistent, does it have a unique solution? \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 2 x_2 &amp;&amp; + &amp;&amp; 3 x_3 = &amp; 5 \\\\ x_1 &amp;&amp; + &amp;&amp; 3 x_2 &amp;&amp; + &amp;&amp; 2 x_3= &amp; 2 \\\\ 3 x_1 &amp;&amp; + &amp;&amp; 2 x_2 &amp;&amp; + &amp;&amp; x_3 = &amp; 7 \\end{align*} \\] 2.2 Reduce row echelon form Reducing a matrix to row echelon form is a useful technique for working with matrices. The row echelon form can be used to solve systems of equations, as well as determine other properties of a matrix that are yet to be discussed, including rank, invertibility, column/row spaces, etc. Definition 2.4 A matrix is said to be in echelon form if all nonzero rows are above any rows of zeros (all rows consisting entirely of zeros are at the bottom) the leading entry/coefficient of a nonzero row (called the pivot) is always strictly to the right of the leading entry/coefficient of the row above Example: echelon matrix example in class Definition 2.5 A matrix is in reduced row echelon form if it is in echelon form and the leading entry/coefficient of each row is 1 The leading entry/coefficient of 1 is the only nonzero entry in its column. Example: rref matrix example in class Definition 2.6 Echelon matrices have the property of being upper diagonal. A matrix is said to be upper diagonal if all entries of the matrix at or above the diagonal are nonzero. Example: **lower and non-lower diagonal matrices Definition 2.7 Two matrices are row-equivalent if one matrix can be transformed to the other through elementary row operations. Theorem 2.1 A nonzero matrix can be transformed into more than one echelon forms. However, the reduced row echelon form of a nonzero matrix is unique. Exercise: using elementary row operations, calculate the reduced row echelon form of the following matrices fill in later fill in later fill in later 2.2.1 Pivot positions The leading entry/coefficients of a row echelon form matrix are called pivots. The positions of the pivot positions are the same for any row echelon form of a matrix. In reduced row echelon form, these pivot positions take the value 1. Definition 2.8 In a matrix that is in reduced echelon form, the pivot position is the first nonzero element of each row. The column in which the pivot position occurs is called a pivot column. Example: pivot position and pivot columns 2.2.2 Finding the reduced row echelon form Calculating the reduced row echelon form is known as Gaussian elimination, which is named after Johann Carl Friedrich Gauss. This algorithm uses elementary row operations to calculate the reduced row echelon form. The following steps perform the Gaussian elimination algorithm. Start with the left-most nonzero column, which is a pivot column If the top row is zero, swap rows so that the top row is nonzero so that the top row has a nonzero element in the pivot position. Use row multiplication and addition to zero out all positions in the pivot column below the top row (pivot position). Ignore this top row and repeat steps 1-3 until there are no more nonzero rows to apply steps 1-3 on. At the end of this step, the matrix is in row echelon form. Starting at the right-most pivot column, use elementary row operations to zero out all positions above each pivot and to make each pivot position 1. At the end of this step, the matrix is in reduced row echelon form. Example: in class # pracma library # rref example in class 2.2.3 Using reduced row echelon forms to solve systems of linear equations When a system of linear equations is expressed as an augmented matrix, the reduced row echelon form can be used to find solutions to those systems of equations. Consider the systems of equations \\[ \\begin{align*} 3x_1 &amp;&amp; + &amp;&amp; 8 x_2 &amp;&amp; - &amp;&amp; 4 x_3 &amp;&amp; = &amp; 6 \\\\ 2 x_1 &amp;&amp; - &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 8 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; &amp;&amp; &amp;&amp; = &amp; 9 \\end{align*} \\] which can be written in the augmented matrix form \\[ \\begin{pmatrix} 3 &amp; 8 &amp; -4 &amp; 6 \\\\ 2 &amp; -4 &amp; -1 &amp; 8 \\\\ 4 &amp; 5 &amp; 0 &amp; 9 \\end{pmatrix} \\] In R, this is the matrix # define matrix Calculating the reduced row echelon form, gives # calculate rref of augmented matrix which gives the solution … Exercise: calculate the RREF for the augmented matrix in the example above by hand Another example where we find a solution is \\[ \\begin{align*} 5 x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; 2 x_3 &amp;&amp; = &amp; 0 \\\\ -3 x_1 &amp;&amp; - &amp;&amp; 2 x_2 &amp;&amp; - &amp;&amp; 4 x_3 &amp;&amp; = &amp; 1 \\\\ \\end{align*} \\] Do same steps Definition 2.9 In a system of linear equations that is underdetermined (fewer equations than unknowns), the determined/basic variables are those variable that have a 1 in the respective columns when in reduced row echelon form (i.e., variables in a pivot position). The variables that are not in a pivot position are called free variable. Example in class 2.2.4 Existence and uniquenss from reduced row echelon form The row echelon form is useful to determine if a system of linear equations is consistent (the system of equations has a solution). To check if a solution to a linear system of equations exists, convert the system of equations to an augmented matrix form. Then, reduce the augmented matrix to row echelon form using elementary matrix operations. As long as there is not an equation of the form \\[ 0 = \\mbox{constant} \\] for some constant number not equal to 0, the system of linear equations is consistent. If the augmented matrix can be written in reduced row echelon form with no free variables, the solution to the linear system of equations is unique. These results give rise to the theorem Theorem 2.2 A linear system of equations is consistent (has a solution) if the furthest right column (the constant column) is not a pivot column. If the system of equations is consistent, (i.e., the furthest right column is not a pivot column), the solution is unique if there are no free variables and there are infinitely many solutions if there is at least one free variable. Example: consistent system of equations \\[\\begin{pmatrix} -7 &amp; -9 &amp; 7 &amp; 8 \\\\ -4 &amp; 0 &amp; 6 &amp; -6 \\\\ -10 &amp; 3 &amp; -8 &amp; 5 \\end{pmatrix}\\] Example: inconsistent system of equations \\[\\begin{pmatrix} -7 &amp; 0 &amp; -8 &amp; -5 \\\\ -4 &amp; 3 &amp; 8 &amp; -2 \\\\ -10 &amp; 7 &amp; -6 &amp; 4 \\\\ -9 &amp; 6 &amp; 5 &amp; 1 \\end{pmatrix}\\] "],["section-vector-spaces.html", "Chapter 3 Vectors spaces 3.1 Vectors 3.2 Vector addition 3.3 Span", " Chapter 3 Vectors spaces library(shiny) library(patchwork) library(tidyverse) # if gg3D package not installed, install the package library(gg3D) library(dasc2594) 3.1 Vectors 3.1.1 Properties of Vectors For any real valued scalars \\(a, b \\in \\mathcal{R}\\) and any vectors \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathcal{R}^n\\) (vectors of real numbers of length \\(n\\)), scalar multiplication \\[ \\begin{align*} a \\mathbf{x} &amp; = a \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} a x_1 \\\\ a x_2 \\\\ \\vdots \\\\ a x_n \\end{pmatrix} \\end{align*} \\] where the scalar \\(a\\) is multiplied by each element of the vector. For example, \\[ \\begin{align*} 4 \\begin{pmatrix} 4 \\\\ 6 \\\\ 7 \\\\ 12 \\end{pmatrix} &amp; = \\begin{pmatrix} 4 * 4 \\\\ 4 * 6 \\\\ 4 * 7 \\\\ 4 * 12 \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} 16 \\\\ 24 \\\\ 28 \\\\ 48 \\end{pmatrix} \\end{align*} \\] In R, we can multiply the vector by a scalar as 4 * c(4, 6, 7, 12) ## [1] 16 24 28 48 or if the vector \\(\\mathbf{x} = \\left( 4, 6, 7, 12 \\right)&#39;\\) we can write this as x &lt;- c(4, 6, 7, 12) 4 * x ## [1] 16 24 28 48 scalar multiplicative commutivity \\[ \\begin{align*} a (b \\mathbf{x}) &amp; = (ab) \\mathbf{x} &amp; = b (a \\mathbf{x}) \\end{align*} \\] 4 * (6 * x) ## [1] 96 144 168 288 (4 * 6) * x ## [1] 96 144 168 288 scalar additive associativity \\[ \\begin{align*} a \\mathbf{x} + b \\mathbf{x} &amp; = (a + b) \\mathbf{x} \\end{align*} \\] vector additive associativity \\[ \\begin{align*} a \\mathbf{x} + a \\mathbf{y} &amp; = a (\\mathbf{x} + \\mathbf{y}) \\end{align*} \\] vector associativity \\[ \\begin{align*} \\mathbf{x} + \\mathbf{y} &amp; = \\mathbf{y} + \\mathbf{x} \\end{align*} \\] \\[ \\begin{align*} (\\mathbf{x} + \\mathbf{y}) + \\mathbf{z} &amp; = \\mathbf{x} + (\\mathbf{y} + \\mathbf{z}) \\end{align*} \\] x &lt;- c(1, 2, 3, 4) y &lt;- c(4, 3, 5, 1) z &lt;- c(5, 2, 4, 6) x + y ## [1] 5 5 8 5 y + x ## [1] 5 5 8 5 (x + y) + z ## [1] 10 7 12 11 x + (y + z) ## [1] 10 7 12 11 Identity Element of Addition: For any vector \\(\\mathbf{x}\\) of length \\(n\\), there exists a vector \\(\\mathbf{0}\\), known as the zero vector, such that \\[ \\begin{align*} \\mathbf{x} + \\mathbf{0} &amp; = \\mathbf{x} \\end{align*} \\] x + 0 ## [1] 1 2 3 4 x + rep(0, 4) ## [1] 1 2 3 4 Inverse Element of Addition: For any vector \\(\\mathbf{x}\\) of length \\(n\\), there exists a vector \\(-\\mathbf{x}\\), known as the additive inverse vector, such that \\[ \\begin{align*} \\mathbf{x} + (- \\mathbf{x}) &amp; = \\mathbf{0} \\end{align*} \\] x + (-x) ## [1] 0 0 0 0 3.2 Vector addition Two vectors of length \\(n\\) can be added elementwise \\[ \\begin{align*} \\mathbf{x} + \\mathbf{y} &amp; = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} + \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix} \\end{align*} \\] For example, \\[ \\begin{align*} \\begin{pmatrix} 3 \\\\ 1 \\\\ -4 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} -3 \\\\ 17 \\\\ -39 \\\\ 4 \\end{pmatrix} &amp; = \\begin{pmatrix} 3 + (-3) \\\\ 1 + 17 \\\\ -4 + (-39) \\\\ 3 + 4 \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} 0 \\\\ 18 \\\\ -43 \\\\ 7 \\end{pmatrix} \\end{align*} \\] In R, we have x &lt;- c(3, 1, -4, 3) y &lt;- c(-3, 17, -39, 4) x + y ## [1] 0 18 -43 7 If two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are of different lengths, then they cannot be added together. Using R, we get the following error: x &lt;- c(1, 2, 3) y &lt;- c(1, 2, 3, 4) x + y ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 2 4 6 5 The error is telling us that the vector \\(\\mathbf{x}\\) and the vector \\(\\mathbf{y}\\) do not have the same length. Be careful when adding vectors in R. R uses “recycling” which means two vectors of different lengths can be added together if one vector is of a length that is a multiple of the other vector. For example, if \\(\\mathbf{x} = (1, 2)&#39;\\) is a vector of length 2 and \\(\\mathbf{y} = (1, 2, 3, 4)\\) is a vector of length 4, R will add \\(\\mathbf{x} + \\mathbf{y}\\) by replicating the vector \\(\\mathbf{x}\\) twice (i.e., \\(\\mathbf{x} + \\mathbf{y} = \\left( \\mathbf{x}&#39;, \\mathbf{x}&#39; \\right)&#39; = \\left(1, 2, 1, 2 \\right)&#39; + \\mathbf{y}\\)) x &lt;- c(1, 2) y &lt;- c(1, 2, 3, 4) x + y ## [1] 2 4 4 6 # replicated x = c(1, 2, 1, 2) c(1, 2, 1, 2) + y ## [1] 2 4 4 6 3.2.1 The geometric interpretation of vectors in \\(\\mathcal{R}^2\\) Let \\(\\mathcal{R}^2\\) be a real coordinate space of \\(2\\) dimensions. You are already familiar with the Cartesian plane that consists of ordered pairs \\((x, y)\\). The Cartesian plane defines the real coordinate space \\(\\mathbf{R}^2\\) of two dimensions. In \\(\\mathbf{R}^2\\), the location of any point of interest can be defined using the \\(x\\) and \\(y\\). For example, the plot below shows the location of the point (2, 3) dat &lt;- data.frame( x = 2, y = 3 ) ggplot(data = dat, aes(x = x, y = y)) + geom_point() + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) A vector space is a generalization of this representation. In \\(\\mathcal{R}^2\\), we say that the vector \\(\\mathbf{z} = c(2, 3)\\) is centered at the origin (0, 0) and has length 2 in the \\(x\\)-axis and length 3 in the \\(y\\)-axis. The plot below shows this vector We can also decompose the vector \\(\\mathbf{z}\\) into its \\(x\\) and \\(y\\) components. The \\(x\\) component of \\(\\mathbf{z}\\) is (2, 0) and the \\(y\\) component of \\(\\mathbf{z}\\) is (0, 3). The following plot shows the \\(x\\) component (2, 0) in blue and the \\(y\\) component (0, 3) in red. The below Shiny app allows you to plot the vector for any \\((x, y)\\) pair of your choosing. The shiny app can be downloaded and run on your own computer using library(shiny) runGitHub(rep = &quot;multivariable-math&quot;, username = &quot;jtipton25&quot;, subdir = &quot;shiny-apps/chapter-03/vector-space&quot;) 3.2.1.1 Addition of vectors We can represent the addition of vectors geometrically as well. Consider the two vectors \\(\\mathbf{u}\\) = (3, 2) and \\(\\mathbf{v}\\) = (-2, 1) where \\(\\mathbf{u} + \\mathbf{v}\\) = (1, 3). data.frame(x = c(3, -2, 1), y = c(2, 1, 3), vector = c(&quot;u&quot;, &quot;v&quot;, &quot;u+v&quot;)) %&gt;% ggplot() + geom_point(aes(x = x, y = y, color = vector)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) We can represent the sum using vectors by adding \\(\\mathbf{u}\\) first then adding \\(\\mathbf{v}\\) to \\(\\mathbf{u}\\) or by adding \\(\\mathbf{v}\\) first and then \\(\\mathbf{u}\\) to get df &lt;- data.frame(x = c(0, 3, 1, -2), y = c(0, 2, 3, 1)) p1 &lt;- ggplot() + geom_segment(aes(x = 0, xend = 3, y = 0, yend = 2), arrow = arrow(), color = &quot;blue&quot;) + geom_segment(aes(x = 3, xend = 3 - 2, y = 2, yend = 2 + 1), arrow = arrow(), color = &quot;red&quot;) + geom_segment(aes(x = 0, xend = 3 - 2, y = 0, yend = 2 + 1), arrow = arrow(), color = &quot;black&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_polygon(data = df, aes(x = x, y = y), fill = &quot;grey&quot;, alpha = 0.5) + ggtitle(&quot;u + v&quot;) p2 &lt;- ggplot() + geom_segment(aes(x = 0, xend = -2, y = 0, yend = 1), arrow = arrow(), color = &quot;red&quot;) + geom_segment(aes(x = -2, xend = -2 + 3, y = 1, yend = 1 + 2), arrow = arrow(), color = &quot;blue&quot;) + geom_segment(aes(x = 0, xend = 3 - 2, y = 0, yend = 2 + 1), arrow = arrow(), color = &quot;black&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_polygon(data = df, aes(x = x, y = y), fill = &quot;grey&quot;, alpha = 0.5) + ggtitle(&quot;v + u&quot;) p1 + p2 Notice that the sum of these vectors defines a parallelogram where the sum \\(\\mathbf{u} + \\mathbf{v}\\) is the diagonal of the shaded parallelogram. This geometric interpretation will serve as a basis for interpreting vector equations in higher dimensions where typical visualization methods fail. 3.2.2 Scalar multiplication of vectors We can represent the multiplication of a vector by a scalar geometrically as well. Consider the vector \\(\\mathbf{u}\\) = (3, 2) and the scalars \\(a = 2\\) and \\(b = -1\\). Then, we can plot \\(\\mathbf{u}\\), \\(a\\mathbf{u}\\), and \\(b\\mathbf{u}\\). data.frame(x = c(3, 2 * 3, -1 * 3), y = c(2, 2 * 2, -1 * 2), vector = c(&quot;u&quot;, &quot;a*u&quot;, &quot;b*u&quot;)) %&gt;% ggplot() + geom_point(aes(x = x, y = y, color = vector)) + geom_segment(aes(x = 0, xend = x, y = 0, yend = y, color = vector), arrow = arrow(), alpha = 0.75) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-6, 6), ylim = c(-6, 6)) In fact, if \\(a\\) is allowed to take on any values, then the set of all possible values of \\(a \\mathbf{u}\\) for all values of \\(a\\) defines an infinite line ggplot() + geom_abline(slope = 2/3, intercept = 0) + geom_point(aes(x = 3, y = 2)) + geom_segment(aes(x = 0, xend = 3, y = 0, yend = 2), arrow = arrow(), color = &quot;black&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) 3.2.3 The geometric interpretation of vectors in \\(\\mathcal{R}^3\\) Let the vector \\(\\mathbf{u} = c(-2, 3, 5)\\). Then, the figure below shows the vector in 3 dimensions. Draw picture by hand 3.2.4 The geometric interpretation of vectors in \\(\\mathcal{R}^n\\) As the number of dimensions increases, the same interpretation can be used, but the ability to visualize higher dimensions becomes more difficult. 3.2.5 Linear Combinations of Vectors We say that for any two scalars \\(a\\) and \\(b\\) and any two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of length \\(n\\), the sum \\[ \\begin{align*} a \\mathbf{x} + b \\mathbf{y} &amp; = \\begin{pmatrix} a x_1 + b y_1 \\\\ a x_2 + b y_2 \\\\ \\vdots \\\\ a x_n + b y_n \\\\ \\end{pmatrix} \\end{align*} \\] is called a linear combination. The idea of a linear combination can be extended to \\(K\\) different scalars \\(\\{ a_1, \\ldots, a_K \\}\\) and \\(K\\) different vectors \\(\\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_K\\}\\) each of length \\(n\\) as \\[ \\begin{align*} a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 + \\ldots + a_K \\mathbf{x}_K = \\sum_{k=1}^K a_k \\mathbf{x}_k &amp; = \\begin{pmatrix} \\sum_{k=1}^K a_k x_{k1} \\\\ \\sum_{k=1}^K a_k x_{k2} \\\\ \\vdots \\\\ \\sum_{k=1}^K a_k x_{kn} \\\\ \\end{pmatrix} \\end{align*} \\] The scalars \\(a_k\\) are called coefficients (sometimes also called weights). Example: Consider the linear combination \\(a \\mathbf{u} + b \\mathbf{v}\\) where \\[ \\begin{align*} \\mathbf{u} = \\begin{pmatrix} 3 \\\\ 6\\end{pmatrix} &amp;&amp; \\mathbf{v} = \\begin{pmatrix} -2 \\\\ 1\\end{pmatrix}. \\end{align*} \\] Are there values of \\(a\\) and \\(b\\) such \\(a \\mathbf{u} + b \\mathbf{v} = \\begin{pmatrix} 9 \\\\ - 4 \\end{pmatrix}\\)? To answer this question, we can write the linear combination as \\[ \\begin{align*} a \\begin{pmatrix} 3 \\\\ 6\\end{pmatrix} + b \\begin{pmatrix} -2 \\\\ 1\\end{pmatrix} &amp; = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix} \\end{align*} \\] which can be written using the property of scalar multiplication as \\[ \\begin{align*} \\begin{pmatrix} 3a \\\\ 6a \\end{pmatrix} + \\begin{pmatrix} -2b \\\\ b \\end{pmatrix} &amp; = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix} \\end{align*} \\] and using properties of vector addition can be written as \\[ \\begin{align*} \\begin{pmatrix} 3a - 2b \\\\ 6a + b \\end{pmatrix} &amp; = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix} \\end{align*} \\] Recognizing this as a system of linear equations \\[ \\begin{align*} 3a - 2b &amp; = 9 \\\\ 6a + b &amp; = -4, \\end{align*} \\] the system of equations can be written in an augmented matrix form as \\[ \\begin{align*} \\begin{pmatrix} 3 &amp; - 2 &amp; 9\\\\ 6 &amp; 1 &amp; -4 \\end{pmatrix} \\end{align*} \\] Reducing the augmented matrix to reduced row echelon form gives rref(matrix(c(3, 6, -2, 1, 9, -4), 2, 3)) ## [,1] [,2] [,3] ## [1,] 1 0 0.06666667 ## [2,] 0 1 -4.40000000 which has solutions \\(a = 0.0667\\) and \\(b = -4.4\\). Result: Any vector equation \\(a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 + \\ldots + a_K \\mathbf{x}_K = \\mathbf{c}\\) for a given constant vector \\(\\mathbf{b}\\) has the same solution set as the augmented matrix \\[ \\begin{align*} \\begin{pmatrix} \\mathbf{x}_1 &amp; \\mathbf{x}_2 &amp; \\cdots &amp; \\mathbf{x}_K &amp; \\mathbf{b} \\end{pmatrix} \\end{align*} \\] Equivalently, the set of vectors \\(\\{\\mathbf{x}_k\\}_{k=1}^K\\) can only be combined with linear coefficients \\(\\{a_k\\}_{k=1}^K\\) to equal the vector \\(\\mathbf{b}\\) if the linear system of equations is consistent. 3.2.6 The geometric interpretation of linear combinations of vectors Consider the vectors \\(\\mathbf{u} = \\begin{pmatrix} \\sqrt{2} \\\\ - \\sqrt{2} \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) shown in the figure below on the left. Exercise: Given \\(\\mathbf{u} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\), estimate the linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) that gives the point \\(\\mathbf{w}\\) in the figure below. 3.3 Span Let \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) be vectors in \\(\\mathcal{R}^n\\). We say the vector \\(\\mathbf{w}\\) is in the span of \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) (\\(\\mathbf{w} \\in \\mbox{span}\\{ \\mathbf{a}_1, \\ldots, \\mathbf{a}_K \\}\\)) if there exists coefficients \\(x_1, \\ldots, x_K\\) such that \\(\\mathbf{w} = \\sum_{k=1}^K x_k \\mathbf{a}_k\\). Example: While not a vector notation, you already understand the span from polynomial functions. For example, assume you have the functions 1, \\(x\\), and \\(x^2\\). Then, the functions \\(-4 + 3x^2\\) (\\(a_1 = -4, a_2 = 0, a_3 = 3\\)) and \\(-3 + 4x - 2x^2\\) (\\(a_1 = -3, a_2 = 4, a_3 = -2\\)) are in the span of functions \\(\\{1, x, x^2\\}\\), but the functions \\(x^3\\), \\(x^4 - 2x^2\\), etc., are not in the span of \\(\\{1, x, x^2\\}\\) because you cannot write these as a linear combination of \\(a_1 1 + a_2 x + a_3 x^2\\). 3.3.1 Geometric example of the span Example: Consider the vector \\(\\mathbf{u} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\). Then, the vector \\(\\mathbf{w} = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}\\) is in the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) because \\(\\mathbf{w} = 2 \\mathbf{u}\\) but the vector \\(\\mathbf{v} = \\begin{pmatrix} 4 \\\\ -4 \\end{pmatrix}\\) is not in the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) because there is no coefficient \\(a\\) such that \\(\\mathbf{w} = a \\mathbf{u}\\). In this example, the vector \\(\\mathbf{u}\\) is a 2-dimensional vector (lives in \\(\\mathcal{R}^2\\)–a plane) but the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) lives in 1-dimension (a line). ggplot() + geom_abline(slope = 1/2, intercept = 0, color = &quot;blue&quot;, size = 2) + geom_segment(aes(x = 0, xend = 2, y = 0, yend = 1), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;red&quot;) + geom_segment(aes(x = 0, xend = 4, y = 0, yend = 2), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;red&quot;) + geom_segment(aes(x = 0, xend = 4, y = 0, yend = -4), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;orange&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_text(data = data.frame(x = c(2, 4, 4), y = c(1, 2, -4), text = c(&quot;u&quot;, &quot;w&quot;, &quot;v&quot;)), aes(x = x, y = y + 0.5, label = text), size = 5, inherit.aes = FALSE, color = c(&quot;red&quot;, &quot;red&quot;, &quot;orange&quot;)) + ggtitle(&quot;span{u} is the blue line \\nw is in span{u}\\nv is not in span{u}&quot;) From the example above, we can answer the question “Is the point (a, b) on the line defined by the vector \\(\\mathbf{u}\\)?” by asking whether the point (a, b) is in the \\(span\\{\\mathbf{u}\\}\\). While this is trivial for such a simple problem, the use of the span will make things easier in higher dimensions. Example: do in class 2 3-d vectors that are not scalar multiples of each other define a plane. Does a point lie within the plane? Use the span to answer this question. "],["section-matrix-equation.html", "Chapter 4 Matrix equations 4.1 Solutions of matrix equations 4.2 Existence of solutions 4.3 Matrix-vector multiplication 4.4 Properties of matrix-vector multiplication 4.5 Solutions of linear systems 4.6 Solutions to nonhomogeneous systems 4.7 Finding solutions", " Chapter 4 Matrix equations library(tidyverse) library(dasc2594) library(gg3D) Here we introduce the concept of the linear equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\). This equation is the most fundamental equation in all of statistics and data science. Given a matrix \\(\\mathbf{A}\\) and a vector of constants \\(\\mathbf{b}\\), the goal is to solve for the value (or values) of \\(\\mathbf{x}\\) that are a solution to this equation. The equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) is a matrix representation of the system of linear equations \\[ \\begin{align*} \\tag{4.1} \\mathbf{A} \\mathbf{x} &amp; = \\mathbf{b} \\\\ \\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_K \\end{pmatrix} &amp; = \\mathbf{b} \\\\ x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K &amp; = \\mathbf{b} \\\\ \\end{align*} \\] as long as the matrix \\(\\mathbf{A}\\) has \\(n\\) rows and \\(K\\) columns and the vectors \\(\\mathbf{a}_k\\) are \\(n\\)-dimensional. Example: in class Example: in class 4.1 Solutions of matrix equations Because the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) is equivalent to a linear system of equations \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\), we can solve the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) by writing the equation in an augmented matrix form \\[ \\begin{align*} \\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K &amp; \\mathbf{b} \\end{pmatrix} \\end{align*} \\] and then reducing the matrix to reduced row echelon form. This gives rise to the theorem Theorem 4.1 The matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\), the vector equation \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\), and the augmented matrix \\(\\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K &amp; \\mathbf{b} \\end{pmatrix}\\) all have the same solution set. 4.2 Existence of solutions A solution to the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) exists if and only if \\(\\mathbf{b}\\) is a linear combination of the columns of \\(\\mathbf{A}\\). In other words, \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a solution if and only if \\(\\mathbf{b}\\) is in the \\(\\mbox{span}\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\}\\). Example: in class Let \\(\\mathbf{A} =\\ldots\\) and \\(\\mathbf{b} = \\ldots\\). Is the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) consistent? Theorem 4.2 For the \\(n \\times K\\) matrix \\(\\mathbf{A}\\), the following statements are equivalent: For each \\(\\mathbf{b} \\in \\mathcal{R}^K\\), the equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has at least one solution Each \\(\\mathbf{b} \\in \\mathcal{R}^K\\) is a linear combination of the columns of \\(\\mathbf{A}\\) The columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^K\\) \\(\\mathbf{A}\\) has a pivot in every row 4.3 Matrix-vector multiplication To calculate \\(\\mathbf{A} \\mathbf{x}\\), we need to define matrix multiplication. The equivalence between the linear systems of equations \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\) and the matrix equation \\(\\mathbf{A} \\mathbf{x}\\) gives a hint in how to do this. First, recall the definition of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1K} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\ldots &amp; a_{nK} \\\\ \\end{pmatrix} &amp;&amp; \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\end{align*} \\] The matrix product \\(\\mathbf{A}\\mathbf{x}\\) is the linear system of equations \\[ \\begin{align*} \\mathbf{A} \\mathbf{x} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1K} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\ldots &amp; a_{nK} \\\\ \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\\\ &amp; = x_1\\begin{pmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{n1} \\end{pmatrix} + x_2 \\begin{pmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{n2} \\end{pmatrix} + \\cdots + x_K \\begin{pmatrix} a_{1K} \\\\ a_{nK} \\\\ \\vdots \\\\ a_{nK} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} a_{11} x_1 + a_{12} x_2 + \\ldots + a_{1K} x_K \\\\ a_{21} x_1 + a_{22} x_2 + \\ldots + a_{2K} x_K \\\\ \\vdots \\\\ a_{n1} x_1 + a_{n2} x_2 + \\ldots + a_{nK} x_K \\\\ \\end{pmatrix} \\end{align*} \\] Notice that the first row of the last matrix above has the sum first row of the matrix \\(\\mathbf{A}\\) multiplied by the corresponding elements in \\(\\mathbf{x}\\) (i.e., first element \\(a_{11}\\) of the first row of \\(\\mathbf{A}\\) times the first element \\(x_1\\) of \\(\\mathbf{x}\\) plus the second, third, fourth, etc.). Likewise, this pattern holds for the second row, and all the other rows. This gives an algorithm for evaluating the product \\(\\mathbf{A} \\mathbf{x}\\). Definition 4.1 The product \\(\\mathbf{A}\\mathbf{x}\\) of a \\(n \\times K\\) matrix \\(\\mathbf{A}\\) with a \\(K\\)-vector \\(\\mathbf{x}\\) is a \\(n\\)-vector where the \\(i\\)th element of \\(\\mathbf{A}\\mathbf{x}\\) is the sum of the \\(i\\)th row of \\(\\mathbf{A}\\) times the corresponding elements of the vector \\(\\mathbf{x}\\) Example: in class Example: in class Example: in R using loops Example: in R using %*% 4.4 Properties of matrix-vector multiplication If \\(\\mathbf{A}\\) is a \\(n \\times K\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^K\\) and \\(c\\) is a scalar, then \\(\\mathbf{A} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{A} \\mathbf{u} + \\mathbf{A} \\mathbf{v}\\) \\(\\mathbf{A} (c \\mathbf{u}) = (c \\mathbf{A}) \\mathbf{u}\\) Proof in class 4.5 Solutions of linear systems 4.5.1 Homogeneous linear systems of equations Definition 4.2 The matrix equation \\[ \\begin{align} \\tag{4.2} \\mathbf{A}\\mathbf{x} = \\mathbf{0} \\end{align} \\] is called a homogeneous system of equations. The vector \\(\\mathbf{0}\\) is a vector of length \\(K\\) composed of all zeros. The trivial solution of the homogeneous equation is when \\(\\mathbf{x} = \\mathbf{0}\\) and is not a very useful solution. Typically one is interested in nontrivial solutions where \\(\\mathbf{x} \\neq \\mathbf{0}\\). The homogeneous linear system of equations can be written in augmented matrix form \\[ \\begin{align*} \\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K &amp; \\mathbf{0} \\end{pmatrix} \\end{align*} \\] which implies that a non-trivial solution only exists if there is a free variable. Another way of saying this is that at least one column must not be a pivot column. If every column were a pivot column, the reduced row echelon form of the augmented matrix would be \\[ \\begin{align*} \\begin{pmatrix} 1 &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\ldots &amp; 1 &amp; 0 \\end{pmatrix} \\end{align*} \\] which implies the only solution is the trivial solution \\(\\mathbf{0}\\). Example: in class \\[ \\begin{align*} 3 x_1 - 2 x_2 + 4 x_3 = 0 \\\\ - 2 x_1 + 4 x_2 - 2 x_3 = 0 \\\\ 5 x_1 - 6 x_2 + 6 x_3 = 0 \\end{align*} \\] * Example: in class Consider the equation \\[ \\begin{align*} 2x_1 + 4 x_2 - x_3 = 0. \\end{align*} \\] we can write this as \\[ \\begin{align*} x_1 = -2 x_2 + \\frac{1}{2} x_3 \\end{align*} \\] where \\(x_2\\) and \\(x_3\\) are free variables. Writing this as a solution \\(\\mathbf{x}\\) gives \\[ \\begin{align*} \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} -2 x_2 + \\frac{1}{2} x_3 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = x_2 \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} + x_3 \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{align*} \\] which is a linear combination of the vectors \\(\\mathbf{u} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix}\\). This implies that we can write the solution \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) for scalars \\(a\\) and \\(b\\). Therefore, the solution set \\(\\mathbf{x}\\) is contained in the \\(\\mbox{span}\\{\\mathbf{u}, \\mathbf{v}\\}\\). Because the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are linearly independent (they don’t point in the same direction), the set of all linear combinations of \\(c \\mathbf{u} + d \\mathbf{v}\\) defines a plane. Definition 4.3 A solution set of the form \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) is called a parametric vector solution. 4.6 Solutions to nonhomogeneous systems Recall the simple linear equation \\[ y = mx + b \\] where \\(m\\) is the slope and \\(b\\) is the y-intercept. Setting \\(b = 0\\) gives a simple homogenous linear equation where the y-intercept goes through the origin (0, 0). When \\(b\\) is nonzero, the line keeps the same slope but is shifted upward/downward by \\(b\\). ggplot(data = data.frame(x = 0, y = 0), aes(x, y)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_abline(slope = 2, intercept = 0, color = &quot;red&quot;) + geom_abline(slope = 2, intercept = 2, color = &quot;blue&quot;) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_text( data = data.frame(x = c(0, 0), y = c(0, 2), text = c(&quot;homogeneous\\nsolution&quot;, &quot;inhomogeneous\\nsolution&quot;)), aes(x = x + c(1.75, -1), y = y + 0.5, label = text), size = 5, inherit.aes = FALSE, color = c(&quot;red&quot;, &quot;blue&quot;)) + geom_segment( aes(x = 0, xend = 0, y = 0, yend = 2), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;orange&quot;) + geom_text( data = data.frame(x = 0, y = 2, text = &quot;b&quot;), aes(x = x + 0.5, y = y, label = text), size = 8, inherit.aes = FALSE, color = &quot;orange&quot;) This shift in location (but not in slope) is called a translation Example: in class Let’s revisit the example from before \\[ \\begin{align*} 3 x_1 - 2 x_2 + 4 x_3 = 0 \\\\ - 2 x_1 + 4 x_2 - 2 x_3 = 0 \\\\ 5 x_1 - 6 x_2 + 6 x_3 = 0 \\end{align*} \\] but change this so that \\(\\mathbf{b} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 1 \\end{pmatrix}\\) Write this as a parametric solution with a mean shift Example: Show this shift for a system of linear equations where the solution set defines a plane. From example above, \\[ \\begin{align*} 2x_1 + 4 x_2 - x_3 = 0. \\end{align*} \\] has the parametric solution \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) with \\[ \\begin{align*} \\mathbf{u} &amp; = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\mathbf{v} &amp; = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{align*} \\] Now, if we change the system of linear equations so that we have the inhomogeneous equation \\[ \\begin{align*} 2x_1 + 4 x_2 - x_3 = 20. \\end{align*} \\] we get the homogeneous solution set \\(x_1 = -2 x_2 + \\frac{1}{2} x_3 + 4\\) which can be written in parametric form as \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v} + \\mathbf{p}\\)$ with \\[ \\begin{align*} \\mathbf{u} &amp; = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\mathbf{v} &amp; = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} \\\\ \\mathbf{p} &amp; = \\begin{pmatrix} 20 \\\\ 0 \\\\ 0 \\end{pmatrix} \\end{align*} \\] A &lt;- matrix(c(3, -2, 5, -2, 4, -6, 4, -2, 6, 2, -6, 8), 3, 4) rref(A) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 1.50 -0.50 ## [2,] 0 1 0.25 -1.75 ## [3,] 0 0 0.00 0.00 \\[ \\begin{align*} x_1 = \\frac{3}{2} x_2 - \\frac{1}{2}\\\\ x_2 = \\frac{1}{4} x_3 - \\frac{7}{4} \\\\ \\end{align*} \\] which was the same solution set as the homoegenous solution plus the additional vector \\(\\begin{pmatrix} 20 \\\\ 0 \\\\ 0 \\end{pmatrix}\\). Thus, the inhomogenous solution is now \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v} + \\mathbf{p}\\) where \\(\\mathbf{u} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\\), \\(\\mathbf{v} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix}\\), and \\(\\mathbf{p} = \\begin{pmatrix} 20\\\\ 0 \\\\ 0 \\end{pmatrix}\\). For plotting, we will solve these equations for \\(x_3\\). Thus, the homogeneous equation has the solution \\(x_3 = 2x_1 + 4x_2\\) and the inhomogenous equation has the solution \\(x_3 = 2x_1 + 4x_2 - 20\\). # uses gg3D library n &lt;- 60 x1 &lt;- x2 &lt;- seq(-10, 10, length = n) region &lt;- expand.grid(x1 = x1, x2 = x2) df &lt;- data.frame( x1 = region$x1, x2 = region$x2, x3 = c( 2 * region$x1 + 4 * region$x2, 2 * region$x1 + 4 * region$x2 - 20), equation = rep(c(&quot;inhomogeneous&quot;, &quot;homogeneous&quot;), each = n^2)) # theta and phi set up the &quot;perspective/viewing angle&quot; of the 3D plot theta &lt;- 45 phi &lt;- 20 ggplot(df, aes(x = x1, y = x2, z = x3, color = equation)) + axes_3D(theta = theta, phi = phi) + stat_wireframe( alpha = 0.75, theta = theta, phi = phi) + scale_color_manual(values = c(&quot;inhomogeneous&quot; = &quot;blue&quot;, &quot;homogeneous&quot; = &quot;red&quot;)) + theme_void() + theme(legend.position = &quot;none&quot;) + labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), angle=c(0, 0, 90), theta = theta, phi = phi) ## Warning: Removed 4 row(s) containing missing values (geom_path). 4.7 Finding solutions The following algorithm describes how to solve a linear system of equations. 1) Put the system of equations in an augmented matrix form 2) Reduce the augmented matrix to reduced row echelon form 3) Express each determined variable as a function of the free variables. 4) Write the solution in a general form where the determined variables are a function of the independent variables 5) Decompose the solution \\(\\mathbf{x}\\) into a linear combination of free variables as parameters "],["section-linear-independence.html", "Chapter 5 Linear independence", " Chapter 5 Linear independence Recall the homogeneous equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\) can be written as a linear combination of coefficients \\(x_1, \\ldots, x_K\\) and vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) where \\[ \\begin{align*} \\sum_{k=1}^K x_k \\mathbf{a}_k = \\mathbf{0} \\end{align*} \\] Definition 5.1 The set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) are called linearly independent if the only solution to the vector equation \\(\\sum_{k=1}^K x_k \\mathbf{a}_k = \\mathbf{0}\\) is the trivial solution. The set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) are called linearly dependent if there are coefficients \\(x_1, \\ldots, x_K\\) that are not all zero. Example: in class What does it mean for a set of vectors to be linearly dependent? This means that there is at least one vector \\(\\mathbf{a}_k\\) that can be written as a sum of the other vectors with coefficients \\(z_k\\): \\[ \\begin{align*} \\mathbf{a}_k = \\sum_{k&#39; \\neq k} z_{k&#39;} \\mathbf{a}_{k&#39;} \\end{align*} \\] Note: this does not imply that all vectors \\(\\mathbf{a}_{k}\\) can be written as a linear combination of other vectors. Example: in class – determine if the vectors are linearly independent and solve the dependence relation Theorem 5.1 The matrix equation \\(\\mathbf{A}\\) has linearly independent columns if and only if the equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution. Example: in class A set of a single vector Example: in class A set of two vectors linearly independent if: linearly dependent if one vector is a scalar multiple of the other: Theorem 5.2 If an \\(n \\times K\\) matrix \\(\\mathbf{A}\\) has \\(K &gt; n\\), then the columns of \\(\\mathbf{A}\\) are linearly dependent. In other words, if a set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) contains more vectors than entries within vectors, the set of vectors is linearly dependent. Proof: If \\(K&gt;n\\), there are more variables (\\(K\\)) than equations (\\(n\\)). Therefore, there is at least one free variable and this implies that the homogeneous equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\) has a non-trivial solution (4.2) Theorem 5.3 If a set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) contains the \\(\\mathbf{0}\\) vector, then the the set of vectors is linearly dependent. Proof: in class Example: in class Determine whether the following sets of vectors are linearly dependent "],["section-linear-transformations.html", "Chapter 6 Linear Transformations 6.1 Linear Transformations 6.2 Types of matrix transformations 6.3 Properties of matrix transformations", " Chapter 6 Linear Transformations library(tidyverse) library(dasc2594) library(gifski) ## Linking to ImageMagick 6.9.11.48 ## Enabled features: freetype, ghostscript, lcms, webp ## Disabled features: cairo, fontconfig, fftw, pango, rsvg, x11 ## Using poppler version 0.73.0 It is often useful to think of \\(\\mathbf{A}\\mathbf{x}\\) as a linear transformation defined by the matrix \\(\\mathbf{A}\\) applied to the vector \\(\\mathbf{x}\\). A linear transformation is mathematically defined as a function/mapping \\(T(\\cdot)\\) (\\(T\\) for transformation) from a domain in \\(\\mathcal{R}^n\\) (function input) to a codomain in \\(\\mathcal{R}^m\\) (function output). In shorthand, this is written as \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) which is read a \"\\(T\\) maps inputs from the domain \\(\\mathcal{R}^n\\) to the codomain \\(\\mathcal{R}^m\\). For each \\(\\mathbf{x} \\in \\mathcal{R}^n\\) (in the domain), \\(T(\\mathbf{x}) \\in \\mathcal{R}^m\\) is known as the image of \\(\\mathbf{x}\\). The set of all \\(T(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathcal{R}^n\\) is known as the range of \\(T(\\mathbf{x})\\). Note that it is possible that the range of \\(T(\\mathbf{x})\\) is not required to be the entire space \\(\\mathcal{R}^m\\) (i.e., the range of the transformation \\(T\\) might be a subset of \\(\\mathcal{R}^m\\)) Draw figure In the case of matrix transformations (linear transformations), the function \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x}\\) where \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix and \\(\\mathbf{x} \\in \\mathcal{R}^n\\) is a \\(n\\)-vector. Question: What kind of object is \\(\\mathbf{A} \\mathbf{x}\\)? scalar vector matrix array Question What are the dimensions of \\(\\mathbf{A} \\mathbf{x}\\)? Using the matrix transformation notation, the domain of the transformation \\(T\\) is \\(\\mathcal{R}^n\\), the codomain of \\(\\mathcal{T}\\) \\(\\mathcal{R}^m\\). The range of the transformation \\(T\\) is the set of all linear combinations of the columns of \\(\\mathbf{A}\\) (the \\(\\mbox{span}\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}\\)) because the transformation \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x}\\) is a linear combination \\(\\sum_{i=1}^n x_i \\mathbf{a}_i\\) of the columns \\(\\{\\mathbf{a}_i\\}_{i=1}^n\\) of \\(\\mathbf{A}\\) with coefficients \\(x_1, \\ldots, x_n\\) Example: \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} 2 &amp; 4 \\\\ -3 &amp; 1 \\\\ -1 &amp; 6 \\end{pmatrix} &amp;&amp; \\mathbf{u} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} &amp;&amp; \\mathbf{b} = \\begin{pmatrix} -2 \\\\ -11 \\\\ -15 \\end{pmatrix} &amp;&amp; \\mathbf{c} = \\begin{pmatrix} 2 \\\\ -2 \\\\ -1 \\end{pmatrix} \\end{align*} \\] ## [,1] ## [1,] 14 ## [2,] 0 ## [3,] 17 ## b ## [1,] 1 0 3 ## [2,] 0 1 -2 ## [3,] 0 0 0 ## c ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Find the image of \\(\\mathbf{u}\\) using the matrix transformation \\(T\\) (e.g., calculate \\(T(\\mathbf{u})\\)). Find a coefficient vector \\(\\mathbf{x} \\in \\mathcal{R}^2\\) such that \\(T(\\mathbf{x})\\). Is there more than one \\(\\mathbf{x}\\) whose image under \\(T\\) is \\(\\mathbf{b}?\\) In other words, is the solution \\(\\mathbf{A} \\mathbf{x}= \\mathbf{b}\\) unique? Determine if \\(\\mathbf{c}\\) is in the range of \\(T\\). In other words, does the solution \\(\\mathbf{A} \\mathbf{x}= \\mathbf{c}\\) exist? 6.1 Linear Transformations Definition 6.1 A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is linear if \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) for all \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in the domain of \\(T\\) \\(T(c \\mathbf{u}) = c T(\\mathbf{u})\\) for all scalars \\(c\\) and all vectors \\(\\mathbf{u}\\) in the domain of \\(T\\) Note: Because a linear transformation is equivalent to a matrix transformation, the definition above is equivalent to the following matrix-vector multiplication properties If \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^m\\) and \\(c\\) is a scalar, then \\(\\mathbf{A} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{A} \\mathbf{u} + \\mathbf{A} \\mathbf{v}\\) \\(\\mathbf{A} (c \\mathbf{u}) = (c \\mathbf{A}) \\mathbf{u}\\) As a consequence of the previous definition, the following properties hold for scalars \\(c\\) and \\(d\\) and vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v} \\in \\mathcal{R}^m\\) \\(T(\\mathbf{0}) = \\mathbf{0}\\) \\(T(c \\mathbf{u} + d \\mathbf{v}) = c T(\\mathbf{u}) + d T(\\mathbf{v})\\) Show why in class These properties give rise to the following statement for scalars \\(c_1, \\ldots, c_m\\) and vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_m \\in \\mathcal{R}^n\\) \\(T(c_1 \\mathbf{u}_1 + \\ldots + c_m \\mathbf{u}_m) = c_1 T(\\mathbf{u}_1) + \\ldots + c_m T(\\mathbf{u}_m)\\) The statements above for linear transformations are equivalent to the matrix statements where \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^m\\) and \\(c\\) is a scalar: \\(\\mathbf{A} \\mathbf{0} = \\mathbf{0}\\) \\(\\mathbf{A}(c \\mathbf{u} + d \\mathbf{v}) = c \\mathbf{A} \\mathbf{u} + d \\mathbf{A} \\mathbf{v}\\) And for a \\(m \\times n\\) matrix \\(\\mathbf{A}\\), scalars \\(c_1, \\ldots, c_m\\), and vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_m \\in \\mathcal{R}^n\\) \\(\\mathbf{A}(c_1 \\mathbf{u}_1 + \\ldots + c_m \\mathbf{u}_m) = c_1 \\mathbf{A}\\mathbf{u}_1 + \\ldots + c_m \\mathbf{A} \\mathbf{u}_m\\) 6.2 Types of matrix transformations The basic types of matrix transformations include contractions/expansions rotations reflections shears projections For the following examples, we will consider the unit vectors \\(\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\) and apply different linear transformations using the matrix \\(\\mathbf{A}\\). To build the matrix transformations, we use the dasc2594 package and build matrix transformations based on code from https://www.bryanshalloway.com/2020/02/20/visualizing-matrix-transformations-with-gganimate/. 6.2.1 Contractions/Expansions 6.2.1.1 Horizonal Expansion The matrix below gives a horizontal expansion when \\(x &gt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} x &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] In the example below, we set \\(x = 2\\) and generate the transformation. transformation_matrix &lt;- tribble( ~ x, ~ y, 2, 0, 0, 1) %&gt;% as.matrix() p &lt;- plot_transformation(transformation_matrix) 6.2.1.2 Horizonal Contraction The matrix below gives a horizontal contraction when \\(x &lt; 1\\) * Horizontal contraction when \\(x &lt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} x &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] In the example below, we set \\(x = 0.5\\) 6.2.1.3 Vertical Expansion The matrix below gives a vertical expansion when \\(x &gt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; x \\end{pmatrix} \\] * In the example below, we set \\(x = 2\\) 6.2.1.4 Vertical Contraction The matrix below gives a vertical contraction when \\(x &lt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; x \\end{pmatrix} \\] In the example below, we set \\(x = 0.5\\) 6.2.2 Rotations 6.2.2.1 Rotation by 90 degrees Rotations in 2D of an angle \\(\\theta \\in [0, 2\\pi]\\) take the form of \\[ \\mathbf{A} = \\begin{pmatrix} \\cos(\\theta) &amp; -\\sin(\\theta) \\\\ \\sin(\\theta) &amp; \\cos(\\theta) \\end{pmatrix} \\] For example, a rotation of 90 degrees counter-clockwise (\\(\\theta = \\frac{\\pi}{2}\\)) is given by the transformation matrix \\[ \\mathbf{A} = \\begin{pmatrix} \\cos(\\frac{\\pi}{2}) &amp; -\\sin(\\frac{\\pi}{2}) \\\\ \\sin(\\frac{\\pi}{2}) &amp; \\cos(\\frac{\\pi}{2}) \\end{pmatrix} = \\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] Another example is for a rotation of 45 degrees clockwise (\\(\\theta = -\\frac{\\pi}{4}\\)) is given by the transformation matrix \\[ = \\begin{pmatrix} \\cos(\\frac{\\pi}{4}) &amp; -\\sin(\\frac{\\pi}{4}) \\\\ \\sin(\\frac{\\pi}{4}) &amp; \\cos(\\frac{\\pi}{4}) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{2}}{2} &amp; -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} &amp; \\frac{\\sqrt{2}}{2} \\end{pmatrix} \\] 6.2.3 Reflections 6.2.3.1 Reflection across the x-axis The matrix below gives a reflection about the x-axis \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} \\] 6.2.3.2 Reflection across the y-axis The matrix below gives a reflection about the y-axis \\[ \\mathbf{A} = \\begin{pmatrix} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] 6.2.3.3 Reflection across the line y = x \\[ \\mathbf{A} = \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] In the example below, we set \\(x = 0.5\\) 6.2.3.4 Reflection across the line y = - x \\[ \\mathbf{A} = \\begin{pmatrix} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{pmatrix} \\] 6.2.3.5 Reflection across the origin (0, 0) \\[ \\mathbf{A} = \\begin{pmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} \\] 6.2.4 Shears A shear transformation is like stretching play-dough if it was possible to stretch all parts of the dough uniformly (rather than some sections getting stretched more than others). 6.2.4.1 Horizontal Shear \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; x \\\\ 0 &amp; 1 \\end{pmatrix} \\] For the example below, we plot a horizontal shear with \\(x = 2\\). 6.2.4.2 Vertical Shear \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; x \\\\ 0 &amp; 1 \\end{pmatrix} \\] For the example below, we plot a horizontal shear with \\(x = 2\\). 6.2.5 Projections A projection is a mapping \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) from one space (\\(\\mathbf{R}^n\\)) to itself (\\(\\mathbf{R}^n\\)) such that \\(T^2 = T\\) 6.2.5.1 Project onto the x-axis \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} \\] For the example below, we plot a projection onto the x-axis 6.2.5.2 Project onto the y-axis \\[ \\mathbf{A} = \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] For the example below, we plot a projection onto the y-axis 6.2.6 Identity The identity transformation is the transformation that leaves the vector input unchanged. The identity matrix is typically written as \\(\\mathbf{I}\\) \\[ \\mathbf{I} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] 6.3 Properties of matrix transformations 6.3.1 One-to-one transformations Definition 6.2 A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is called one-to-one if every vector \\(\\mathbf{b}\\) in the image \\(\\mathcal{R}^m\\), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at most one solution in \\(\\mathcal{R}^n\\). The following statements are equivalent was of saying \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is one-to-one: For every \\(\\mathbf{b} \\in \\mathcal{R}^m\\) (for every vector in the image), the equation \\(T(\\mathbf{x}]) = \\mathbf{b}\\) has either zero or one solution Every different input into the function \\(T(\\cdot)\\) has a different output If \\(T(\\mathbf{u}) = T(\\mathbf{v})\\) then \\(\\mathbf{u} = \\mathbf{v}\\) The following statements are equivalent was of saying \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is not one-to-one: a) There exists as least one \\(\\mathbf{b} \\in \\mathcal{R}^m\\) such that the equation \\(T(\\mathbf{x}]) = \\mathbf{b}\\) has more than one solution b) There are at least two different inputs into the function \\(T(\\cdot)\\) that have the same output c) There exist vectors \\(\\mathbf{u} \\neq \\mathbf{v} \\in \\mathcal{R}^n\\) such that \\(T(\\mathbf{u}) = T(\\mathbf{v})\\) Theorem 6.1 Let \\(\\mathbf{A}\\mathbf{x}\\) be the matrix representation of the linear transformation \\(T(\\mathbf{x})\\) for the \\(m \\times n\\) matrix \\(\\mathbf{A}\\). Then the following statements are equivalent: \\(T\\) is one-to-one. For every \\(\\mathbf{b} \\in \\mathcal{R}^m\\), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at most one solution. For every \\(\\mathbf{b} \\in \\mathcal{R}^m\\), the equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) has a unique solution or is inconsistent. The equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only a trivial solution. The columns of \\(\\mathbf{A}\\) are linearly independent. \\(\\mathbf{A}\\) has a pivot in every column. The range of \\(\\mathbf{A}\\) has dimension \\(n\\) Example: is the following matrix one-to-one? \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix} \\] Example: is the following matrix one-to-one? \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\end{pmatrix} \\] Note: Matrices that are wider than they are tall are not one-to-one transformations. (This does not mean that all tall matrices are one-to-one) 6.3.2 Onto transformations Definition 6.3 A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is called onto if, for every vector \\(\\mathbf{b} \\in \\mathcal{R}^m\\), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at least one solution \\(\\mathbf{x} \\in \\mathcal{R}^n\\) The following are equivalent ways of saying that \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is onto: The range of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is equal to the codomain of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) Every vector in the codomain is the output of some input vector The following are equivalent ways of saying that \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is not onto: The range of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is smaller than the codomain of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\). There exists a vector \\(\\mathbf{b} \\in \\mathcal{R}^m\\) such that the equation \\(T(\\mathbf{x})\\) does not have a solution. There is a vector in the codomain that is not the output of any input vector. Theorem 6.2 Let \\(\\mathbf{A}\\mathbf{x}\\) be the matrix representation of the linear transformation \\(T(\\mathbf{x})\\) for the \\(m \\times n\\) matrix \\(\\mathbf{A}\\). Then the following statements are equivalent: \\(T\\) is onto \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at least one solution for every \\(\\mathbf{b} \\in \\mathcal{R}^m\\). The equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) is consistent for every \\(\\mathbf{b} \\in \\mathcal{R}^m\\). The columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^m\\) \\(\\mathbf{A}\\) has a pivot in every row The range of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) has dimension \\(m\\) Example: Example: is the following matrix onto? \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\end{pmatrix} \\] Example: is the following matrix one-to-one? \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] Note: Matrices that are taller than they are wide are not onto transformations. (This does not mean that all wide matrices are onto) "],["section-matrix-operations.html", "Chapter 7 Matrix operations 7.1 Properties of matrices", " Chapter 7 Matrix operations Note: add examples: 7.1 Properties of matrices 7.1.1 Matrix Addition Matrix Addition: If the matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of the same dimension (e.g., both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) have the same number of rows \\(m\\) and the same number of columns \\(n\\)), then \\[ \\begin{align*} \\tag{7.1} \\mathbf{A} + \\mathbf{B} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} + \\begin{pmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1n} \\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{m1} &amp; b_{m2} &amp; \\cdots &amp; b_{mn} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} a_{11} + b_{11} &amp; b_{12} + b_{12} &amp; \\cdots &amp; a_{1n} + b_{1n} \\\\ a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \\cdots &amp; a_{2n} + b_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \\cdots &amp; a_{mn} + b_{mn} \\end{pmatrix} \\\\ &amp; = \\left\\{ a_{ij} + b_{ij} \\right\\} \\end{align*} \\] … Another way to If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of the same dimension (same number of rows and columns) you can add the matrices together A &lt;- matrix(c(4, 1, 33, 2, 0, -4), 3, 2) B &lt;- matrix(c(7, -24, 3, 9, 11, -9), 3, 2) A ## [,1] [,2] ## [1,] 4 2 ## [2,] 1 0 ## [3,] 33 -4 B ## [,1] [,2] ## [1,] 7 9 ## [2,] -24 11 ## [3,] 3 -9 A + B ## [,1] [,2] ## [1,] 11 11 ## [2,] -23 11 ## [3,] 36 -13 We can also write this using for loops # initialize an empty matrix to fill C &lt;- matrix(0, 3, 2) for (i in 1:nrow(A)) { # loop over the rows for (j in 1:ncol(A)) { # loop over the columns C[i, j] &lt;- A[i, j] + B[i, j] } } C ## [,1] [,2] ## [1,] 11 11 ## [2,] -23 11 ## [3,] 36 -13 If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of different dimensions (they differ in either the number of rows or columns), R will return an error warning you that the matrices are of different sizes and can’t be added A &lt;- matrix(c(4, 1, 33, 2, 0, -4), 3, 2) B &lt;- matrix(c(7, -24, 3, 9), 2, 2) A ## [,1] [,2] ## [1,] 4 2 ## [2,] 1 0 ## [3,] 33 -4 B ## [,1] [,2] ## [1,] 7 3 ## [2,] -24 9 A + B ## Error in A + B: non-conformable arrays Theorem 7.1 Let \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) be \\(m \\times n\\) matrices and let \\(a\\) and \\(b\\) be scalars, then: \\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\) \\((\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\) \\(\\mathbf{A} + \\mathbf{0} = \\mathbf{A}\\) \\(a (\\mathbf{A} + \\mathbf{B}) = a \\mathbf{A} + a \\mathbf{B}\\) \\((a + b)\\mathbf{A} = a \\mathbf{A} + b \\mathbf{A}\\) \\((ab)\\mathbf{A} = a (b\\mathbf{A})\\) 7.1.2 Matrix Multipliation Matrix Multiplication: If \\(\\mathbf{A} = \\left\\{ a_{ij} \\right\\}\\) is an \\(m \\times n\\) matrix and \\(\\mathbf{B} = \\left\\{ a_{jk} \\right\\}\\) is a \\(n \\times p\\) matrix, then the matrix product \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) is an \\(m \\times p\\) matrix where \\(\\mathbf{C} = \\left\\{ \\sum_{j=1}^p a_{ij} b{jk} \\right\\}\\) \\[ \\begin{align} \\tag{7.2} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\begin{pmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1p} \\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{n1} &amp; b_{n2} &amp; \\cdots &amp; b_{np} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} \\sum_{j=1}^n a_{1j} b_{j1} &amp; \\sum_{j=1}^n a_{1j} b_{j2} &amp; \\cdots &amp; \\sum_{j=1}^n a_{1j} b_{jp} \\\\ \\sum_{j=1}^n a_{2j} b_{j1} &amp;\\sum_{j=1}^n a_{2j} b_{j2} &amp; \\cdots &amp; \\sum_{j=1}^n a_{2j} b_{jp} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum_{j=1}^n a_{mj} b_{j1} &amp;\\sum_{j=1}^n a_{nj} b_{j2} &amp; \\cdots &amp; \\sum_{j=1}^n a_{mj} b_{jp} \\end{pmatrix} \\end{align} \\] Another way to define matrix multiplication is through inner product notation. Define the \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and the \\(n \\times p\\) matrix \\(\\mathbf{B}\\) as the partition \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{a}_{1}&#39; \\\\ \\mathbf{a}_{2}&#39; \\\\ \\vdots \\\\ \\mathbf{a}_{m}&#39; \\end{pmatrix} &amp; \\mbox{ and } &amp;&amp; \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{b}_{1} &amp; \\mathbf{b}_{2} &amp; \\cdots &amp; \\mathbf{b}_{p} \\end{pmatrix} \\end{align*} \\] where \\(\\mathbf{a}_i\\) and \\(\\mathbf{b}_k\\) are both \\(n\\)-vectors. Then, we have \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) can be written as \\[ \\begin{align*} \\mathbf{A} \\mathbf{B} = \\mathbf{A} \\begin{pmatrix} \\mathbf{b}_1 &amp; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{b}_p \\end{pmatrix} = \\begin{pmatrix} \\mathbf{A} \\mathbf{b}_1 &amp; \\mathbf{A} \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{A} \\mathbf{b}_p \\end{pmatrix} \\end{align*} \\] Note that in this representation, each column of the matrix \\(\\mathbf{A}\\mathbf{B}\\) is a linear combination the the columns of \\(\\mathbf{A}\\) with coefficients given by the corresponding column of \\(\\mathbf{B}\\). \\[ \\begin{align*} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{a}_1&#39; \\mathbf{b}_1 &amp; \\mathbf{a}_1&#39; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}_1&#39; \\mathbf{b}_q \\\\ \\mathbf{a}_2&#39; \\mathbf{b}_1 &amp; \\mathbf{a}_2&#39; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}_2&#39; \\mathbf{b}_q \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{a}_n&#39; \\mathbf{b}_1 &amp; \\mathbf{a}_n&#39; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}_n&#39; \\mathbf{b}_q \\end{pmatrix} \\\\ &amp; = \\left\\{ \\mathbf{a}_i&#39; \\mathbf{b}_k \\right\\}. \\end{align*} \\] Written in this notation, we arrive at the multiplication rule for \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) – the \\(ik\\)th element \\(c_{ik}\\) of \\(\\mathbf{C}\\) is the inner product of the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). 7.1.3 Properties of Matrix Multiplication Define the \\(m \\times m\\) identity matrix \\(\\mathbf{I}_m\\) with ones on the diagonal and zeros off diagonal as \\[ \\mathbf{I}_m = \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1\\end{pmatrix} \\] Let \\(\\mathbf{A}\\) be an \\(m \\times n\\) matrix, then: Let \\(\\mathbf{B}\\) be an \\(n \\times p\\) matrix and \\(\\mathbf{C}\\) a \\(p \\times q\\) matrix. Then \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\) is an \\(m \\times q\\) matrix. Let \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) be \\(n \\times p\\) matrices. Then \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\\) is an \\(m \\times p\\) matrix. Let \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) be \\(p \\times m\\) matrices. Then \\((\\mathbf{B} + \\mathbf{C})\\mathbf{A} = \\mathbf{B}\\mathbf{A} + \\mathbf{C}\\mathbf{A}\\) is an \\(p \\times m\\) matrix. Let \\(\\mathbf{B}\\) be an \\(p \\times m\\) matrix and \\(c\\) a scalar. Then \\(c(\\mathbf{A} \\mathbf{B}) = (c \\mathbf{A}) \\mathbf{B} = \\mathbf{A}(c\\mathbf{B})\\) is an \\(p \\times m\\) matrix. \\(\\mathbf{I}_m \\mathbf{A} = \\mathbf{A} \\mathbf{I}_n = \\mathbf{A}\\) Examples: in class Note: Matrix multiplication violates some of the rules of multiplication that you might be used to. Pay attention for the following: In general \\(\\mathbf{A} \\mathbf{B} \\neq \\mathbf{B} \\mathbf{A}\\) (sometimes these are equal, but usually are not) \\(\\mathbf{A}\\mathbf{B} = \\mathbf{A} \\mathbf{C}\\) does not imply \\(\\mathbf{B} = \\mathbf{C}\\) \\(\\mathbf{A}\\mathbf{B} = \\mathbf{0}\\) does not imply that \\(\\mathbf{A} = \\mathbf{0}\\) or \\(\\mathbf{B} = \\mathbf{0}\\) 7.1.4 Matrix Multiplication complexity (Big O notation) In the study of algorithms, the notation \\(O(n)\\) is used to describe the number of calculations that need to be done to evaluate the equation. As an example, consider \\(\\mathbf{A} = \\begin{pmatrix}3 &amp; 1 \\\\ 2 &amp; -3 \\end{pmatrix}\\), \\(\\mathbf{B} = \\begin{pmatrix} -2 &amp; 4 \\\\ -1 &amp; 2 \\end{pmatrix}\\), and \\(\\mathbf{x} = \\begin{pmatrix} -3 \\\\ 1 \\end{pmatrix}\\). By hand: Calculate \\((\\mathbf{A} \\mathbf{B}) \\mathbf{x}\\) \\(\\mathbf{A} (\\mathbf{B} \\mathbf{x})\\) Which was easier? Which required less calculation? Matrix-matrix multiplication of and \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and an \\(m \\times p\\) matrix \\(\\mathbf{B}\\) has complexity \\(O(m n p)\\). Matrix-vector multiplication of and \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and an \\(p\\)-vector \\(\\mathbf{x}\\) has complexity \\(O(n m)\\). From example above: \\(O(m n p)\\) matrix-matrix multiplication \\((\\mathbf{A} \\mathbf{B})\\) followed by \\(O(m n)\\) matrix-vector multiplication \\((\\mathbf{A} \\mathbf{B}) \\mathbf{x}\\) which has computational complexity \\(O(m n p) + O(m n)\\) \\(O(m n)\\) matrix-vector multiplication \\((\\mathbf{B} \\mathbf{x})\\) followed by \\(O(m n)\\) matrix-vector multiplication \\(\\mathbf{A} (\\mathbf{B} \\mathbf{x})\\) which has computational complexity \\(O(m n) + O(m n)\\) 7.1.5 Matrix powers Powers of a \\(n \\times n\\) (square) matrix are defined as the product of \\(\\mathbf{A}\\) multiplied \\(k\\) times \\[ \\mathbf{A}^k = \\underbrace{\\mathbf{A} \\cdots \\mathbf{A}}_k \\] 7.1.6 Matrix Transpose The matrix transpose is an operator that swaps the rows and columns of a matrix. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, then \\(\\mathbf{A}&#39;\\) is a \\(\\m \\times n\\) matrix (Note: some use \\(\\mathbf{A}^T\\) to denote a transpose; I prefer the \\(&#39;\\) notation as it is much simpler and cleaner notation). The matrix \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1p} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{np} \\end{pmatrix} \\end{align*} \\] has transpose \\[ \\begin{align*} \\mathbf{A}&#39; &amp; = \\begin{pmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{p1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{p2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1n} &amp; a_{2n} &amp; \\cdots &amp; a_{pn} \\end{pmatrix}, \\end{align*} \\] Theorem 7.2 Let \\(\\mathbf{A}\\) be an \\(m \\times n\\) matrix, then \\((\\mathbf{A}&#39;)&#39; = \\mathbf{A}\\). Let \\(\\mathbf{B}\\) be an \\(m \\times n\\) matrix, then \\((\\mathbf{A} + \\mathbf{B})&#39; = \\mathbf{A}&#39; + \\mathbf{B}&#39;\\). For any scalar \\(c\\), \\((c \\mathbf{A})&#39; = c \\mathbf{A}&#39;\\). Let \\(\\mathbf{B}\\) be an \\(n \\times p\\) matrix, then \\(( \\mathbf{A} \\mathbf{B})&#39; = \\mathbf{B}&#39; \\mathbf{A}&#39;\\) is an \\(p \\times m\\) matrix. Note: The power of video games: GPUs and modern CPUs are becoming more and more parallelized. Because the \\(ij\\)th element of \\(\\mathbf{A}\\mathbf{B}\\) requires only the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\), matrix multiplication is easily parallelized under modern computing architectures. Thanks to video games, this parallelization has been made faster than ever. Examples: in class "],["section-matrix-inverse.html", "Chapter 8 Matrix Inverses 8.1 Elementary matrices 8.2 Finding the inverse of \\(\\mathbf{A}\\) 8.3 The Invertible Matrix Theorem 8.4 Invertible Linear Transformations", " Chapter 8 Matrix Inverses library(dasc2594) library(tidyverse) For scalars, the multiplicative identity is \\[ a \\frac{1}{a} = a a^{-1} = a^{-1} a = 1 \\] where \\(a^{-1}\\) is the inverse of \\(a\\). The \\(n \\times n\\) square matrix \\(\\mathbf{A}\\) is said to be invertible if there exists a \\(n \\times n\\) matrix \\(\\mathbf{C}\\)( which we call \\(\\mathbf{A}^{-1}\\) once we verify the inverse exists) such that \\[ \\begin{align*} \\mathbf{C}\\mathbf{A} = \\mathbf{A} \\mathbf{C} &amp; = \\mathbf{I} \\\\ \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{-1} &amp; = \\mathbf{I} \\end{align*} \\] where \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix (the matrix with 1s on the diagonal and zeros everywhere else). In R, an identity matrix is easy to construct. An \\(n \\times n\\) identity matrix can be constructed using the diag() function n &lt;- 4 I &lt;- diag(n) I ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 Example: \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} 1 &amp; -1 \\\\ 2 &amp; -3 \\end{pmatrix} &amp;&amp; \\mathbf{B} = \\begin{pmatrix} 1 &amp; -1 \\\\ 2 &amp; -3 \\end{pmatrix} \\end{align*} \\] Theorem 8.1 Let \\(\\mathbf{A} = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\). If \\(ad - bc \\neq 0\\) then \\(\\mathbf{A}\\) is invertible and \\[ \\begin{align*} \\mathbf{A} = \\frac{1}{ad - bc} \\begin{pmatrix} d &amp; -b \\\\ -c &amp; a \\end{pmatrix} \\end{align*} \\] If \\(ad - bc = 0\\), then the matrix is not invertible. Question: why is the matrix not invertible when \\(ad - bc = 0\\)? Have you heard of “singular” or “singularity” before? Black holes are called singularities. Why is this? Square matrices that are not invertible are call “singular” Definition 8.1 For the \\(2 \\times 2\\) matrix \\(\\mathbf{A} = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\), the term \\(ad - bc\\) is called the determinant of the matrix \\(\\mathbf{A}\\) and is written as \\(\\operatorname{det}(\\mathbf{A})\\). Sometimes the determinant is written as \\(| \\mathbf{A}|\\) A consequence of the above theorem is that a \\(2 \\times 2\\) matrix is invertible only if its determinant is nonzero. Example: in class Determine if the following \\(2 \\times 2\\) matrix is invertible Theorem 8.2 If the \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is invertible, then for each \\(\\mathbf{b} \\in \\mathcal{R}^n\\), the matrix equation \\[ \\mathbf{A} \\mathbf{x} = \\mathbf{b} \\] has the unique solution \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\). Proof: in class Example in R: in class Theorem 8.3 (Invertible Matrix Theorem) 1) If \\(\\mathbf{A}\\) is an invertible matrix, then \\(\\mathbf{A}^{-1}\\) is invertible and \\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\) If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are \\(n \\times n\\) invertible matrices, then \\(\\mathbf{A} \\mathbf{B}\\) is also an invertible matrix whose inverse is \\[ (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1} \\] which is the inverse of the matrices in reverse order. If \\(\\mathbf{A}\\) is an invertible matrix, then the transpose \\(\\mathbf{A}&#39;\\) is also invertible and the inverse of \\(\\mathbf{A}&#39;\\) is the transpose of \\(\\mathbf{A}^{-1}\\). Equivalently, \\[ (\\mathbf{A}&#39;)^{-1} = (\\mathbf{A}^{-1})&#39; \\] Proof: in class *Note: The product of \\(k\\) invertible \\(n \\times n\\) matrices \\(\\mathbf{A}_1 \\mathbf{A}_2 \\cdots \\mathbf{A}_k\\) has inverse \\(\\mathbf{A}_k^{-1} \\mathbf{A}_{k-1}^{-1} \\cdots \\mathbf{A}_1\\) 8.1 Elementary matrices Elementary matrices are matrices that perform basic row operations (i.e., we can write the reduced row echelon algorithm as a produce of elementary matrices). Recall the elementary row operations: swaps: swapping two rows. sums: replacing a row by the sum itself and a multiple of another row. scalar multiplication: replacing a row by a scalar multiple times itself. Example: Consider a \\(3 \\times 3\\) matrix A &lt;- matrix(c(4, 5, 9, -2, -4, 1, 4, 6, -2), 3, 3) \\(\\mathbf{A} = \\begin{pmatrix} 4 &amp; -2 &amp; 4 \\\\ 5 &amp; -4 &amp; 6 \\\\ 9 &amp; 1 &amp; -2 \\end{pmatrix}\\) What is the elementary matrix (let’s call it \\(\\mathbf{E}_1\\) that swaps the first and second rows of \\(\\mathbf{A}\\)? E_1 &lt;- matrix(c(0, 1, 0, 1, 0, 0, 0, 0, 1), 3, 3) \\(\\mathbf{E}_1 = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 5 -4 6 ## [3,] 9 1 -2 ## left multiple A by E_1 E_1 %*% A ## [,1] [,2] [,3] ## [1,] 5 -4 6 ## [2,] 4 -2 4 ## [3,] 9 1 -2 Thus, the matrix \\(\\mathbf{E}_1 = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) is the matrix that swaps the first and second row. What is the elementary matrix (let’s call it \\(\\mathbf{E}_2\\) that adds two times the first of \\(\\mathbf{A}\\) to the third row of \\(\\mathbf{A}\\)? E_2 &lt;- matrix(c(1, 0, 2, 0, 1, 0, 0, 0, 1), 3, 3) \\(\\mathbf{E}_2 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\end{pmatrix}\\) A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 5 -4 6 ## [3,] 9 1 -2 ## left multiple A by E_2 E_2 %*% A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 5 -4 6 ## [3,] 17 -3 6 Thus, the matrix \\(\\mathbf{E}_2 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\end{pmatrix}\\) is the matrix that adds two times the first of \\(\\mathbf{A}\\) to the third row of \\(\\mathbf{A}\\) What is the elementary matrix (let’s call it \\(\\mathbf{E}_3\\) that mutliples the second row of \\(\\mathbf{A}\\) by 3? E_3 &lt;- matrix(c(1, 0, 0, 0, 3, 0, 0, 0, 1), 3, 3) \\(\\mathbf{E}_3 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 5 -4 6 ## [3,] 9 1 -2 ## left multiple A by E_3 E_3 %*% A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 15 -12 18 ## [3,] 9 1 -2 Thus, the matrix \\(\\mathbf{E}_3 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) is the matrix that mutliples the second row of \\(\\mathbf{A}\\) by 3. Question: Do you see any patterns with how the example elementary matrices look? \\[ \\begin{align*} \\mathbf{E_1} = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} &amp;&amp; \\mathbf{E_2} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\end{pmatrix} &amp;&amp; \\mathbf{E_3} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\end{align*} \\] The elementary matrices look like the identity matrix \\(\\mathbf{I}\\) with an elementary row operation applied to \\(\\mathbf{I}\\). In fact, this leads us to this general fact: Fact: If an elementary row matrix is applied to the \\(m \\times n\\) matrix \\(\\mathbf{A}\\), the result of this elementary row operation applied to \\(\\mathbf{A}\\) can be written as \\(\\mathbf{E} \\mathbf{A}\\) where \\(\\mathbf{E}\\) is the \\(m \\times m\\) identity matrix \\(\\mathbf{I}\\) with the respective elementary row operation applied to \\(\\mathbf{I}\\). Fact: Each elementary matrix \\(\\mathbf{E}\\) is invertible Example: in class The next theorem is quite important as the result gives an algorithm for calculating the inverse of a \\(n \\times n\\) matrix \\(\\mathbf{A}\\) which also makes it possible to solve matrix equations \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) Theorem 8.4 If an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is invertible, then \\(\\mathbf{A}\\) is row-equivalent to \\(\\mathbf{I}\\) (\\(\\mathbf{A} \\sim \\mathbf{I}\\); row-equivalent means \\(\\mathbf{A}\\) can be reduced to \\(\\mathbf{I}\\) using elementary row operations). The row-equivalency implies that there is a series of elementary row operations (e.g., elementary matrices \\(\\mathbf{E}_1, \\ldots, \\mathbf{E}_k\\)) that converts \\(\\mathbf{A}\\) to \\(\\mathbf{I}\\). In addition, the application of these row matrices to \\(\\mathbf{I}\\) transforms \\(\\mathbf{I}\\) to the matrix inverse \\(\\mathbf{A}^{-1}\\). Proof: in class 8.2 Finding the inverse of \\(\\mathbf{A}\\) The previous theorem states that for a \\(n \\times n\\) invertible matrix \\(\\mathbf{A}\\), the elementary row operations that covert \\(\\mathbf{A}\\) to \\(\\mathbf{I}\\) also convert \\(\\mathbf{I}\\) to \\(\\mathbf{A}^{-1}\\). This suggests an algorithm for finding the inverse \\(\\mathbf{A}^{-1}\\) of \\(\\mathbf{A}\\): Create the augmented matrix \\(\\begin{pmatrix} \\mathbf{A} &amp; \\mathbf{I} \\end{pmatrix}\\) and row reduce the augmented matrix. If the row-reduced augmented matrix is of the form \\(\\begin{pmatrix} \\mathbf{I} &amp; \\mathbf{A}^{-1} \\end{pmatrix}\\) then \\(\\mathbf{A}^{-1}\\) is the inverse of \\(\\mathbf{A}\\). If the leading matrix in the augmented matrix is not the identity matrix \\(\\mathbf{I}\\), then \\(\\mathbf{A}\\) is not row equivalent to \\(\\mathbf{I}\\) and is therefore not invertible. Example: Let \\(\\mathbf{A} = \\begin{pmatrix} -3 &amp; -3 &amp; -4 \\\\ -4 &amp; 2 &amp; -4 \\\\ 4 &amp; -4 &amp; 4 \\end{pmatrix}\\). Does \\(\\mathbf{A}\\) have an inverse, and if so, what is it? Example in R 8.3 The Invertible Matrix Theorem The Invertible Matrix Theorem: Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix. Then the following statements are equivalent (i.e., they are all either simultaneously true or false). \\(\\mathbf{A}\\) is an invertible matrix. \\(\\mathbf{A}\\) is row equivalent to the \\(n \\times n\\) identity matrix \\(\\mathbf{I}\\) (\\(\\mathbf{A} \\sim \\mathbf{I}\\)). \\(\\mathbf{A}\\) and \\(n\\) pivot columns. The homogeneous matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\). The columns of \\(\\mathbf{A}\\) are linearly independent. The linear transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) given by the matrix transformation \\(\\mathbf{x} \\rightarrow \\mathbf{A}\\mathbf{x}\\) is one-to-one. The inhomogeneous matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a unique solution for all \\(\\mathbf{b} \\in \\mathcal{R}^n\\). The columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^n\\). The linear transformation \\(\\mathbf{x} \\rightarrow \\mathbf{A} \\mathbf{x}\\) maps \\(\\mathcal{R}^n\\) onto \\(\\mathcal{R}^n\\). There is an \\(n \\times n\\) matrix \\(\\mathbf{C}\\) such that \\(\\mathbf{C}\\mathbf{A} = \\mathbf{I}\\). There is an \\(n \\times n\\) matrix \\(\\mathbf{D}\\) such that \\(\\mathbf{A}\\mathbf{D} = \\mathbf{I}\\). \\(\\mathbf{A}&#39;\\) is an invertible matrix. Proof: in class A result of the invertible matrix theorem is that if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are \\(n \\times n\\) matrices with \\(\\mathbf{A} \\mathbf{B} = \\mathbf{I}\\) then \\(\\mathbf{A} = \\mathbf{B}^{-1}\\) and \\(\\mathbf{B} = \\mathbf{A}^{-1}\\). 8.4 Invertible Linear Transformations Definition 8.2 A linear transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) is said to be invertible if there exists a transformation \\(S:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) such that \\[ \\begin{align*} S(T(\\mathbf{x})) = \\mathbf{x} &amp;&amp; \\mbox{for all } \\mathbf{x} \\in \\mathcal{R}^n T(S(\\mathbf{x})) = \\mathbf{x} &amp;&amp; \\mbox{for all } \\mathbf{x} \\in \\mathcal{R}^n \\\\ \\end{align*} \\] Draw figure in class Theorem 8.5 Let \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) be a linear transformation and let \\(\\mathbf{A}\\) be the matrix representing the transformation \\(T\\). Then the transformation \\(T\\) is invertible if and only if the matrix \\(\\mathbf{A}\\) is invertible. Therefore, the matrix that represents \\(S:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\), the inverse transformation of \\(T\\), is unique and is represented by the matrix \\(\\mathbf{A}^{-1}\\). "],["section-block-matrices.html", "Chapter 9 Block Matrices 9.1 Block Matrix Addition 9.2 Block Matrix Multiplication 9.3 The column-row matrix product 9.4 Special Block Matrices", " Chapter 9 Block Matrices Another way to represent matrices is using a block (or partitioned) form. A block-representation of a matrix arises when the \\(n \\times p\\) matrix \\(\\mathbf{A}\\) is represented using smaller blocks as follows: \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} &amp; \\cdots &amp; \\mathbf{A}_{1K} \\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} &amp; \\cdots &amp; \\mathbf{A}_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{J1} &amp; \\mathbf{A}_{J2} &amp; \\cdots &amp; \\mathbf{A}_{JK} \\\\ \\end{pmatrix} \\\\ \\end{align*} \\] where \\(\\mathbf{A}_{ij}\\) is a \\(n_j \\times p_k\\) matrix where \\(\\sum_{j=1}^J n_j = n\\) and \\(\\sum_{k=1}^K p_k = p\\). For example, the matrix \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} 5 &amp; 7 &amp; 1 \\\\ 5 &amp; -22 &amp; 2 \\\\ -14 &amp; 5 &amp; 99 \\\\ 42 &amp; -3 &amp; 0\\end{pmatrix}, \\end{align*} \\] can be written in block matrix form with \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} \\begin{bmatrix} 5 &amp; 7 \\\\ 5 &amp; -22 \\end{bmatrix} &amp; \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\\\ \\begin{bmatrix} -14 &amp; 5 \\\\ 42 &amp; -3 \\end{bmatrix} &amp; \\begin{bmatrix} 99 \\\\ 0 \\end{bmatrix} \\end{pmatrix}, \\end{align*} \\] where \\(\\mathbf{A}_{11} = \\begin{bmatrix} 5 &amp; 7 \\\\ 5 &amp; -22 \\end{bmatrix}\\) is a \\(2 \\times 2\\) matrix, \\(\\mathbf{A}_{12} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) is a \\(1 \\times 2\\) matrix, etc. A_11 &lt;- matrix(c(5, 5, 7, -22), 2, 2) A_12 &lt;- c(1, 2) A_21 &lt;- matrix(c(-14, 42, 5, -3), 2, 2) A_22 &lt;- c(99, 0) ## bind columns then rows rbind( cbind(A_11, A_12), cbind(A_21, A_22) ) ## A_12 ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 ## bind rows then columns cbind( rbind(A_11, A_21), c(A_12, A_22) ## rbind on vectors is different than c() ) ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 ## bind rows then columns cbind( rbind(A_11, A_21), ## convert the vectors to matrices for rbind rbind(as.matrix(A_12), as.matrix(A_22)) ) ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 9.1 Block Matrix Addition If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m \\times n\\) block matrices with blocks in \\(r\\) rows and \\(c\\) columns where \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} &amp; \\cdots &amp; \\mathbf{A}_{1c}\\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} &amp;\\cdots &amp; \\mathbf{A}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{r1} &amp; \\mathbf{A}_{r2} &amp;\\cdots &amp; \\mathbf{A}_{rc} \\\\ \\end{pmatrix} &amp; \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{B}_{11} &amp; \\mathbf{B}_{12} &amp; \\cdots &amp; \\mathbf{B}_{1c}\\\\ \\mathbf{B}_{21} &amp; \\mathbf{B}_{22} &amp;\\cdots &amp; \\mathbf{B}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{B}_{r1} &amp; \\mathbf{B}_{r2} &amp;\\cdots &amp; \\mathbf{B}_{rc} \\\\ \\end{pmatrix} \\\\ \\end{align*} \\] and each block \\(\\mathbf{A}_{ij}\\) and \\(\\mathbf{B}_ij\\) have the same dimension, then \\[ \\begin{align} \\tag{9.1} \\mathbf{A} + \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} + \\mathbf{B}_{11} &amp; \\mathbf{A}_{12} + \\mathbf{B}_{12} &amp; \\cdots &amp; \\mathbf{A}_{1c} + \\mathbf{B}_{1c}\\\\ \\mathbf{A}_{21} + \\mathbf{B}_{21} &amp; \\mathbf{A}_{22} + \\mathbf{B}_{22} &amp; \\cdots &amp; \\mathbf{A}_{2c} + \\mathbf{B}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{r1} + \\mathbf{B}_{r1} &amp; \\mathbf{A}_{r2} + \\mathbf{B}_{r2} &amp; \\cdots &amp; \\mathbf{A}_{rc} + \\mathbf{B}_{rc} \\\\ \\end{pmatrix} \\end{align} \\] which is a matrix where each block is the sum of the other blocks. Notice that if each block was a scalar rather than a block matrix, this would be the usual definition of matrix addition (compare equation (7.1) above to (9.1)). The one requirement is that each of the blocks \\(\\mathbf{A}_{ij}\\) and \\(\\mathbf{B}_ij\\) have the same dimension. When this is true, we say that \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are conformable for block matrix addition. 9.2 Block Matrix Multiplication If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m \\times n\\) block matrices with blocks in \\(r\\) rows and \\(c\\) columns (same as above) where \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} &amp; \\cdots &amp; \\mathbf{A}_{1c}\\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} &amp;\\cdots &amp; \\mathbf{A}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{r1} &amp; \\mathbf{A}_{r2} &amp;\\cdots &amp; \\mathbf{A}_{rc} \\\\ \\end{pmatrix} &amp; \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{B}_{11} &amp; \\mathbf{B}_{12} &amp; \\cdots &amp; \\mathbf{B}_{1c}\\\\ \\mathbf{B}_{21} &amp; \\mathbf{B}_{22} &amp;\\cdots &amp; \\mathbf{B}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{B}_{r1} &amp; \\mathbf{B}_{r2} &amp;\\cdots &amp; \\mathbf{B}_{rc} \\\\ \\end{pmatrix} \\\\ \\end{align*} \\] and each row of blocks \\(\\mathbf{A}_{ij}\\) has the same number of columns as the block \\(\\mathbf{B}_ij\\) has rows, then the block matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are said to be conformable for block matrix multiplication. A consequence of this is that \\(r = c\\). When this is the case, the matrix products is \\[ \\begin{align*} \\tag{9.2} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} \\sum_{j = 1}^c \\mathbf{A}_{1j} \\mathbf{B}_{j1} &amp; \\sum_{j = 1}^c \\mathbf{A}_{1j} \\mathbf{B}_{j2} &amp; \\cdots &amp; \\sum_{j = 1}^c \\mathbf{A}_{1j} \\mathbf{B}_{jc} \\\\ \\sum_{j = 1}^c \\mathbf{A}_{2j} \\mathbf{B}_{j1} &amp; \\sum_{j = 1}^c \\mathbf{A}_{2j} \\mathbf{B}_{j2} &amp; \\cdots &amp; \\sum_{j = 1}^c \\mathbf{A}_{2j} \\mathbf{B}_{jc} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum_{j = 1}^c \\mathbf{A}_{rj} \\mathbf{B}_{j1} &amp; \\sum_{j = 1}^c \\mathbf{A}_{rj} \\mathbf{B}_{j2} &amp; \\cdots &amp; \\sum_{j = 1}^c \\mathbf{A}_{rj} \\mathbf{B}_{jc} \\end{pmatrix} \\end{align*} \\] which can be said in words as \"each block-element (the \\(ij\\)th element (\\(\\mathbf{A} \\mathbf{B}\\))_{ij}) of the block-matrix product \\(\\mathbf{A} \\mathbf{B}\\) is the sum of the \\(i\\)th block-row of \\(\\mathbf{A}\\) and the \\(j\\)th block column of \\(\\mathbf{B}\\) .Notice that if each block was a scalar rather than a block matrix, this would be the usual definition of matrix multiplication (compare equation (7.2) above to (9.2)). Example 9.1 in class 9.3 The column-row matrix product Theorem 9.1 The matrix product \\(\\mathbf{A}\\mathbf{B}\\) of an \\(m \\times n\\) matrix \\(\\mathbf{A} = \\begin{pmatrix} \\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\cdots &amp; \\mathbf{a}_n \\end{pmatrix}\\) with columns \\(\\{\\mathbf{a}_i\\}_{i=1}^n\\) and an \\(n \\times p\\) matrix \\(\\mathbf{B} = \\begin{pmatrix} \\mathbf{b}_1&#39; \\\\ \\mathbf{b}_2&#39; \\\\ \\vdots \\\\ \\mathbf{b}_n&#39; \\end{pmatrix}\\) with rows \\(\\{\\mathbf{b}_i&#39;\\}_{i=1}^n\\) can be written as the column-row expansion below: \\[ \\begin{align*} \\tag{9.2} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\cdots &amp; \\mathbf{a}_n \\end{pmatrix} \\begin{pmatrix} \\mathbf{b}_1&#39; \\\\ \\mathbf{b}_2&#39; \\\\ \\vdots \\\\ \\mathbf{b}_n&#39; \\end{pmatrix} \\\\ &amp; = \\mathbf{a}_1 \\mathbf{b}_1&#39; + \\mathbf{a}_2 \\mathbf{b}_2&#39; + \\cdots + \\mathbf{a}_n \\mathbf{b}_n&#39; \\end{align*} \\] Note: The notation \\(\\mathbf{b}_i&#39;\\) has a transpose because a vector is defined in the vertical orientation (column vector). Therefore, to formally define a row vector, we take a vertical vector of the values in the row and take its transpose to turn the column vector into a row vector. Example 9.2 in class 9.4 Special Block Matrices There are many different forms of block matrices. Two that deserve special mention here include block diagonal matrices and block triangular matrices. Definition 9.1 The matrix \\(\\mathbf{A}\\) is said to be block diagonal if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_1 &amp; \\mathbf{O} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{A}_2 &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{A}_3 &amp; \\cdots &amp; \\mathbf{0} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{A}_n \\\\ \\end{pmatrix} \\end{align*} \\] Definition 9.2 The matrix \\(\\mathbf{A}\\) is said to be block (upper) triangular if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} &amp; \\mathbf{A}_{13} &amp; \\cdots &amp; \\mathbf{A}_{1n} \\\\ \\mathbf{0} &amp; \\mathbf{A}_{22} &amp; \\mathbf{A}_{23} &amp; \\cdots &amp; \\mathbf{A}_{2n} \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{A}_{33} &amp; \\cdots &amp; \\mathbf{A}_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{A}_{nn} \\\\ \\end{pmatrix} \\end{align*} \\] \\(\\mathbf{A}\\) is block (lower) triangular if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{A}_{31} &amp; \\mathbf{A}_{32} &amp; \\mathbf{A}_{33} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{m1} &amp; \\mathbf{A}_{m2} &amp; \\mathbf{A}_{m3} &amp; \\cdots &amp; \\mathbf{A}_{mn} \\\\ \\end{pmatrix} \\end{align*} \\] Example 9.3 Assume that \\(\\mathbf{A}\\), which has the form \\[ \\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} \\\\ \\mathbf{0} &amp; \\mathbf{A}_{22} \\end{pmatrix}, \\] is an invertible matrix with \\(\\mathbf{A}_{11}\\) a \\(p \\times p\\) matrix, \\(\\mathbf{A}_{12}\\) a \\(p \\times q\\) matrix, and $_{22} a \\(q \\times q\\) matrix. Solve for $^{-1} "],["section-matrix-factorizations.html", "Chapter 10 Matrix Factorizations 10.1 The LU factorization 10.2 Obtaining the LU factorization 10.3 The Cholesky factor", " Chapter 10 Matrix Factorizations library(tidyverse) library(dasc2594) library(mvnfast) In scalar mathematics, a factorization is an expression that writes a scalar \\(a\\) as a product of two or more scalars. For example, the scalar 2 has a square-root factorization of \\(2 =\\sqrt{2} * \\sqrt{2}\\) and 15 has a prime factorization of \\(15 = 3 * 5\\). A matrix factorization is a similar concept where a matrix \\(\\mathbf{A}\\) can be represented by a product or two or more matrices (e.g., \\(\\mathbf{A} = \\mathbf{B} \\mathbf{C}\\)). In data science, matrix factorizations are fundamental to working with data. 10.1 The LU factorization First, we define lower and upper triangular matrices. Definition 10.1 The matrix \\(\\mathbf{A}\\) is said to be lower triangular if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} a_{11} &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ a_{21} &amp; a_{22} &amp; 0 &amp; \\cdots &amp; 0 \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\cdots &amp; a_{nn} \\\\ \\end{pmatrix} \\end{align*} \\] Similarly, the matrix \\(\\mathbf{A}\\) is said to be upper triangular if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\cdots &amp; a_{1n} \\\\ 0 &amp; a_{22} &amp; a_{23} &amp; \\cdots &amp; a_{2n} \\\\ 0 &amp; 0 &amp; a_{33} &amp; \\cdots &amp; a_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; a_{nn} \\\\ \\end{pmatrix} \\end{align*} \\] The LU factorization of a matrix \\(\\mathbf{A}\\) reduces the matrix \\(\\mathbf{A}\\) into two components. The first component \\(\\mathbf{L}\\) is a lower-triangular matrix and the second component \\(\\mathbf{U}\\) is an upper triangular matrix. Using the LU factorization, the matrix factorization \\(\\mathbf{A} = \\mathbf{L} \\mathbf{U}\\) can be used in the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{L} \\mathbf{U}\\mathbf{x} = \\mathbf{b}\\) by first solving the sub-equation \\(\\mathbf{L} \\mathbf{y} = \\mathbf{b}\\) and then solving the second sub-equation \\(\\mathbf{U} \\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\). Thus, the matrix factorization applied to the matrix equation gives the pair of equations \\[ \\begin{align*} \\tag{10.1} \\mathbf{L} \\mathbf{y} &amp; = \\mathbf{b} \\\\ \\mathbf{U} \\mathbf{x} &amp; = \\mathbf{y} \\end{align*} \\] At first glance, this seems like we are trading the challenge of solving one system of equations \\(\\mathbf{A}\\mathbf{x}\\) (4.1) for the two equations in (10.1). However, the computational benefits arise due to the fact that \\(\\mathbf{L}\\) and \\(\\mathbf{U}\\) are triangular matrices and solving matrix equations with triangular matrices is much faster. Example 10.1 in class: Let \\(\\mathbf{A} = \\begin{pmatrix} -2 &amp; 4 &amp; -6 &amp; 2 \\\\ -2 &amp; 4 &amp; -6 &amp; 2 \\\\ -3 &amp; 0 &amp; -1 &amp; -3 \\\\ -3 &amp; 6 &amp; -5 &amp; -1 \\end{pmatrix}\\) which has the LU decomposition \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} -2 &amp; 4 &amp; -6 &amp; 2 \\\\ -2 &amp; 4 &amp; -6 &amp; 2 \\\\ -3 &amp; 0 &amp; -1 &amp; -3 \\\\ -3 &amp; 6 &amp; -5 &amp; -1 \\end{pmatrix} = \\begin{pmatrix} -2 &amp; 0 &amp; 0 &amp; 0 \\\\ -2 &amp; 0 &amp; 0 &amp; 0 \\\\ -3 &amp; 2 &amp; 2 &amp; 0 \\\\ -3 &amp; 0 &amp; 2 &amp; -1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; -2 &amp; 3 &amp; -1 \\\\ 0 &amp; -3 &amp; 2 &amp; -1 \\\\ 0 &amp; 0 &amp; 2 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\end{align*} \\] solve \\(\\mathbf{L} \\mathbf{y} = \\mathbf{b}\\) using augmented matrix solve \\(\\mathbf{U} \\mathbf{x} = \\mathbf{y}\\) using augmented matrix * compare to the solution \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) Exercise 10.1 in lab: Solve some large systems of equations by brute force which shows how the LU decomposition is faster. 10.1.1 Geometric interpretation of the LU factorization Draw image in class 10.2 Obtaining the LU factorization Notice that the upper-triangular matrix \\(\\mathbf{U}\\) is in echelon form. Congratulations! you know how to construct a matrix \\(\\mathbf{U}\\) by reducing the matrix \\(\\mathbf{A}\\) to an echelon form \\(\\mathbf{U}\\) using elementary matrices \\(\\mathbf{E}_1, \\ldots \\mathbf{E}_k\\). Now, we only need to find the lower triangular matrix \\(\\mathbf{L}\\). Combining the LU factorization and the fact that we can find an upper triangular matrix \\(\\mathbf{U}\\) using elementary row matrices, we have \\[ \\begin{align} \\tag{10.2} \\mathbf{A} &amp; = \\mathbf{L} \\mathbf{U} \\\\ \\mathbf{E}_k \\cdots \\mathbf{E}_1 \\mathbf{A} &amp; = \\mathbf{U}. \\end{align} \\] We also know that each of the elementary row matrices \\(\\mathbf{E}_j\\) are invertible (you can always re-swap rows, subtract instead of add rows, etc.) which says that each inverse \\(\\mathbf{E}_j^{-1}\\) exists. Thus, the product \\(\\mathbf{E}_k \\cdots \\mathbf{E}_1\\) must have an inverse which is \\[ \\begin{align*} (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1} &amp; = \\mathbf{E}_1^{-1} \\cdots \\mathbf{E}_k^{-1}. \\end{align*} \\] Plugging this inverse into (10.2) gives (left multiplying by \\((\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\) on both sides) \\[ \\begin{align*} (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1} (\\mathbf{E}_k \\cdots \\mathbf{E}_1) \\mathbf{A} &amp; = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\mathbf{U} \\\\ \\mathbf{A} &amp; = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\mathbf{U} \\\\ &amp; = \\mathbf{L} \\mathbf{U} \\end{align*} \\] where \\(\\mathbf{L} = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\) Algorithm for finding the LU decomposition Given the matrix \\(\\mathbf{A}\\) Find elementary matrices \\(\\mathbf{E}_1, \\ldots, \\mathbf{E}_k\\) such that \\(\\mathbf{E}_k \\cdots \\mathbf{E}_1 \\mathbf{A}\\) is in row echelon form (if this is possible, otherwise an LU factorization does not exist). Call this matrix \\(\\mathbf{U}\\), the upper triangular component of the LU factorization. The, the lower triangular \\(\\mathbf{L} = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\). Notice that the algorithm does not say to find a specific matrix \\(\\mathbf{U}\\). In general, any row echelon form matrix \\(\\mathbf{U}\\) will work. 10.3 The Cholesky factor A Cholesky decomposition is special type of LU decomposition. A Cholesky decomposition is an LU decomposition on a symmetric, positive-definite square matrix. Definition 10.2 * A matrix \\(\\mathbf{A}\\) is said to by symmetric if \\(\\mathbf{A} = \\mathbf{A}^{-1}\\) A \\(n \\times n\\) matrix is said to be positive definite if for all \\(\\mathbf{x} \\in \\mathcal{R}^n\\), the quadratic form \\(\\mathbf{x}&#39; \\mathbf{A }\\mathbf{x} \\geq 0\\) Note: the condition of positive definiteness is actually impossible to check. Can you show this is true for all vectors? Luckily, a \\(n \\times n\\) symmetric matrix is positive definite if and only if the matrix \\(\\mathbf{A}\\) is invertible (which we know about by the invertible matrix theorem 8.3). Definition 10.3 Let \\(\\mathbf{A}\\) be a symmetric, positive definite matrix (by this, \\(\\mathbf{A}\\) is a \\(n \\times n\\) square matrix). Then \\[ \\begin{align*} \\mathbf{A} = \\mathbf{L} \\mathbf{L}&#39; \\end{align*} \\] is the Cholesky decomposition of \\(\\mathbf{A}\\) if \\(\\mathbf{L}\\) is a lower-triangular matrix. Also, the lower triangular Cholesky matrix \\(\\mathbf{L}\\) is unique. What makes the Cholesky factor special? The decomposition \\(\\mathbf{A} = \\mathbf{L} \\mathbf{U}\\) has the property that \\(\\mathbf{U} = \\mathbf{L}&#39;\\) so that the computer only has to store one of the matrix components (reduce memory demands). As about half of the elements of \\(\\mathbf{L}\\) are 0, matrix multiplication is much less computationally demanding as about half of the flops are not required to be evaluated (x * 0 = 0). The Cholesky factor is unique. There is only one Cholesky factor for each symmetric positive definite matrix. The Cholesky has properties related to multivariate normal distributions. Let \\(\\mathbf{y} \\sim \\operatorname{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})\\), and \\(\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}&#39;\\). Then, if \\(\\mathbf{z} \\sim \\operatorname{N}(\\mathbf{0}, \\mathbf{I})\\), then \\(\\mathbf{L} \\mathbf{z} \\sim \\operatorname{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})\\). We say the \\(\\mathbf{y}\\) and \\(\\mathbf{L}\\mathbf{z}\\) are equal in distribution. # simulate N 2-dimensional random normal vectors N &lt;- 5000 mu &lt;- rep(0, 2) Sigma &lt;- matrix(c(2, 1.5, 1.5, 2), 2, 2) y &lt;- rmvn(N, mu, Sigma) # calculate the Cholesky factor L &lt;- t(chol(Sigma)) # R calculates the upper (right) Cholesky factor by default z &lt;- rmvn(N, mu, diag(2)) Lz &lt;- t(L %*% t(z)) # pay attention to the dimensions of L and z here... data.frame( observation = 1:N, x1 = c(y[, 1], z[, 1], Lz[, 1]), x2 = c(y[, 2], z[, 2], Lz[, 2]), variable = factor(rep(c(&quot;y&quot;, &quot;z&quot;, &quot;Lz&quot;), each = N), levels = c(&quot;y&quot;, &quot;z&quot;, &quot;Lz&quot;)) ) %&gt;% ggplot(aes(x = x1, y = x2, color = variable)) + geom_point(alpha = 0.1) + geom_density2d() + facet_wrap(~ variable) "],["section-subspaces-Rn.html", "Chapter 11 Subspaces of \\(\\mathcal{R}^n\\) 11.1 Special subspaces: column space and null space 11.2 The basis of a subspace", " Chapter 11 Subspaces of \\(\\mathcal{R}^n\\) First, let’s recall the definition of a subset. A set \\(A\\) is a subset of a set \\(B\\) if all elements of \\(A\\) are also members of \\(B\\). For example, the integers \\(\\mathcal{Z}\\) are a subset of the real numbers \\(\\mathbf{R}\\) (\\(\\mathcal{Z} \\subset \\mathcal{R}\\)) and the real numbers are a subset of the complex numbers \\(\\mathcal{C}\\) (\\(\\mathcal{R} \\subset \\mathcal{C}\\)). Subspaces are a generalization of the idea of subsets that are useful for understanding vector spaces. Definition 11.1 A subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) is a set that has the properties The zero vector \\(\\mathbf{0} \\in \\mathcal{H}\\) For each \\(\\mathbf{u}, \\mathbf{v} \\in \\mathcal{H}\\), the sum \\(\\mathbf{u} + \\mathbf{v}\\) is in \\(\\mathcal{H}\\) For each \\(\\mathbf{u} \\in \\mathcal{H}\\) and scalar \\(c\\), the scalar multiple \\(c \\mathbf{u}\\) is in \\(\\mathcal{H}\\) Example 11.1 Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors in \\(\\mathcal{R}^n\\). Then the vector space defined by span\\(\\{\\mathbf{u}, \\mathbf{v} \\}\\) is a subspace of \\(\\mathcal{R}^n\\) Show this in class. Exercise 11.1 * Is a line through the origin a subspace? * Is a line not through the origin a subspace? Note: For any vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_k \\in \\mathcal{R}^n\\), the span\\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}\\) is a subspace of \\(\\mathcal{R}^n\\). 11.1 Special subspaces: column space and null space Definition 11.2 The column space, denoted \\(\\operatorname{col}(\\mathbf{A})\\), of a \\(m \\times n\\) matrix \\(\\mathbf{A}\\) which has columns \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_n \\in \\mathcal{R}^m\\) is the set of vectors that are linear combinations of the columns of \\(\\mathbf{A}\\) which is equivalent to the span\\(\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}\\). Example 11.2 in class Definition 11.3 The null space, denoted \\(\\operatorname{null}(\\mathbf{A})\\), of a matrix \\(\\mathbf{A}\\) is the set of all solutions to the homogeneous matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\). While the idea of a null space seems unclear, the null space is the set of all vectors which the matrix transformation defined by \\(\\mathbf{A}\\) maps to \\(\\mathbf{0}\\). Theorem 11.1 The null space of a n \\(m \\times m\\) matrix \\(\\mathbf{A}\\) is a subspace of \\(\\mathcal{R}^n\\). Proof. Do in class Example: give \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) and determine if \\(\\mathbf{x}\\) is in the null space of \\(\\mathbf{A}\\) using R 11.2 The basis of a subspace Definition 11.4 A basis for a subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) is a linearly independent set in \\(\\mathcal{H}\\) that spans \\(\\mathbf{H}\\). Equivlently, a baisis is a set of linearly independent vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\) such that span\\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\} = \\mathcal{H}\\). The requirement that the vectors of a basis are linearly independent while spanning a subspace \\(\\mathcal{H}\\) means that a basis is a minimal spanning set for the subspace \\(\\mathcal{H}\\) Question: Is a basis unique? Definition 11.5 The standard basis for \\(\\mathcal{R}^n\\) Example 11.3 Basis for \\(\\mathcal{R}^3\\) Example 11.4 Do the following set of vectors form a basis for \\(\\mathcal{R}^3\\)? in class Example 11.5 Using R, find a basis for the null space of the matrix \\[ \\mathbf{A} = \\begin{pmatrix} 2 &amp; 4 &amp; 1 &amp; 3 \\\\ -1 &amp; -2 &amp; 6 &amp; 5 \\\\ 1 &amp; 2 &amp; -3 &amp; 2 \\end{pmatrix} \\] Theorem 11.2 The pivot columns of a matrix \\(\\mathbf{A}\\) for a basis for the column space of \\(\\mathbf{A}\\). Note: Use the columns of \\(\\mathbf{A}\\), not the columns of the matrix in echelon form. corollary Corollary 11.1 Example 11.6 Using R, find a basis for the column space of the matrix \\[ \\mathbf{A} = \\begin{pmatrix} 3 &amp; 1 &amp; 2 &amp; -3 \\\\ 4 &amp; 1 &amp; -3 &amp; -2 \\\\ 4 &amp; -1 &amp; -3 &amp; 1 \\end{pmatrix} \\] "],["section-dimension-and-rank.html", "Chapter 12 Dimension and Rank 12.1 Coordinate systems 12.2 Dimension of a subspace", " Chapter 12 Dimension and Rank 12.1 Coordinate systems Recall the idea of polynomials (e.g., a polynomial of order \\(p\\) is \\(a_1x^p + a_2x^{p-1} + \\ldots + a_p x^1 + a_{p+1} x^0\\)) where the polynomials \\(x^p, x^{p-1}, \\ldots, x^1, x^0\\) form a set of powers up to the power \\(p\\) of \\(x\\) from which the coefficients \\(a_p, \\ldots, a_{p+1}\\) can be used to make any polynomial of order \\(p\\). It can be said that the powers of \\(x\\) (\\(x^p, x^{p-1}, \\ldots, x^1, x^0\\)) form a basis for all polynomials of order \\(p\\). In the previous section, we extended this analogy to vector spaces using the concept of a minimal spanning set. Consider the basis \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) for a subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) where span\\(\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\} = \\mathcal{H}\\). Because the set \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) is a basis, the set of vectors is linearly independent. Then, because the set \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) is a basis, we have the following result. Theorem 12.1 For each vector \\(\\mathbf{x}\\) in the subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\), and a basis \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\), there is a unique set of coefficients \\(a_1, \\ldots, a_k\\) such that \\[\\begin{align*} \\mathbf{x} &amp; = a_1 \\mathbf{b}_1 + \\cdots + a_k \\mathbf{b}_k \\end{align*}\\] Proof. In class: assume contradiction that there are two ways \\(a_1, \\ldots, a_k\\) and \\(b_1, \\ldots, b_k\\)… Definition 12.1 Let \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\}\\) be a basis for a subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\). Then, for each \\(\\mathbf{x} \\in \\mathcal{H}\\), the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\) are the set of coefficients \\(\\{a_1, \\ldots, a_k\\}\\) where \\[\\begin{align*} \\mathbf{x} &amp; = a_1 \\mathbf{b}_1 + \\cdots + a_k \\mathbf{b}_k. \\end{align*}\\] Example 12.1 Let \\(\\mathcal{B} = \\left\\{ \\mathbf{b}_1 = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1\\end{pmatrix}, \\mathbf{b}_2 = \\begin{pmatrix} 2 \\\\ -3 \\\\ 1\\end{pmatrix} \\right\\}\\) and \\(\\mathbf{x} = \\begin{pmatrix} 5 \\\\ 6 \\\\ 1\\end{pmatrix}\\). What are the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\)? Solution. In class: write out vector equation with coefficients, put in augmented matrix form, find consistent solution 12.2 Dimension of a subspace Definition 12.2 The dimension \\(\\operatorname{dim}(\\mathcal{H})\\) of a nonzero subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) is the number of (nonzero) vectors that make up a basis \\(\\mathcal{B}\\) for \\(\\mathcal{H}\\). The dimension of the subspace \\(\\mathcal{H} = \\{\\mathbf{0}\\}\\) is 0. Note that under this definition, the basis \\(\\mathcal{B}\\) is not unique. For example, the following bases for the 2-dimensional subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^3\\) both have two linearly independent vectors. \\[\\begin{align*} \\mathcal{B}_1 = \\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\} &amp;&amp; \\mathcal{B}_2 = \\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\} \\end{align*}\\] For example, let \\(\\mathbf{x} = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix}\\). Then under the basis \\(\\mathcal{B}_1\\), the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}_1\\) are \\(a_1 = 3\\) and \\(a_2 = 4\\) because \\[\\begin{align*} \\mathbf{x} = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + 4 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\end{align*}\\] while the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}_2\\) are \\(a_1 = 3\\) and \\(a_2 = 1\\) because \\[\\begin{align*} \\mathbf{x} = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}. \\end{align*}\\] Exercise 12.1 What is the dimension of a basis for \\(\\mathcal{R}^n\\)? Exercise 12.2 What is the dimension of a subspace that is a plane in 3 dimensions? Definition 12.3 The rank \\(\\operatorname{rank}(\\mathbf{A})\\) of a matrix \\(\\mathbf{A}\\) is the dimension of the column space of \\(\\mathcal{A}\\). Recall that the pivot columns of \\(\\mathbf{A}\\) form a basis for the column space of \\(\\mathbf{A}\\). Hence, the number of pivot columns in the matrix \\(\\mathbf{A}\\) is the rank of the matrix \\(\\mathbf{A}\\). Example 12.2 Determine the rank of the following matrix: in class example Theorem 12.2 (The Rank Theorem) If a matrix \\(\\mathbf{A}\\) has \\(n\\) columns, then \\(\\operatorname{rank}(\\mathbf{A}) + \\operatorname{dim}(\\operatorname{null}(\\mathbf{A})) = n\\) Proof. in class: sketch– rank(A) is number of linearly independent columns. null(A) is number of linearly dependent columns (solutions to Ax=0) The following theorem states that any \\(p\\) vectors in \\(\\mathcal{R}^p\\) that are linearly independent must span \\(\\mathcal{R}^p\\). Theorem 12.3 (The Basis Theorem) Let \\(\\mathcal{H}\\) be a p-dimensional subspace of \\(\\mathcal{R}^n\\). Then any linearly independent set of \\(p\\) elements in \\(\\mathcal{H}\\) is a basis for \\(\\mathcal{H}\\). Equivalently, any set of \\(p\\) elements of \\(\\mathcal{H}\\) that span \\(\\mathcal{H}\\) is a basis for \\(\\mathcal{H}\\) Theorem 12.4 (Invertible Matrix Theorem (again)) Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) matrix. The the following statements are equivalent to \\(\\mathbf{A}\\) being an invertible matrix: The columns of \\(\\mathbf{A}\\) form a basis for \\(\\mathcal{R}^n\\) \\(\\operatorname{col}(\\mathbf{A}) = \\mathcal{R}^n\\) \\(\\operatorname{dim}(\\operatorname{col}(\\mathbf{A})) = n\\) \\(\\operatorname{rank}(\\mathbf{A}) = n\\) \\(\\operatorname{null}(\\mathbf{A}) = \\{\\mathbf{0}\\}\\) \\(\\operatorname{dim}(\\operatorname{null}(\\mathbf{A})) = 0\\) "],["section-determinants.html", "Chapter 13 Determinants", " Chapter 13 Determinants "],["section-vectors-and-matrices.html", "Chapter 14 Vectors and matrices 14.1 Exercises", " Chapter 14 Vectors and matrices This can produce an error, so we wrap the function array_to_latex in a call to cat() A &lt;- matrix(c(3,4,5,6,7,9,4,5,122), ncol=3, byrow=TRUE) array_to_latex(A) [1] “\\begin{pmatrix} 3 &amp; 4 &amp; 5 \\\\ 6 &amp; 7 &amp; 9 \\\\ 4 &amp; 5 &amp; 122 \\end{pmatrix}” A &lt;- matrix(c(3,4,5,6,7,9,4,5,122), ncol=3, byrow=TRUE) cat(array_to_latex(A)) \\[\\begin{pmatrix} 3 &amp; 4 &amp; 5 \\\\ 6 &amp; 7 &amp; 9 \\\\ 4 &amp; 5 &amp; 122 \\end{pmatrix}\\] The fundamental objects in this text are scalars, vectors, and matrices. 14.0.1 Arrays Higher order arrays (for example, tensors in the tensorflow library) can be represented using subscript notation where \\([\\mathbf{A}_1 | \\mathbf{A}_2 | \\cdots \\mathbf{A}_n]\\) is a 3-dimensional array. Higher order arrays can be represented using additional subscripts. 14.0.2 Lists To add: vector addition, multiplication To add: matrix addition, multiplication To add: determinants 14.1 Exercises What is 3 + \\(\\begin{pmatrix} 4 \\\\ 7 \\\\ 3 \\end{pmatrix}\\)? Why can’t you add the following two vectors: \\[ \\begin{align*} \\mathbf{x} = \\begin{pmatrix} 14 \\\\ 3 \\\\ 3 \\\\ -5 \\end{pmatrix} &amp; &amp; \\mathbf{y} = \\begin{pmatrix} 4 \\\\ 7 \\\\ 3 \\end{pmatrix} \\end{align*} \\] Based on the notation, what type of object is \\(\\mathbf{x}&#39;\\)? \\(\\mathbf{x}&#39; \\mathbf{y}\\)? \\(\\mathbf{x}&#39; \\mathbf{A}\\)? \\(\\mathbf{A}&#39; \\mathbf{y}\\)? \\(\\mathbf{X}&#39; \\mathbf{Z}\\)? "]]
