[["index.html", "Multivariable Mathematics for Data Science Chapter 1 Preface 1.1 Getting started in R 1.2 Notation", " Multivariable Mathematics for Data Science John Tipton 2021-04-16 Chapter 1 Preface This book will introduce students to multivariable Calculus and linear algebra methods and techniques to be successful in data science, statistics, computer science, and other data-driven, computational disciplines. The motiviation for this text is to provide both a theoretical understanding of important multivariable methods used in data science as well as giving a hands-on experience using software. Throughout this text, we assume the reader has a solid foundation in univariate calculus (typically two semesters) as well as familiarity with a scripting language (e.g., R or python). 1.1 Getting started in R TBD 1.2 Notation For notation, we let lowercase Roman letters represent scalar numbers (e.g., n = 5, d = 7), lowercase bold letters represent vectors \\[ \\begin{align*} \\textbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\end{align*} \\] where the elements \\(x_1, \\ldots, x_n\\) are scalars written in lowercase Roman. Note that vectors are assumed to follow a vertical notation where the elements of the vector (the \\(x_i\\)s are stacked on top of one another) and the order matters. For example, the vector \\[ \\begin{align*} \\mathbf{x} &amp; = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix} \\end{align*} \\] has the first element \\(x_1 = 5\\), second element \\(x_2 = 2\\) and third element \\(x_3 = 8\\). Note that the vector \\(\\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) is not the same as the vector \\(\\begin{pmatrix} 8 \\\\ 2 \\\\ 5 \\end{pmatrix}\\) because the order of the elements matters. We can also write the vector as \\[ \\begin{align*} \\textbf{x} = \\left( x_1, x_2, \\ldots, x_n \\right)&#39;, \\end{align*} \\] where the \\(&#39;\\) symbol represents the transpose function. For our example matrix, we have \\(\\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}&#39; = \\begin{pmatrix} 5 &amp; 2 &amp; 8 \\end{pmatrix}\\) which is the original vector but arranged in a row rather than a column. Likewise, the transpose of a row vector \\(\\begin{pmatrix} 5 &amp; 2 &amp; 8 \\end{pmatrix}&#39; = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) is a column vector. If \\(\\mathbf{x}\\) is a column vector, we say that \\(\\mathbf{x}&#39;\\) is a row vector and if \\(\\mathbf{x}\\) is a row vector, the \\(\\mathbf{x}&#39;\\) is a column vector. To create a vector we can use the concatenate function c(). For example, the vector \\(\\mathbf{x} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 8 \\end{pmatrix}\\) can be created as the R object using x &lt;- c(5, 2, 8) where the &lt;- assigns the values in the vector c(5, 2, 8) to the object named x. To print the values of x, we can use x ## [1] 5 2 8 which prints the elements of x. Notice that R prints the elements of \\(\\mathbf{x}\\) in a row; however, \\(\\mathbf{x}\\) is a column vector. This inconsistency is present to allow the output to be printed in a manner easier to read (more numbers fit on a row). If we put the column vector into a data.frame, then the vector will be presented as a column vector data.frame(x) ## x ## 1 5 ## 2 2 ## 3 8 One can use the index operator \\([\\hspace{2mm}]\\) to select specific elements of the vector \\(\\mathbf{x}\\). For example, the first element of \\(\\mathbf{x}\\), \\(x_1\\), is x[1] ## [1] 5 and the third element of \\(\\mathbf{x}\\), \\(x_3\\), is x[3] ## [1] 8 The transpose function t() turns a column vector into a row vector (or a row vector into a column vector). For example the transpose \\(\\mathbf{x}&#39;\\) of \\(\\mathbf{x}\\) is tx &lt;- t(x) tx ## [,1] [,2] [,3] ## [1,] 5 2 8 where tx is R object storing the transpose of \\(\\mathbf{x}\\) and is a row vector. The transpose of tx. Notice the indices on the output of the row vector tx. The index operator [1, ] selects the first row to tx and the index operator [, 1] gives the first column tx. Taking the transpose again gives us back the original column vector t(tx) ## [,1] ## [1,] 5 ## [2,] 2 ## [3,] 8 1.2.1 Matrices We let uppercase bold letters \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), etc., represent matrices. We define the matrix \\(\\mathbf{A}\\) with \\(m\\) rows and \\(n\\) columns as \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}, \\end{align*} \\] with \\(a_{ij}\\) being the value of the matrix \\(\\mathbf{A}\\) in the \\(i\\)th row and the \\(j\\)th column. If the matrix \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} 5 &amp; 7 &amp; 1 \\\\ 5 &amp; -22 &amp; 2 \\\\ -14 &amp; 5 &amp; 99 \\\\ 42 &amp; -3 &amp; 0\\end{pmatrix}, \\end{align*} \\] the elements \\(a_{11}\\) = 5, \\(a_{12}\\) = 7, \\(a_{21}\\) = 5, and \\(a_{33}\\) = 99, etc. In R, we can define the matrix A using the matrix() function A &lt;- matrix( data = c(5, 5, -14, 42, 7, -22, 5, -3, 1, 2, 99, 0), nrow = 4, ncol = 3 ) A ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 Notice in the above creation of \\(\\mathbf{A}\\), we wrote defined the elements of the \\(\\mathbf{A}\\) using the columns stacked on top of one another. If we want to fill in the elements of \\(\\mathbf{A}\\) using the rows, we can add the option byrow = TRUE to the matrix() function A &lt;- matrix( data = c(5, 7, 1, 5, -22, 2, -14, 5, 99, 42, -3, 0), nrow = 4, ncol = 3, byrow = TRUE ) A ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 To select the \\(ij\\)th elements of \\(\\mathbf{A}\\), we use the subset operator [ to select the element. For example, to get the element \\(a_{11} = 5\\) in the first row and first column of \\(\\mathbf{A}\\), we use A[1, 1] ## [1] 5 The element \\(a_{3, 3} = 99\\) in the third row and third column can be selected using A[3, 3] ## [1] 99 The matrix \\(\\mathbf{A}\\) can also be represented as a set of either column vectors \\(\\{\\mathbf{c}_j \\}_{j=1}^n\\) or row vectors \\(\\{\\mathbf{r}_i \\}_{i=1}^m\\). For example, the column vector representation is \\[ \\begin{align*} \\mathbf{A} &amp; = \\left( \\mathbf{c}_{1} \\middle| \\mathbf{c}_{2} \\middle| \\cdots \\middle| \\mathbf{c}_{n} \\right), \\end{align*} \\] where the notation \\(|\\) is used to separate the vectors \\[ \\begin{align*} \\mathbf{c}_1 &amp; = \\begin{pmatrix} a_{11} \\\\ a_{12} \\\\ \\vdots \\\\ a_{1m} \\end{pmatrix}, &amp; \\mathbf{c}_2 &amp; = \\begin{pmatrix} a_{21} \\\\ a_{22} \\\\ \\vdots \\\\ a_{2m} \\end{pmatrix}, &amp; \\cdots, &amp; &amp; \\mathbf{c}_n &amp; = \\begin{pmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\end{pmatrix} \\end{align*} \\] In R you can extract the columns using the [ selection operator c1 &lt;- A[, 1] # first column c2 &lt;- A[, 2] # second column c3 &lt;- A[, 3] # third column and you can give the column representation of the matrix A with with column bind function cbind() cbind(c1, c2, c3) ## c1 c2 c3 ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 The row vector representation of \\(\\mathbf{A}\\) is \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{r}_{1} \\\\ \\mathbf{r}_{2} \\\\ \\vdots \\\\ \\mathbf{r}_{m} \\end{pmatrix}, \\end{align*} \\] where the row vectors \\(\\mathbf{r}_i\\) are \\[ \\begin{align*} \\mathbf{r}_1 &amp; = \\left( a_{11}, a_{12}, \\ldots, a_{1n} \\right) \\\\ \\mathbf{r}_2 &amp; = \\left( a_{21}, a_{22}, \\ldots, a_{2n} \\right) \\\\ &amp; \\vdots \\\\ \\mathbf{r}_m &amp; = \\left( a_{m1}, a_{m2}, \\ldots, a_{mn} \\right) \\end{align*} \\] In R you can extract the rows using the [ selection operator r1 &lt;- A[1, ] # first row r2 &lt;- A[2, ] # second row r3 &lt;- A[3, ] # third row r4 &lt;- A[4, ] # fourth row and you can give the row representation of the matrix A with with row bind function rbind() rbind(r1, r2, r3, r4) ## [,1] [,2] [,3] ## r1 5 7 1 ## r2 5 -22 2 ## r3 -14 5 99 ## r4 42 -3 0 "],["section-linear-systems-of-equations.html", "Chapter 2 Linear Systems of Equations 2.1 Linear Systems of equations 2.2 Reduce row echelon form", " Chapter 2 Linear Systems of Equations library(tidyverse) # For 3-d plotting # if devtools package not installed, install the package if (!require(devtools)) { install.packages(&quot;devtools&quot;) } # if gg3D package not installed, install the package if (!require(gg3D)) { devtools::install_github(&quot;AckerDWM/gg3D&quot;) library(gg3D) } ## Warning in fun(libname, pkgname): no display name and no $DISPLAY environment ## variable # if dasc2594 package not installed, install the package if (!require(dasc2594)) { devtools::install_github(&quot;jtipton25/dasc2594&quot;) library(dasc2594) } 2.1 Linear Systems of equations 2.1.1 Linear equations Let \\(x_1, x_2, \\ldots, x_n\\) be variables with coefficients \\(a_1, a_2, \\ldots, a_n\\), and \\(b\\) are fixed and known numbers. Then, we say \\[ \\begin{align} \\tag{2.1} a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n &amp; = b \\end{align} \\] is a linear equation. For example, the equation for a line with slope \\(m\\) and \\(y\\)-intercept \\(b\\) is \\[ \\begin{align*} y &amp; = m x + b, \\end{align*} \\] is a linear equation because it can be re-written as \\[ \\begin{align*} y - m x &amp; = b, \\end{align*} \\] where \\(a_1 = 1\\), \\(a_2 = m\\), \\(x_1 = y\\) and \\(x_2 = x\\). The equations \\[ \\begin{align*} \\sqrt{19} x_1 &amp; = (4 + \\sqrt{2}) x_2 - x_3 - 9 &amp; \\mbox{ and } &amp;&amp; -4 x_1 + 5 x_2 - 11 &amp; = x_3 \\end{align*} \\] are both linear equations because they can be written as \\[ \\begin{align*} \\sqrt{19} x_1 - (4 + \\sqrt{2}) x_2 + x_3 &amp; = - 9 &amp; \\mbox{ and } &amp;&amp; -4 x_1 + 5 x_2 - x_3 &amp; = 11, \\end{align*} \\] respectively. The equations \\[ \\begin{align*} x_1 &amp; = x_2^2 + 3 &amp; \\mbox{ and } &amp;&amp; x_1 + x_2 - x_1 x_2 &amp; = 16 \\end{align*} \\] are not linear equations because they do not meet the form of (2.1) (The first equation above has a quadratic power of \\(x_2\\) and the second equation has a product of \\(x_1\\) and \\(x_2\\)). 2.1.2 Systems of linear equations A set of two or more linear equations that each contain the same set of variables is called a system of linear equations. The equations \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; &amp;&amp; &amp;&amp; = &amp; 9 \\end{align*} \\] are a system of equations. Note that in the second equation, the coefficient for \\(x_3\\) is 0, meaning we could re-write the above example as \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; + &amp;&amp; 0 x_3 &amp;&amp; = &amp; 9. \\end{align*} \\] Exercises: For the following, are these linear equations? \\(x_1 + 3 x_1 x_2 = 5\\) \\(5x + 7y + 8z = 11.2\\) \\(y / 4 + \\sqrt{2} z = 2^6\\) \\(x + 4 y^2 = 9\\) 2.1.3 Solutions of linear systems A fundamental question when presented with a linear system of equations is whether the system has a solution. A solution to a system means that there are numbers \\((s_1, s_2, \\ldots, s_n)\\) that each of the variables \\(x_1, x_2, \\ldots, x_n\\) take that allow for all the equations to simultaneously be true. For example, consider the system of equations \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 5 y &amp;&amp; = &amp; 7 \\end{align*} \\] To find if a solution to this equation exists, we can do some algebra and take 4 times the top equation and then subtract the bottom equation, replacing the bottom equation with this new sum like \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x - 4 * (x) &amp;&amp; + &amp;&amp; 5 y - 4 * (4y) &amp;&amp; = &amp; 7 - 4 * (8) \\end{align*} \\] where the part of the equations in () is the top equation. This system of equations now simplifies to \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 0 &amp;&amp; + &amp;&amp; -11 y &amp;&amp; = &amp; -25 \\end{align*} \\] which gives \\(y = \\frac{25}{11}\\). Plugging this value into the top equation gives \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 * \\frac{25}{11} &amp;&amp; = &amp; 8 \\\\ 0 &amp;&amp; + &amp;&amp; y &amp;&amp; = &amp; \\frac{25}{11} \\end{align*} \\] where we can solve \\(x = 8 - \\frac{100}{11} = -\\frac{12}{11}\\) giving the solution of the form \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 0 &amp;&amp; = &amp; - \\frac{12}{11} \\\\ 0 &amp;&amp; + &amp;&amp; y &amp;&amp; = &amp; \\frac{25}{11}. \\end{align*} \\] In this case, the system of equation has the solution \\(x = -\\frac{12}{11}\\) and \\(y = \\frac{25}{11}\\). While finding the solution can be done algebraically, what does this mean visually (geometrically)? The original equations were \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 5 y &amp;&amp; = &amp; 7 \\end{align*} \\] which define two lines: \\(y = -\\frac{x}{4} + 2\\) \\(y = -\\frac{4x}{5} + \\frac{7}{5}\\) Let’s plot these equations in R and see what they look like # define some grid points to evaluate the line x &lt;- seq(-2, 2, length = 1000) dat &lt;- data.frame( x = c(x, x), y = c(-x / 4 + 2, - 4 / 5 * x + 7/5), equation = factor(rep(c(1, 2), each = 1000)) ) glimpse(dat) ## Rows: 2,000 ## Columns: 3 ## $ x &lt;dbl&gt; -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.979… ## $ y &lt;dbl&gt; 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, 2… ## $ equation &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… dat %&gt;% ggplot(aes(x = x, y = y, color = equation, group = equation)) + geom_line() + scale_color_viridis_d(end = 0.8) + # solution x = -12/11, y = 25/11 geom_point(aes(x = -12/11, y = 25/11), color = &quot;red&quot;, size = 2) + ggtitle(&quot;Linear system of equations&quot;) Figure 2.1: Linear system of equations with one solution From this plot, it is clear that the solution to the system of equations is the location where the two lines intersect! 2.1.4 Types of solutions Typically, there are 3 cases for the solutions to a system of linear equations There are no solutions There is one solution (Figure 2.1) There are infinitely many solutions Definition 2.1 A linear system of equations is called consistent if the system has either one or infinitely many solutions and is called inconsistent if the system has no solution. 2.1.4.1 There are no solutions: Consider the system of linear equations \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 16 y &amp;&amp; = &amp; 18 \\end{align*} \\] # define some grid points to evaluate the line x &lt;- seq(-2, 2, length = 1000) dat &lt;- data.frame( x = c(x, x), y = c(-x / 4 + 8 / 4, - x / 4 + 18 / 4), equation = factor(rep(c(1, 2), each = 1000)) ) glimpse(dat) ## Rows: 2,000 ## Columns: 3 ## $ x &lt;dbl&gt; -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.979… ## $ y &lt;dbl&gt; 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, 2… ## $ equation &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… dat %&gt;% ggplot(aes(x = x, y = y, color = equation, group = equation)) + geom_line() + scale_color_viridis_d(end = 0.8) + # solution x = -12/11, y = 25/11 ggtitle(&quot;Linear system of equations&quot;) Figure 2.2: Linear system of equations with no solution In this case, the linear equations are parallel lines and will never intersect so therefore there is no solution. 2.1.4.2 There is one solution: We have seen this example in Figure 2.1 2.1.4.3 There are infinitely many solutions: Consider the system of linear equations \\[ \\begin{align*} x &amp;&amp; + &amp;&amp; 4 y &amp;&amp; = &amp; 8 \\\\ 4 x &amp;&amp; + &amp;&amp; 16 y &amp;&amp; = &amp; 32 \\end{align*} \\] # define some grid points to evaluate the line x &lt;- seq(-2, 2, length = 1000) dat &lt;- data.frame( x = c(x, x), y = c(-x / 4 + 8 / 4, - 4 * x / 16 + 32 / 16), equation = factor(rep(c(1, 2), each = 1000)) ) glimpse(dat) ## Rows: 2,000 ## Columns: 3 ## $ x &lt;dbl&gt; -2.000000, -1.995996, -1.991992, -1.987988, -1.983984, -1.979… ## $ y &lt;dbl&gt; 2.500000, 2.498999, 2.497998, 2.496997, 2.495996, 2.494995, 2… ## $ equation &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… dat %&gt;% ggplot(aes(x = x, y = y, color = equation, group = equation)) + geom_line() + scale_color_viridis_d(end = 0.8) + # solution x = -12/11, y = 25/11 ggtitle(&quot;Linear system of equations&quot;) Figure 2.3: Linear system of equations with no solution In this case, the linear equations are perfectly overlapping lines and always intersect so therefore there are infinitely many solutions (all points on the line). Definition 2.2 Two linear systems of equations are called equivalent if both systems share the same solution set. For example, the system of equations \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; + &amp;&amp; 2 x_3 &amp;&amp; = &amp; 9. \\end{align*} \\] and the system of equations \\[ \\begin{align*} 2x_1 &amp;&amp; + &amp;&amp; 8 x_2 &amp;&amp; - &amp;&amp; 2 x_3 &amp;&amp; = &amp; 22 \\\\ 8 x_1 &amp;&amp; + &amp;&amp; 10 x_2 &amp;&amp; + &amp;&amp; 4 x_3 &amp;&amp; = &amp; 18. \\end{align*} \\] have the same solution set (the second set of equations is just 2 times the first set of equations). Exercise/Lab: generate some equations, plot them, and determine if there is a solution. Then try to solve these using algebra. Exercises: for the following systems of equations, determine if a solution(s) exist and if so, solve for the solution \\[\\begin{align*} 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; = 8\\\\ 9 x_1 &amp;&amp; - &amp;&amp; 3 x_2 &amp;&amp; = 4 \\end{align*}\\] \\[\\begin{align*} 7 x_1 &amp;&amp; + &amp;&amp; 3 x_2 &amp;&amp; + &amp;&amp; 4 x_3 &amp;&amp; = 5\\\\ 4 x_1 &amp;&amp; - &amp;&amp; 5 x_2 &amp;&amp; &amp;&amp; &amp;&amp; = -2 \\end{align*}\\] \\[\\begin{align*} 4 x_1 &amp;&amp; - &amp;&amp; 2 x_2 &amp;&amp; = 8\\\\ 2 x_1 &amp;&amp; + &amp;&amp; x_2 &amp;&amp; = 7 \\\\ -3 x_1 &amp;&amp; + &amp;&amp; 6 x_2 &amp;&amp; = 11 \\end{align*}\\] 2.1.5 Elementary row and column operations on matrices The elementary row (column) operations include swaps: swapping two rows (columns), sums: replacing a row (column) by the sum itself and a multiple of another row (column) scalar multiplication: replacing a row (column) by a scalar multiple times itself Note that these operations are exactly what we used to solve the equation using algebra above (except for swapping rows). Add in examples in class here 2.1.6 The Augmented matrix form of a system of equations Consider the linear system of equations \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 11 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; + &amp;&amp; 2 x_3 &amp;&amp; = &amp; 9. \\end{align*} \\] The augmented matrix representation of this system of linear equations is given by the matrix \\[ \\begin{align*} \\begin{pmatrix} 1 &amp; 4 &amp; - 1 &amp; 11 \\\\ 4 &amp; 5 &amp; 2 &amp; 9 \\end{pmatrix}, \\end{align*} \\] where the first column of the matrix represents the variable \\(x_1\\), the second column of the matrix represents the variable \\(x_2\\), the third column of the matrix represents the variable \\(x_3\\), and the fourth column of the matrix represents the constant terms. We can express the augmented form in R using a matrix augmented_matrix &lt;- matrix(c(1, 4, 4, 5, -1, 2, 11, 9), 2, 4) augmented_matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 4 -1 11 ## [2,] 4 5 2 9 and to make clear the respective variables, we can add in column names as a matrix attribute using the colnames() function colnames(augmented_matrix) &lt;- c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;constants&quot;) augmented_matrix ## x1 x2 x3 constants ## [1,] 1 4 -1 11 ## [2,] 4 5 2 9 which adds labels to each of the columns. Now, using elementary row operations on the matrix, we can attempt to find solutions to the system of equations. First, we multiply the first row by -4 and add it to the second row of the matrix and replace the second row with this sum augmented_matrix[2, ] &lt;- -4 * augmented_matrix[1, ] + augmented_matrix[2, ] augmented_matrix ## x1 x2 x3 constants ## [1,] 1 4 -1 11 ## [2,] 0 -11 6 -35 Next, scale the second row to have a leading value of 1 by dividing by -11 augmented_matrix[2, ] &lt;- augmented_matrix[2, ] / (-11) augmented_matrix ## x1 x2 x3 constants ## [1,] 1 4 -1.0000000 11.000000 ## [2,] 0 1 -0.5454545 3.181818 We can then multiply the second row by -4 and add it to the first row and replace the first row with this value. augmented_matrix[1, ] &lt;- augmented_matrix[1, ] - 4 * augmented_matrix[2, ] augmented_matrix ## x1 x2 x3 constants ## [1,] 1 0 1.1818182 -1.727273 ## [2,] 0 1 -0.5454545 3.181818 Notice how the matrix has a “triangular” form (The lower part of the “triangle” is made of 0s and the upper part has numbers). The triangular form tells us that There are infinitely many solutions to this system of equation. The infinite solutions are subject to the requirements that \\[x_1 = - \\frac{19}{11} - \\frac{13}{11} x_3\\] and \\[x_2 = \\frac{35}{11} + \\frac{6}{11} x_3.\\] To get this into a reasonable form, we will solve these equations as a function of \\(x_1\\). Solving the first equation for \\(x_3\\) gives \\[x_3 = - \\frac{19}{13} -\\frac{11}{13} x_1.\\] Then, plugging this into \\(x_3\\) in the second equation gives \\[ \\begin{align*} x_2 &amp; = \\frac{35}{11} + \\frac{6}{11} \\left( - \\frac{19}{13} -\\frac{11}{13} x_1 \\right) \\\\ &amp; = \\frac{341}{143} - \\frac{6}{13} x_1 \\end{align*} \\] which defines a linear relationship between \\(x_1\\) and \\(x_2\\). Notice that in these last two solutions, \\(x_1\\) is a “free variable” and \\(x_2\\) and \\(x_3\\) are “determined” by \\(x_1\\). In the plot below, the two planes (red and blue) are the geometric plots of the linear equations in the system of equations (the red plane is the top equation and the blue plane is the bottom equation). The purple line is the equation for the solution given the free variable \\(x_3\\) and lies at the intersection of the two planes, much like the point in the two lines in figure linking reference here lies at the intersection of the two points. # uses gg3D library n &lt;- 60 x1 &lt;- x2 &lt;- seq(-10, 10, length = n) region &lt;- expand.grid(x1 = x1, x2 = x2) df &lt;- data.frame( x1 = region$x1, x2 = region$x2, x3 = - 11 + (region$x1 + 4 * region$x2) ) df2 &lt;- data.frame( x1 = region$x1, x2 = region$x2, x3 = (9 - 4 * region$x1 - 5 * region$x2) / 2 ) df_solution &lt;- data.frame( x1 = x1, x2 = 341 / 143 - 6 / 13 * x1, x3 = -19/13 - 11/13 * x1 ) # theta and phi set up the &quot;perspective/viewing angle&quot; of the 3D plot theta &lt;- 63 phi &lt;- -12 ggplot(df, aes(x1, x2, z = x3)) + axes_3D(theta = theta, phi = phi) + stat_wireframe(alpha = 0.25, color = &quot;red&quot;, theta = theta, phi = phi) + stat_wireframe(data = df2, aes(x = x1, y = x2, z = x3), alpha = 0.25, color = &quot;blue&quot;, theta = theta, phi = phi) + stat_3D(data = df_solution, aes(x1, x2, z = x3), geom = &quot;line&quot;, theta = theta, phi = phi, color = &quot;purple&quot;) + theme_void() + theme(legend.position = &quot;none&quot;) + labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), angle=c(0, 0, 90), theta = theta, phi = phi) ## Warning: Removed 2 row(s) containing missing values (geom_path). ## Warning: Removed 2 row(s) containing missing values (geom_path). 2.1.7 Existence and Uniqueness Definition 2.3 A system of linear equations is said to be consistent if at least one solution exists. The linear system of equations is said to have a unique solution if only one solution exists. Example: (do in class) Is the system of linear equations consistent? IF the system is consistent, does it have a unique solution? \\[ \\begin{align*} 16 x_1 &amp;&amp; + &amp;&amp; 2 x_2 &amp;&amp; + &amp;&amp; 3 x_3 &amp;&amp; = &amp; 13 \\\\ 5 x_1 &amp;&amp; + &amp;&amp; 11 x_2 &amp;&amp; + &amp;&amp; 10 x_3 &amp;&amp; = &amp; 8 \\\\ 9 x_1 &amp;&amp; + &amp;&amp; 7 x_2 &amp;&amp; + &amp;&amp; 6 x_3 &amp;&amp; = &amp; 12 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 14 x_2 &amp;&amp; + &amp;&amp; 15 x_3 &amp;&amp; = &amp; 1 \\\\ \\end{align*} \\] Example: (do in class) Is the system of linear equations consistent? IF the system is consistent, does it have a unique solution? \\[ \\begin{align*} x_1 &amp;&amp; + &amp;&amp; 2 x_2 &amp;&amp; + &amp;&amp; 3 x_3 = &amp; 5 \\\\ x_1 &amp;&amp; + &amp;&amp; 3 x_2 &amp;&amp; + &amp;&amp; 2 x_3= &amp; 2 \\\\ 3 x_1 &amp;&amp; + &amp;&amp; 2 x_2 &amp;&amp; + &amp;&amp; x_3 = &amp; 7 \\end{align*} \\] 2.2 Reduce row echelon form Reducing a matrix to row echelon form is a useful technique for working with matrices. The row echelon form can be used to solve systems of equations, as well as determine other properties of a matrix that are yet to be discussed, including rank, invertibility, column/row spaces, etc. Definition 2.4 A matrix is said to be in echelon form if all nonzero rows are above any rows of zeros (all rows consisting entirely of zeros are at the bottom) the leading entry/coefficient of a nonzero row (called the pivot) is always strictly to the right of the leading entry/coefficient of the row above Example: echelon matrix example in class Definition 2.5 A matrix is in reduced row echelon form if it is in echelon form and the leading entry/coefficient of each row is 1 The leading entry/coefficient of 1 is the only nonzero entry in its column. Example: rref matrix example in class Definition 2.6 Echelon matrices have the property of being upper diagonal. A matrix is said to be upper diagonal if all entries of the matrix at or above the diagonal are nonzero. Example: **lower and non-lower diagonal matrices Definition 2.7 Two matrices are row-equivalent if one matrix can be transformed to the other through elementary row operations. Theorem 2.1 A nonzero matrix can be transformed into more than one echelon forms. However, the reduced row echelon form of a nonzero matrix is unique. Exercise: using elementary row operations, calculate the reduced row echelon form of the following matrices fill in later fill in later fill in later 2.2.1 Pivot positions The leading entry/coefficients of a row echelon form matrix are called pivots. The positions of the pivot positions are the same for any row echelon form of a matrix. In reduced row echelon form, these pivot positions take the value 1. Definition 2.8 In a matrix that is in reduced echelon form, the pivot position is the first nonzero element of each row. The column in which the pivot position occurs is called a pivot column. Example: pivot position and pivot columns 2.2.2 Finding the reduced row echelon form Calculating the reduced row echelon form is known as Gaussian elimination, which is named after Johann Carl Friedrich Gauss. This algorithm uses elementary row operations to calculate the reduced row echelon form. The following steps perform the Gaussian elimination algorithm. Start with the left-most nonzero column, which is a pivot column If the top row is zero, swap rows so that the top row is nonzero so that the top row has a nonzero element in the pivot position. Use row multiplication and addition to zero out all positions in the pivot column below the top row (pivot position). Ignore this top row and repeat steps 1-3 until there are no more nonzero rows to apply steps 1-3 on. At the end of this step, the matrix is in row echelon form. Starting at the right-most pivot column, use elementary row operations to zero out all positions above each pivot and to make each pivot position 1. At the end of this step, the matrix is in reduced row echelon form. Example: in class # pracma library # rref example in class 2.2.3 Using reduced row echelon forms to solve systems of linear equations When a system of linear equations is expressed as an augmented matrix, the reduced row echelon form can be used to find solutions to those systems of equations. Consider the systems of equations \\[ \\begin{align*} 3x_1 &amp;&amp; + &amp;&amp; 8 x_2 &amp;&amp; - &amp;&amp; 4 x_3 &amp;&amp; = &amp; 6 \\\\ 2 x_1 &amp;&amp; - &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; x_3 &amp;&amp; = &amp; 8 \\\\ 4 x_1 &amp;&amp; + &amp;&amp; 5 x_2 &amp;&amp; &amp;&amp; &amp;&amp; = &amp; 9 \\end{align*} \\] which can be written in the augmented matrix form \\[ \\begin{pmatrix} 3 &amp; 8 &amp; -4 &amp; 6 \\\\ 2 &amp; -4 &amp; -1 &amp; 8 \\\\ 4 &amp; 5 &amp; 0 &amp; 9 \\end{pmatrix} \\] In R, this is the matrix # define matrix Calculating the reduced row echelon form, gives # calculate rref of augmented matrix which gives the solution … Exercise: calculate the RREF for the augmented matrix in the example above by hand Another example where we find a solution is \\[ \\begin{align*} 5 x_1 &amp;&amp; + &amp;&amp; 4 x_2 &amp;&amp; - &amp;&amp; 2 x_3 &amp;&amp; = &amp; 0 \\\\ -3 x_1 &amp;&amp; - &amp;&amp; 2 x_2 &amp;&amp; - &amp;&amp; 4 x_3 &amp;&amp; = &amp; 1 \\\\ \\end{align*} \\] Do same steps Definition 2.9 In a system of linear equations that is underdetermined (fewer equations than unknowns), the determined/basic variables are those variable that have a 1 in the respective columns when in reduced row echelon form (i.e., variables in a pivot position). The variables that are not in a pivot position are called free variable. Example in class 2.2.4 Existence and uniquenss from reduced row echelon form The row echelon form is useful to determine if a system of linear equations is consistent (the system of equations has a solution). To check if a solution to a linear system of equations exists, convert the system of equations to an augmented matrix form. Then, reduce the augmented matrix to row echelon form using elementary matrix operations. As long as there is not an equation of the form \\[ 0 = \\mbox{constant} \\] for some constant number not equal to 0, the system of linear equations is consistent. If the augmented matrix can be written in reduced row echelon form with no free variables, the solution to the linear system of equations is unique. These results give rise to the theorem Theorem 2.2 A linear system of equations is consistent (has a solution) if the furthest right column (the constant column) is not a pivot column. If the system of equations is consistent, (i.e., the furthest right column is not a pivot column), the solution is unique if there are no free variables and there are infinitely many solutions if there is at least one free variable. Example: consistent system of equations \\[\\begin{pmatrix} -7 &amp; -9 &amp; 7 &amp; 8 \\\\ -4 &amp; 0 &amp; 6 &amp; -6 \\\\ -10 &amp; 3 &amp; -8 &amp; 5 \\end{pmatrix}\\] Example: inconsistent system of equations \\[\\begin{pmatrix} -7 &amp; 0 &amp; -8 &amp; -5 \\\\ -4 &amp; 3 &amp; 8 &amp; -2 \\\\ -10 &amp; 7 &amp; -6 &amp; 4 \\\\ -9 &amp; 6 &amp; 5 &amp; 1 \\end{pmatrix}\\] "],["section-vector-spaces.html", "Chapter 3 Vectors spaces 3.1 Vectors 3.2 Vector addition 3.3 Span", " Chapter 3 Vectors spaces library(shiny) library(patchwork) library(tidyverse) # if gg3D package not installed, install the package library(gg3D) library(dasc2594) 3.1 Vectors 3.1.1 Properties of Vectors For any real valued scalars \\(a, b \\in \\mathcal{R}\\) and any vectors \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathcal{R}^n\\) (vectors of real numbers of length \\(n\\)), scalar multiplication \\[ \\begin{align*} a \\mathbf{x} &amp; = a \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} a x_1 \\\\ a x_2 \\\\ \\vdots \\\\ a x_n \\end{pmatrix} \\end{align*} \\] where the scalar \\(a\\) is multiplied by each element of the vector. For example, \\[ \\begin{align*} 4 \\begin{pmatrix} 4 \\\\ 6 \\\\ 7 \\\\ 12 \\end{pmatrix} &amp; = \\begin{pmatrix} 4 * 4 \\\\ 4 * 6 \\\\ 4 * 7 \\\\ 4 * 12 \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} 16 \\\\ 24 \\\\ 28 \\\\ 48 \\end{pmatrix} \\end{align*} \\] In R, we can multiply the vector by a scalar as 4 * c(4, 6, 7, 12) ## [1] 16 24 28 48 or if the vector \\(\\mathbf{x} = \\left( 4, 6, 7, 12 \\right)&#39;\\) we can write this as x &lt;- c(4, 6, 7, 12) 4 * x ## [1] 16 24 28 48 scalar multiplicative commutivity \\[ \\begin{align*} a (b \\mathbf{x}) &amp; = (ab) \\mathbf{x} &amp; = b (a \\mathbf{x}) \\end{align*} \\] 4 * (6 * x) ## [1] 96 144 168 288 (4 * 6) * x ## [1] 96 144 168 288 scalar additive associativity \\[ \\begin{align*} a \\mathbf{x} + b \\mathbf{x} &amp; = (a + b) \\mathbf{x} \\end{align*} \\] vector additive associativity \\[ \\begin{align*} a \\mathbf{x} + a \\mathbf{y} &amp; = a (\\mathbf{x} + \\mathbf{y}) \\end{align*} \\] vector associativity \\[ \\begin{align*} \\mathbf{x} + \\mathbf{y} &amp; = \\mathbf{y} + \\mathbf{x} \\end{align*} \\] \\[ \\begin{align*} (\\mathbf{x} + \\mathbf{y}) + \\mathbf{z} &amp; = \\mathbf{x} + (\\mathbf{y} + \\mathbf{z}) \\end{align*} \\] x &lt;- c(1, 2, 3, 4) y &lt;- c(4, 3, 5, 1) z &lt;- c(5, 2, 4, 6) x + y ## [1] 5 5 8 5 y + x ## [1] 5 5 8 5 (x + y) + z ## [1] 10 7 12 11 x + (y + z) ## [1] 10 7 12 11 Identity Element of Addition: For any vector \\(\\mathbf{x}\\) of length \\(n\\), there exists a vector \\(\\mathbf{0}\\), known as the zero vector, such that \\[ \\begin{align*} \\mathbf{x} + \\mathbf{0} &amp; = \\mathbf{x} \\end{align*} \\] x + 0 ## [1] 1 2 3 4 x + rep(0, 4) ## [1] 1 2 3 4 Inverse Element of Addition: For any vector \\(\\mathbf{x}\\) of length \\(n\\), there exists a vector \\(-\\mathbf{x}\\), known as the additive inverse vector, such that \\[ \\begin{align*} \\mathbf{x} + (- \\mathbf{x}) &amp; = \\mathbf{0} \\end{align*} \\] x + (-x) ## [1] 0 0 0 0 3.2 Vector addition Two vectors of length \\(n\\) can be added elementwise \\[ \\begin{align*} \\mathbf{x} + \\mathbf{y} &amp; = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} + \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix} \\end{align*} \\] For example, \\[ \\begin{align*} \\begin{pmatrix} 3 \\\\ 1 \\\\ -4 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} -3 \\\\ 17 \\\\ -39 \\\\ 4 \\end{pmatrix} &amp; = \\begin{pmatrix} 3 + (-3) \\\\ 1 + 17 \\\\ -4 + (-39) \\\\ 3 + 4 \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} 0 \\\\ 18 \\\\ -43 \\\\ 7 \\end{pmatrix} \\end{align*} \\] In R, we have x &lt;- c(3, 1, -4, 3) y &lt;- c(-3, 17, -39, 4) x + y ## [1] 0 18 -43 7 If two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are of different lengths, then they cannot be added together. Using R, we get the following error: x &lt;- c(1, 2, 3) y &lt;- c(1, 2, 3, 4) x + y ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 2 4 6 5 The error is telling us that the vector \\(\\mathbf{x}\\) and the vector \\(\\mathbf{y}\\) do not have the same length. Be careful when adding vectors in R. R uses “recycling” which means two vectors of different lengths can be added together if one vector is of a length that is a multiple of the other vector. For example, if \\(\\mathbf{x} = (1, 2)&#39;\\) is a vector of length 2 and \\(\\mathbf{y} = (1, 2, 3, 4)\\) is a vector of length 4, R will add \\(\\mathbf{x} + \\mathbf{y}\\) by replicating the vector \\(\\mathbf{x}\\) twice (i.e., \\(\\mathbf{x} + \\mathbf{y} = \\left( \\mathbf{x}&#39;, \\mathbf{x}&#39; \\right)&#39; = \\left(1, 2, 1, 2 \\right)&#39; + \\mathbf{y}\\)) x &lt;- c(1, 2) y &lt;- c(1, 2, 3, 4) x + y ## [1] 2 4 4 6 # replicated x = c(1, 2, 1, 2) c(1, 2, 1, 2) + y ## [1] 2 4 4 6 3.2.1 The geometric interpretation of vectors in \\(\\mathcal{R}^2\\) Let \\(\\mathcal{R}^2\\) be a real coordinate space of \\(2\\) dimensions. You are already familiar with the Cartesian plane that consists of ordered pairs \\((x, y)\\). The Cartesian plane defines the real coordinate space \\(\\mathbf{R}^2\\) of two dimensions. In \\(\\mathbf{R}^2\\), the location of any point of interest can be defined using the \\(x\\) and \\(y\\). For example, the plot below shows the location of the point (2, 3) dat &lt;- data.frame( x = 2, y = 3 ) ggplot(data = dat, aes(x = x, y = y)) + geom_point() + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) A vector space is a generalization of this representation. In \\(\\mathcal{R}^2\\), we say that the vector \\(\\mathbf{z} = c(2, 3)\\) is centered at the origin (0, 0) and has length 2 in the \\(x\\)-axis and length 3 in the \\(y\\)-axis. The plot below shows this vector We can also decompose the vector \\(\\mathbf{z}\\) into its \\(x\\) and \\(y\\) components. The \\(x\\) component of \\(\\mathbf{z}\\) is (2, 0) and the \\(y\\) component of \\(\\mathbf{z}\\) is (0, 3). The following plot shows the \\(x\\) component (2, 0) in blue and the \\(y\\) component (0, 3) in red. The below Shiny app allows you to plot the vector for any \\((x, y)\\) pair of your choosing. The shiny app can be downloaded and run on your own computer using library(shiny) runGitHub(rep = &quot;multivariable-math&quot;, username = &quot;jtipton25&quot;, subdir = &quot;shiny-apps/chapter-03/vector-space&quot;) 3.2.1.1 Addition of vectors We can represent the addition of vectors geometrically as well. Consider the two vectors \\(\\mathbf{u}\\) = (3, 2) and \\(\\mathbf{v}\\) = (-2, 1) where \\(\\mathbf{u} + \\mathbf{v}\\) = (1, 3). data.frame(x = c(3, -2, 1), y = c(2, 1, 3), vector = c(&quot;u&quot;, &quot;v&quot;, &quot;u+v&quot;)) %&gt;% ggplot() + geom_point(aes(x = x, y = y, color = vector)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) We can represent the sum using vectors by adding \\(\\mathbf{u}\\) first then adding \\(\\mathbf{v}\\) to \\(\\mathbf{u}\\) or by adding \\(\\mathbf{v}\\) first and then \\(\\mathbf{u}\\) to get df &lt;- data.frame(x = c(0, 3, 1, -2), y = c(0, 2, 3, 1)) p1 &lt;- ggplot() + geom_segment(aes(x = 0, xend = 3, y = 0, yend = 2), arrow = arrow(), color = &quot;blue&quot;) + geom_segment(aes(x = 3, xend = 3 - 2, y = 2, yend = 2 + 1), arrow = arrow(), color = &quot;red&quot;) + geom_segment(aes(x = 0, xend = 3 - 2, y = 0, yend = 2 + 1), arrow = arrow(), color = &quot;black&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_polygon(data = df, aes(x = x, y = y), fill = &quot;grey&quot;, alpha = 0.5) + ggtitle(&quot;u + v&quot;) p2 &lt;- ggplot() + geom_segment(aes(x = 0, xend = -2, y = 0, yend = 1), arrow = arrow(), color = &quot;red&quot;) + geom_segment(aes(x = -2, xend = -2 + 3, y = 1, yend = 1 + 2), arrow = arrow(), color = &quot;blue&quot;) + geom_segment(aes(x = 0, xend = 3 - 2, y = 0, yend = 2 + 1), arrow = arrow(), color = &quot;black&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_polygon(data = df, aes(x = x, y = y), fill = &quot;grey&quot;, alpha = 0.5) + ggtitle(&quot;v + u&quot;) p1 + p2 Notice that the sum of these vectors defines a parallelogram where the sum \\(\\mathbf{u} + \\mathbf{v}\\) is the diagonal of the shaded parallelogram. This geometric interpretation will serve as a basis for interpreting vector equations in higher dimensions where typical visualization methods fail. 3.2.2 Scalar multiplication of vectors We can represent the multiplication of a vector by a scalar geometrically as well. Consider the vector \\(\\mathbf{u}\\) = (3, 2) and the scalars \\(a = 2\\) and \\(b = -1\\). Then, we can plot \\(\\mathbf{u}\\), \\(a\\mathbf{u}\\), and \\(b\\mathbf{u}\\). data.frame(x = c(3, 2 * 3, -1 * 3), y = c(2, 2 * 2, -1 * 2), vector = c(&quot;u&quot;, &quot;a*u&quot;, &quot;b*u&quot;)) %&gt;% ggplot() + geom_point(aes(x = x, y = y, color = vector)) + geom_segment(aes(x = 0, xend = x, y = 0, yend = y, color = vector), arrow = arrow(), alpha = 0.75) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-6, 6), ylim = c(-6, 6)) In fact, if \\(a\\) is allowed to take on any values, then the set of all possible values of \\(a \\mathbf{u}\\) for all values of \\(a\\) defines an infinite line ggplot() + geom_abline(slope = 2/3, intercept = 0) + geom_point(aes(x = 3, y = 2)) + geom_segment(aes(x = 0, xend = 3, y = 0, yend = 2), arrow = arrow(), color = &quot;black&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) 3.2.3 The geometric interpretation of vectors in \\(\\mathcal{R}^3\\) Let the vector \\(\\mathbf{u} = c(-2, 3, 5)\\). Then, the figure below shows the vector in 3 dimensions. Draw picture by hand 3.2.4 The geometric interpretation of vectors in \\(\\mathcal{R}^n\\) As the number of dimensions increases, the same interpretation can be used, but the ability to visualize higher dimensions becomes more difficult. 3.2.5 Linear Combinations of Vectors We say that for any two scalars \\(a\\) and \\(b\\) and any two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of length \\(n\\), the sum \\[ \\begin{align*} a \\mathbf{x} + b \\mathbf{y} &amp; = \\begin{pmatrix} a x_1 + b y_1 \\\\ a x_2 + b y_2 \\\\ \\vdots \\\\ a x_n + b y_n \\\\ \\end{pmatrix} \\end{align*} \\] is called a linear combination. The idea of a linear combination can be extended to \\(K\\) different scalars \\(\\{ a_1, \\ldots, a_K \\}\\) and \\(K\\) different vectors \\(\\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_K\\}\\) each of length \\(n\\) as \\[ \\begin{align*} a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 + \\ldots + a_K \\mathbf{x}_K = \\sum_{k=1}^K a_k \\mathbf{x}_k &amp; = \\begin{pmatrix} \\sum_{k=1}^K a_k x_{k1} \\\\ \\sum_{k=1}^K a_k x_{k2} \\\\ \\vdots \\\\ \\sum_{k=1}^K a_k x_{kn} \\\\ \\end{pmatrix} \\end{align*} \\] The scalars \\(a_k\\) are called coefficients (sometimes also called weights). Example: Consider the linear combination \\(a \\mathbf{u} + b \\mathbf{v}\\) where \\[ \\begin{align*} \\mathbf{u} = \\begin{pmatrix} 3 \\\\ 6\\end{pmatrix} &amp;&amp; \\mathbf{v} = \\begin{pmatrix} -2 \\\\ 1\\end{pmatrix}. \\end{align*} \\] Are there values of \\(a\\) and \\(b\\) such \\(a \\mathbf{u} + b \\mathbf{v} = \\begin{pmatrix} 9 \\\\ - 4 \\end{pmatrix}\\)? To answer this question, we can write the linear combination as \\[ \\begin{align*} a \\begin{pmatrix} 3 \\\\ 6\\end{pmatrix} + b \\begin{pmatrix} -2 \\\\ 1\\end{pmatrix} &amp; = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix} \\end{align*} \\] which can be written using the property of scalar multiplication as \\[ \\begin{align*} \\begin{pmatrix} 3a \\\\ 6a \\end{pmatrix} + \\begin{pmatrix} -2b \\\\ b \\end{pmatrix} &amp; = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix} \\end{align*} \\] and using properties of vector addition can be written as \\[ \\begin{align*} \\begin{pmatrix} 3a - 2b \\\\ 6a + b \\end{pmatrix} &amp; = \\begin{pmatrix} 9 \\\\ -4 \\end{pmatrix} \\end{align*} \\] Recognizing this as a system of linear equations \\[ \\begin{align*} 3a - 2b &amp; = 9 \\\\ 6a + b &amp; = -4, \\end{align*} \\] the system of equations can be written in an augmented matrix form as \\[ \\begin{align*} \\begin{pmatrix} 3 &amp; - 2 &amp; 9\\\\ 6 &amp; 1 &amp; -4 \\end{pmatrix} \\end{align*} \\] Reducing the augmented matrix to reduced row echelon form gives rref(matrix(c(3, 6, -2, 1, 9, -4), 2, 3)) ## [,1] [,2] [,3] ## [1,] 1 0 0.06666667 ## [2,] 0 1 -4.40000000 which has solutions \\(a = 0.0667\\) and \\(b = -4.4\\). Result: Any vector equation \\(a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 + \\ldots + a_K \\mathbf{x}_K = \\mathbf{c}\\) for a given constant vector \\(\\mathbf{b}\\) has the same solution set as the augmented matrix \\[ \\begin{align*} \\begin{pmatrix} \\mathbf{x}_1 &amp; \\mathbf{x}_2 &amp; \\cdots &amp; \\mathbf{x}_K &amp; \\mathbf{b} \\end{pmatrix} \\end{align*} \\] Equivalently, the set of vectors \\(\\{\\mathbf{x}_k\\}_{k=1}^K\\) can only be combined with linear coefficients \\(\\{a_k\\}_{k=1}^K\\) to equal the vector \\(\\mathbf{b}\\) if the linear system of equations is consistent. 3.2.6 The geometric interpretation of linear combinations of vectors Consider the vectors \\(\\mathbf{u} = \\begin{pmatrix} \\sqrt{2} \\\\ - \\sqrt{2} \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) shown in the figure below on the left. Exercise: Given \\(\\mathbf{u} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\), estimate the linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) that gives the point \\(\\mathbf{w}\\) in the figure below. 3.3 Span Let \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) be vectors in \\(\\mathcal{R}^n\\). We say the vector \\(\\mathbf{w}\\) is in the span of \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) (\\(\\mathbf{w} \\in \\mbox{span}\\{ \\mathbf{a}_1, \\ldots, \\mathbf{a}_K \\}\\)) if there exists coefficients \\(x_1, \\ldots, x_K\\) such that \\(\\mathbf{w} = \\sum_{k=1}^K x_k \\mathbf{a}_k\\). Example: While not a vector notation, you already understand the span from polynomial functions. For example, assume you have the functions 1, \\(x\\), and \\(x^2\\). Then, the functions \\(-4 + 3x^2\\) (\\(a_1 = -4, a_2 = 0, a_3 = 3\\)) and \\(-3 + 4x - 2x^2\\) (\\(a_1 = -3, a_2 = 4, a_3 = -2\\)) are in the span of functions \\(\\{1, x, x^2\\}\\), but the functions \\(x^3\\), \\(x^4 - 2x^2\\), etc., are not in the span of \\(\\{1, x, x^2\\}\\) because you cannot write these as a linear combination of \\(a_1 1 + a_2 x + a_3 x^2\\). 3.3.1 Geometric example of the span Example: Consider the vector \\(\\mathbf{u} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\). Then, the vector \\(\\mathbf{w} = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}\\) is in the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) because \\(\\mathbf{w} = 2 \\mathbf{u}\\) but the vector \\(\\mathbf{v} = \\begin{pmatrix} 4 \\\\ -4 \\end{pmatrix}\\) is not in the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) because there is no coefficient \\(a\\) such that \\(\\mathbf{w} = a \\mathbf{u}\\). In this example, the vector \\(\\mathbf{u}\\) is a 2-dimensional vector (lives in \\(\\mathcal{R}^2\\)–a plane) but the \\(\\mbox{span}\\{\\mathbf{u}\\}\\) lives in 1-dimension (a line). ggplot() + geom_abline(slope = 1/2, intercept = 0, color = &quot;blue&quot;, size = 2) + geom_segment(aes(x = 0, xend = 2, y = 0, yend = 1), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;red&quot;) + geom_segment(aes(x = 0, xend = 4, y = 0, yend = 2), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;red&quot;) + geom_segment(aes(x = 0, xend = 4, y = 0, yend = -4), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;orange&quot;) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_text(data = data.frame(x = c(2, 4, 4), y = c(1, 2, -4), text = c(&quot;u&quot;, &quot;w&quot;, &quot;v&quot;)), aes(x = x, y = y + 0.5, label = text), size = 5, inherit.aes = FALSE, color = c(&quot;red&quot;, &quot;red&quot;, &quot;orange&quot;)) + ggtitle(&quot;span{u} is the blue line \\nw is in span{u}\\nv is not in span{u}&quot;) From the example above, we can answer the question “Is the point (a, b) on the line defined by the vector \\(\\mathbf{u}\\)?” by asking whether the point (a, b) is in the \\(span\\{\\mathbf{u}\\}\\). While this is trivial for such a simple problem, the use of the span will make things easier in higher dimensions. Example: do in class 2 3-d vectors that are not scalar multiples of each other define a plane. Does a point lie within the plane? Use the span to answer this question. The shiny app below demonstrates how the concept of span can be understood in 2 dimensions. The app can be downloaded and run library(shiny) runGitHub(rep = &quot;multivariable-math&quot;, username = &quot;jtipton25&quot;, subdir = &quot;shiny-apps/chapter-03/span&quot;) "],["section-matrix-equation.html", "Chapter 4 Matrix equations 4.1 Solutions of matrix equations 4.2 Existence of solutions 4.3 Matrix-vector multiplication 4.4 Properties of matrix-vector multiplication 4.5 Solutions of linear systems 4.6 Solutions to nonhomogeneous systems 4.7 Finding solutions", " Chapter 4 Matrix equations library(tidyverse) library(dasc2594) library(gg3D) Here we introduce the concept of the linear equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\). This equation is the most fundamental equation in all of statistics and data science. Given a matrix \\(\\mathbf{A}\\) and a vector of constants \\(\\mathbf{b}\\), the goal is to solve for the value (or values) of \\(\\mathbf{x}\\) that are a solution to this equation. The equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) is a matrix representation of the system of linear equations \\[ \\begin{align*} \\tag{4.1} \\mathbf{A} \\mathbf{x} &amp; = \\mathbf{b} \\\\ \\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_K \\end{pmatrix} &amp; = \\mathbf{b} \\\\ x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K &amp; = \\mathbf{b} \\\\ \\end{align*} \\] as long as the matrix \\(\\mathbf{A}\\) has \\(n\\) rows and \\(K\\) columns and the vectors \\(\\mathbf{a}_k\\) are \\(n\\)-dimensional. Example: in class Example: in class 4.1 Solutions of matrix equations Because the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) is equivalent to a linear system of equations \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\), we can solve the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) by writing the equation in an augmented matrix form \\[ \\begin{align*} \\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K &amp; \\mathbf{b} \\end{pmatrix} \\end{align*} \\] and then reducing the matrix to reduced row echelon form. This gives rise to the theorem Theorem 4.1 The matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\), the vector equation \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\), and the augmented matrix \\(\\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K &amp; \\mathbf{b} \\end{pmatrix}\\) all have the same solution set. 4.2 Existence of solutions A solution to the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) exists if and only if \\(\\mathbf{b}\\) is a linear combination of the columns of \\(\\mathbf{A}\\). In other words, \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a solution if and only if \\(\\mathbf{b}\\) is in the \\(\\mbox{span}\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\}\\). Example: in class Let \\(\\mathbf{A} =\\ldots\\) and \\(\\mathbf{b} = \\ldots\\). Is the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) consistent? Theorem 4.2 For the \\(n \\times K\\) matrix \\(\\mathbf{A}\\), the following statements are equivalent: For each \\(\\mathbf{b} \\in \\mathcal{R}^K\\), the equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has at least one solution Each \\(\\mathbf{b} \\in \\mathcal{R}^K\\) is a linear combination of the columns of \\(\\mathbf{A}\\) The columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^K\\) \\(\\mathbf{A}\\) has a pivot in every column 4.3 Matrix-vector multiplication To calculate \\(\\mathbf{A} \\mathbf{x}\\), we need to define matrix multiplication. The equivalence between the linear systems of equations \\(x_1 \\mathbf{a}_1 + \\ldots + x_K \\mathbf{a}_K = \\mathbf{b}\\) and the matrix equation \\(\\mathbf{A} \\mathbf{x}\\) gives a hint in how to do this. First, recall the definition of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1K} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\ldots &amp; a_{nK} \\\\ \\end{pmatrix} &amp;&amp; \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_K \\end{pmatrix} \\end{align*} \\] The matrix product \\(\\mathbf{A}\\mathbf{x}\\) is the linear system of equations \\[ \\begin{align*} \\mathbf{A} \\mathbf{x} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1K} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\ldots &amp; a_{nK} \\\\ \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\\\ &amp; = x_1\\begin{pmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{n1} \\end{pmatrix} + x_2 \\begin{pmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{n2} \\end{pmatrix} + \\cdots + x_K \\begin{pmatrix} a_{1K} \\\\ a_{nK} \\\\ \\vdots \\\\ a_{nK} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} a_{11} x_1 + a_{12} x_2 + \\ldots + a_{1K} x_K \\\\ a_{21} x_1 + a_{22} x_2 + \\ldots + a_{2K} x_K \\\\ \\vdots \\\\ a_{n1} x_1 + a_{n2} x_2 + \\ldots + a_{nK} x_K \\\\ \\end{pmatrix} \\end{align*} \\] Notice that the first row of the last matrix above has the sum first row of the matrix \\(\\mathbf{A}\\) multiplied by the corresponding elements in \\(\\mathbf{x}\\) (i.e., first element \\(a_{11}\\) of the first row of \\(\\mathbf{A}\\) times the first element \\(x_1\\) of \\(\\mathbf{x}\\) plus the second, third, fourth, etc.). Likewise, this pattern holds for the second row, and all the other rows. This gives an algorithm for evaluating the product \\(\\mathbf{A} \\mathbf{x}\\). Definition 4.1 The product \\(\\mathbf{A}\\mathbf{x}\\) of a \\(n \\times K\\) matrix \\(\\mathbf{A}\\) with a \\(K\\)-vector \\(\\mathbf{x}\\) is a \\(n\\)-vector where the \\(i\\)th element of \\(\\mathbf{A}\\mathbf{x}\\) is the sum of the \\(i\\)th row of \\(\\mathbf{A}\\) times the corresponding elements of the vector \\(\\mathbf{x}\\) Example: in class Example: in class Example: in R using loops Example: in R using %*% 4.4 Properties of matrix-vector multiplication If \\(\\mathbf{A}\\) is a \\(n \\times K\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^K\\) and \\(c\\) is a scalar, then \\(\\mathbf{A} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{A} \\mathbf{u} + \\mathbf{A} \\mathbf{v}\\) \\(\\mathbf{A} (c \\mathbf{u}) = (c \\mathbf{A}) \\mathbf{u}\\) Proof in class 4.5 Solutions of linear systems 4.5.1 Homogeneous linear systems of equations Definition 4.2 The matrix equation \\[ \\begin{align} \\tag{4.2} \\mathbf{A}\\mathbf{x} = \\mathbf{0} \\end{align} \\] is called a homogeneous system of equations. The vector \\(\\mathbf{0}\\) is a vector of length \\(K\\) composed of all zeros. The trivial solution of the homogeneous equation is when \\(\\mathbf{x} = \\mathbf{0}\\) and is not a very useful solution. Typically one is interested in nontrivial solutions where \\(\\mathbf{x} \\neq \\mathbf{0}\\). The homogeneous linear system of equations can be written in augmented matrix form \\[ \\begin{align*} \\begin{pmatrix} \\mathbf{a}_1 &amp; \\ldots &amp; \\mathbf{a}_K &amp; \\mathbf{0} \\end{pmatrix} \\end{align*} \\] which implies that a non-trivial solution only exists if there is a free variable. Another way of saying this is that at least one column must not be a pivot column. If every column were a pivot column, the reduced row echelon form of the augmented matrix would be \\[ \\begin{align*} \\begin{pmatrix} 1 &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\ldots &amp; 1 &amp; 0 \\end{pmatrix} \\end{align*} \\] which implies the only solution is the trivial solution \\(\\mathbf{0}\\). Example: in class \\[ \\begin{align*} 3 x_1 - 2 x_2 + 4 x_3 = 0 \\\\ - 2 x_1 + 4 x_2 - 2 x_3 = 0 \\\\ 5 x_1 - 6 x_2 + 6 x_3 = 0 \\end{align*} \\] * Example: in class Consider the equation \\[ \\begin{align*} 2x_1 + 4 x_2 - x_3 = 0. \\end{align*} \\] we can write this as \\[ \\begin{align*} x_1 = -2 x_2 + \\frac{1}{2} x_3 \\end{align*} \\] where \\(x_2\\) and \\(x_3\\) are free variables. Writing this as a solution \\(\\mathbf{x}\\) gives \\[ \\begin{align*} \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} -2 x_2 + \\frac{1}{2} x_3 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = x_2 \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} + x_3 \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{align*} \\] which is a linear combination of the vectors \\(\\mathbf{u} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix}\\). This implies that we can write the solution \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) for scalars \\(a\\) and \\(b\\). Therefore, the solution set \\(\\mathbf{x}\\) is contained in the \\(\\mbox{span}\\{\\mathbf{u}, \\mathbf{v}\\}\\). Because the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are linearly independent (they don’t point in the same direction), the set of all linear combinations of \\(c \\mathbf{u} + d \\mathbf{v}\\) defines a plane. Definition 4.3 A solution set of the form \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) is called a parametric vector solution. 4.6 Solutions to nonhomogeneous systems Recall the simple linear equation \\[ y = mx + b \\] where \\(m\\) is the slope and \\(b\\) is the y-intercept. Setting \\(b = 0\\) gives a simple homogenous linear equation where the y-intercept goes through the origin (0, 0). When \\(b\\) is nonzero, the line keeps the same slope but is shifted upward/downward by \\(b\\). ggplot(data = data.frame(x = 0, y = 0), aes(x, y)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_abline(slope = 2, intercept = 0, color = &quot;red&quot;) + geom_abline(slope = 2, intercept = 2, color = &quot;blue&quot;) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_text( data = data.frame(x = c(0, 0), y = c(0, 2), text = c(&quot;homogeneous\\nsolution&quot;, &quot;inhomogeneous\\nsolution&quot;)), aes(x = x + c(1.75, -1), y = y + 0.5, label = text), size = 5, inherit.aes = FALSE, color = c(&quot;red&quot;, &quot;blue&quot;)) + geom_segment( aes(x = 0, xend = 0, y = 0, yend = 2), arrow = arrow(length = unit(0.1, &quot;inches&quot;)), size = 1.5, color = &quot;orange&quot;) + geom_text( data = data.frame(x = 0, y = 2, text = &quot;b&quot;), aes(x = x + 0.5, y = y, label = text), size = 8, inherit.aes = FALSE, color = &quot;orange&quot;) This shift in location (but not in slope) is called a translation Example: Show this shift for a system of linear equations where the solution set defines a plane. From example above, \\[ \\begin{align*} 2x_1 + 4 x_2 - x_3 = 0. \\end{align*} \\] has the parametric solution \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v}\\) with \\[ \\begin{align*} \\mathbf{u} &amp; = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} &amp; \\mathbf{v} &amp; = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{align*} \\] Now, if we change the system of linear equations so that we have the inhomogeneous equation \\[ \\begin{align*} 2x_1 + 4 x_2 - x_3 = 20. \\end{align*} \\] we get the homogeneous solution set \\(x_1 = -2 x_2 + \\frac{1}{2} x_3 + 10\\) which can be written in parametric form as \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v} + \\mathbf{p}\\) with \\[ \\begin{align*} \\mathbf{u} &amp; = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} &amp; \\mathbf{v} &amp; = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ 1 \\end{pmatrix} &amp; \\mathbf{p} &amp; = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix} \\end{align*} \\] For plotting, we will solve these equations for \\(x_3\\), letting \\(x_1\\) and \\(x_2\\) be free variables (this is just for the requirements of the plotting function). Thus, the homogeneous equation has the solution \\(x_3 = 2x_1 + 4x_2\\) and the inhomogenous equation has the solution \\(x_3 = 2x_1 + 4x_2 - 20\\). # uses gg3D library n &lt;- 60 x1 &lt;- x2 &lt;- seq(-10, 10, length = n) region &lt;- expand.grid(x1 = x1, x2 = x2) df &lt;- data.frame( x1 = region$x1, x2 = region$x2, x3 = c( 2 * region$x1 + 4 * region$x2, 2 * region$x1 + 4 * region$x2 - 20), equation = rep(c(&quot;inhomogeneous&quot;, &quot;homogeneous&quot;), each = n^2)) # theta and phi set up the &quot;perspective/viewing angle&quot; of the 3D plot theta &lt;- 45 phi &lt;- 20 ggplot(df, aes(x = x1, y = x2, z = x3, color = equation)) + axes_3D(theta = theta, phi = phi) + stat_wireframe( alpha = 0.75, theta = theta, phi = phi) + scale_color_manual(values = c(&quot;inhomogeneous&quot; = &quot;blue&quot;, &quot;homogeneous&quot; = &quot;red&quot;)) + theme_void() + labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), angle=c(0, 0, 90), theta = theta, phi = phi) ## Warning: Removed 4 row(s) containing missing values (geom_path). Example: in class Let’s revisit the example from before \\[ \\begin{align*} 3 x_1 - 2 x_2 + 4 x_3 = 0 \\\\ - 2 x_1 + 4 x_2 - 2 x_3 = 0 \\\\ 5 x_1 - 6 x_2 + 6 x_3 = 0 \\end{align*} \\] but change this so that \\(\\mathbf{b} = \\begin{pmatrix} 2 \\\\ -6 \\\\ 8 \\end{pmatrix}\\) Write this as a parametric solution with a mean shift A &lt;- matrix(c(3, -2, 5, -2, 4, -6, 4, -2, 6, 2, -6, 8), 3, 4) rref(A) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 1.50 -0.50 ## [2,] 0 1 0.25 -1.75 ## [3,] 0 0 0.00 0.00 \\[ \\begin{align*} x_1 = \\frac{3}{2} x_2 - \\frac{1}{2}\\\\ x_2 = \\frac{1}{4} x_3 - \\frac{7}{4} \\\\ \\end{align*} \\] which was the same solution set as the homoegenous solution plus the additional vector \\(\\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{7}{4} \\\\ 0 \\end{pmatrix}\\). Thus, the inhomogenous solution is now \\(\\mathbf{x} = c \\mathbf{u} + d \\mathbf{v} + \\mathbf{p}\\) where \\[ \\begin{align*} \\mathbf{u} &amp;= \\begin{pmatrix} 1 \\\\ 0 \\\\ \\frac{3}{2} \\end{pmatrix} &amp; \\mathbf{v} &amp;= \\begin{pmatrix} 0 \\\\ 1 \\\\ \\frac{1}{4} \\end{pmatrix} &amp; \\mathbf{p} &amp;= \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{7}{4} \\\\ 0 \\end{pmatrix} \\end{align*} \\] 4.7 Finding solutions The following algorithm describes how to solve a linear system of equations. Put the system of equations in an augmented matrix form Reduce the augmented matrix to reduced row echelon form Express each determined variable as a function of the free variables. Write the solution in a general form where the determined variables are a function of the independent variables Decompose the solution \\(\\mathbf{x}\\) into a linear combination of free variables as parameters "],["section-linear-independence.html", "Chapter 5 Linear independence", " Chapter 5 Linear independence Recall the homogeneous equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\) can be written as a linear combination of coefficients \\(x_1, \\ldots, x_K\\) and vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) where \\[ \\begin{align*} \\sum_{k=1}^K x_k \\mathbf{a}_k = \\mathbf{0} \\end{align*} \\] Definition 5.1 The set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) are called linearly independent if the only solution to the vector equation \\(\\sum_{k=1}^K x_k \\mathbf{a}_k = \\mathbf{0}\\) is the trivial solution. The set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) are called linearly dependent if there are coefficients \\(x_1, \\ldots, x_K\\) that are not all zero. Example: in class What does it mean for a set of vectors to be linearly dependent? This means that there is at least one vector \\(\\mathbf{a}_k\\) that can be written as a sum of the other vectors with coefficients \\(z_k\\): \\[ \\begin{align*} \\mathbf{a}_k = \\sum_{k&#39; \\neq k} z_{k&#39;} \\mathbf{a}_{k&#39;} \\end{align*} \\] Note: this does not imply that all vectors \\(\\mathbf{a}_{k}\\) can be written as a linear combination of other vectors. Example: in class – determine if the vectors are linearly independent and solve the dependence relation Theorem 5.1 The matrix equation \\(\\mathbf{A}\\) has linearly independent columns if and only if the equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution. Example: in class A set of a single vector Example: in class A set of two vectors linearly independent if: linearly dependent if one vector is a scalar multiple of the other: Theorem 5.2 If an \\(n \\times K\\) matrix \\(\\mathbf{A}\\) has \\(K &gt; n\\), then the columns of \\(\\mathbf{A}\\) are linearly dependent. In other words, if a set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) contains more vectors than entries within vectors, the set of vectors is linearly dependent. Proof: If \\(K&gt;n\\), there are more variables (\\(K\\)) than equations (\\(n\\)). Therefore, there is at least one free variable and this implies that the homogeneous equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\) has a non-trivial solution (4.2) Theorem 5.3 If a set of vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_K\\) contains the \\(\\mathbf{0}\\) vector, then the the set of vectors is linearly dependent. Proof: in class Example: in class Determine whether the following sets of vectors are linearly dependent "],["section-linear-transformations.html", "Chapter 6 Linear Transformations 6.1 Linear Transformations 6.2 Types of matrix transformations 6.3 Properties of matrix transformations", " Chapter 6 Linear Transformations library(tidyverse) library(dasc2594) library(gifski) ## Linking to ImageMagick 6.9.11.32 ## Enabled features: cairo, fontconfig, freetype, lcms, pango, rsvg, webp ## Disabled features: fftw, ghostscript, x11 ## Using poppler version 0.73.0 It is often useful to think of \\(\\mathbf{A}\\mathbf{x}\\) as a linear transformation defined by the matrix \\(\\mathbf{A}\\) applied to the vector \\(\\mathbf{x}\\). A linear transformation is mathematically defined as a function/mapping \\(T(\\cdot)\\) (\\(T\\) for transformation) from a domain in \\(\\mathcal{R}^n\\) (function input) to a codomain in \\(\\mathcal{R}^m\\) (function output). In shorthand, this is written as \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) which is read a “\\(T\\) maps inputs from the domain \\(\\mathcal{R}^n\\) to the codomain \\(\\mathcal{R}^m\\).” For each \\(\\mathbf{x} \\in \\mathcal{R}^n\\) (in the domain), \\(T(\\mathbf{x}) \\in \\mathcal{R}^m\\) is known as the image of \\(\\mathbf{x}\\). The set of all \\(T(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathcal{R}^n\\) is known as the range of \\(T(\\mathbf{x})\\). Note that it is possible that the range of \\(T(\\mathbf{x})\\) is not required to be the entire space \\(\\mathcal{R}^m\\) (i.e., the range of the transformation \\(T\\) might be a subset of \\(\\mathcal{R}^m\\)) Draw figure In the case of matrix transformations (linear transformations), the function \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x}\\) where \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix and \\(\\mathbf{x} \\in \\mathcal{R}^n\\) is a \\(n\\)-vector. Question: What kind of object is \\(\\mathbf{A} \\mathbf{x}\\)? scalar vector matrix array Question What are the dimensions of \\(\\mathbf{A} \\mathbf{x}\\)? Using the matrix transformation notation, the domain of the transformation \\(T\\) is \\(\\mathcal{R}^n\\), the codomain of \\(\\mathcal{T}\\) \\(\\mathcal{R}^m\\). The range of the transformation \\(T\\) is the set of all linear combinations of the columns of \\(\\mathbf{A}\\) (the \\(\\mbox{span}\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}\\)) because the transformation \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x}\\) is a linear combination \\(\\sum_{i=1}^n x_i \\mathbf{a}_i\\) of the columns \\(\\{\\mathbf{a}_i\\}_{i=1}^n\\) of \\(\\mathbf{A}\\) with coefficients \\(x_1, \\ldots, x_n\\) Example: \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} 2 &amp; 4 \\\\ -3 &amp; 1 \\\\ -1 &amp; 6 \\end{pmatrix} &amp;&amp; \\mathbf{u} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} &amp;&amp; \\mathbf{b} = \\begin{pmatrix} -2 \\\\ -11 \\\\ -15 \\end{pmatrix} &amp;&amp; \\mathbf{c} = \\begin{pmatrix} 2 \\\\ -2 \\\\ -1 \\end{pmatrix} \\end{align*} \\] ## [,1] ## [1,] 14 ## [2,] 0 ## [3,] 17 ## b ## [1,] 1 0 3 ## [2,] 0 1 -2 ## [3,] 0 0 0 ## c ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Find the image of \\(\\mathbf{u}\\) using the matrix transformation \\(T\\) (e.g., calculate \\(T(\\mathbf{u})\\)). Find a coefficient vector \\(\\mathbf{x} \\in \\mathcal{R}^2\\) such that \\(T(\\mathbf{x})\\). Is there more than one \\(\\mathbf{x}\\) whose image under \\(T\\) is \\(\\mathbf{b}?\\) In other words, is the solution \\(\\mathbf{A} \\mathbf{x}= \\mathbf{b}\\) unique? Determine if \\(\\mathbf{c}\\) is in the range of \\(T\\). In other words, does the solution \\(\\mathbf{A} \\mathbf{x}= \\mathbf{c}\\) exist? 6.1 Linear Transformations Definition 6.1 A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is linear if \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) for all \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in the domain of \\(T\\) \\(T(c \\mathbf{u}) = c T(\\mathbf{u})\\) for all scalars \\(c\\) and all vectors \\(\\mathbf{u}\\) in the domain of \\(T\\) Note: Because a linear transformation is equivalent to a matrix transformation, the definition above is equivalent to the following matrix-vector multiplication properties If \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^m\\) and \\(c\\) is a scalar, then \\(\\mathbf{A} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{A} \\mathbf{u} + \\mathbf{A} \\mathbf{v}\\) \\(\\mathbf{A} (c \\mathbf{u}) = (c \\mathbf{A}) \\mathbf{u}\\) As a consequence of the previous definition, the following properties hold for scalars \\(c\\) and \\(d\\) and vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v} \\in \\mathcal{R}^m\\) \\(T(\\mathbf{0}) = \\mathbf{0}\\) \\(T(c \\mathbf{u} + d \\mathbf{v}) = c T(\\mathbf{u}) + d T(\\mathbf{v})\\) Show why in class These properties give rise to the following statement for scalars \\(c_1, \\ldots, c_m\\) and vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_m \\in \\mathcal{R}^n\\) \\(T(c_1 \\mathbf{u}_1 + \\ldots + c_m \\mathbf{u}_m) = c_1 T(\\mathbf{u}_1) + \\ldots + c_m T(\\mathbf{u}_m)\\) The statements above for linear transformations are equivalent to the matrix statements where \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathcal{R}^m\\) and \\(c\\) is a scalar: \\(\\mathbf{A} \\mathbf{0} = \\mathbf{0}\\) \\(\\mathbf{A}(c \\mathbf{u} + d \\mathbf{v}) = c \\mathbf{A} \\mathbf{u} + d \\mathbf{A} \\mathbf{v}\\) And for a \\(m \\times n\\) matrix \\(\\mathbf{A}\\), scalars \\(c_1, \\ldots, c_m\\), and vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_m \\in \\mathcal{R}^n\\) \\(\\mathbf{A}(c_1 \\mathbf{u}_1 + \\ldots + c_m \\mathbf{u}_m) = c_1 \\mathbf{A}\\mathbf{u}_1 + \\ldots + c_m \\mathbf{A} \\mathbf{u}_m\\) 6.2 Types of matrix transformations The basic types of matrix transformations include contractions/expansions rotations reflections shears projections For the following examples, we will consider the unit vectors \\(\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\) and apply different linear transformations using the matrix \\(\\mathbf{A}\\). To build the matrix transformations, we use the dasc2594 package and build matrix transformations based on code from https://www.bryanshalloway.com/2020/02/20/visualizing-matrix-transformations-with-gganimate/. 6.2.1 Contractions/Expansions 6.2.1.1 Horizonal Expansion The matrix below gives a horizontal expansion when \\(x &gt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} x &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] In the example below, we set \\(x = 2\\) and generate the transformation. transformation_matrix &lt;- tribble( ~ x, ~ y, 2, 0, 0, 1) %&gt;% as.matrix() p &lt;- plot_transformation(transformation_matrix) 6.2.1.2 Horizonal Contraction The matrix below gives a horizontal contraction when \\(x &lt; 1\\) * Horizontal contraction when \\(x &lt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} x &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] In the example below, we set \\(x = 0.5\\) 6.2.1.3 Vertical Expansion The matrix below gives a vertical expansion when \\(x &gt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; x \\end{pmatrix} \\] * In the example below, we set \\(x = 2\\) 6.2.1.4 Vertical Contraction The matrix below gives a vertical contraction when \\(x &lt; 1\\) \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; x \\end{pmatrix} \\] In the example below, we set \\(x = 0.5\\) 6.2.2 Rotations 6.2.2.1 Rotation by 90 degrees Rotations in 2D of an angle \\(\\theta \\in [0, 2\\pi]\\) take the form of \\[ \\mathbf{A} = \\begin{pmatrix} \\cos(\\theta) &amp; -\\sin(\\theta) \\\\ \\sin(\\theta) &amp; \\cos(\\theta) \\end{pmatrix} \\] For example, a rotation of 90 degrees counter-clockwise (\\(\\theta = \\frac{\\pi}{2}\\)) is given by the transformation matrix \\[ \\mathbf{A} = \\begin{pmatrix} \\cos(\\frac{\\pi}{2}) &amp; -\\sin(\\frac{\\pi}{2}) \\\\ \\sin(\\frac{\\pi}{2}) &amp; \\cos(\\frac{\\pi}{2}) \\end{pmatrix} = \\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] Another example is for a rotation of 45 degrees clockwise (\\(\\theta = -\\frac{\\pi}{4}\\)) is given by the transformation matrix \\[ = \\begin{pmatrix} \\cos(\\frac{\\pi}{4}) &amp; -\\sin(\\frac{\\pi}{4}) \\\\ \\sin(\\frac{\\pi}{4}) &amp; \\cos(\\frac{\\pi}{4}) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{2}}{2} &amp; -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} &amp; \\frac{\\sqrt{2}}{2} \\end{pmatrix} \\] 6.2.3 Reflections 6.2.3.1 Reflection across the x-axis The matrix below gives a reflection about the x-axis \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} \\] 6.2.3.2 Reflection across the y-axis The matrix below gives a reflection about the y-axis \\[ \\mathbf{A} = \\begin{pmatrix} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] 6.2.3.3 Reflection across the line y = x \\[ \\mathbf{A} = \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] In the example below, we set \\(x = 0.5\\) 6.2.3.4 Reflection across the line y = - x \\[ \\mathbf{A} = \\begin{pmatrix} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{pmatrix} \\] 6.2.3.5 Reflection across the origin (0, 0) \\[ \\mathbf{A} = \\begin{pmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} \\] 6.2.4 Shears A shear transformation is like stretching play-dough if it was possible to stretch all parts of the dough uniformly (rather than some sections getting stretched more than others). 6.2.4.1 Horizontal Shear \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; x \\\\ 0 &amp; 1 \\end{pmatrix} \\] For the example below, we plot a horizontal shear with \\(x = 2\\). 6.2.4.2 Vertical Shear \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ x &amp; 1 \\end{pmatrix} \\] For the example below, we plot a horizontal shear with \\(x = 2\\). 6.2.5 Projections A projection is a mapping \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) from one space (\\(\\mathbf{R}^n\\)) to itself (\\(\\mathbf{R}^n\\)) such that \\(T^2 = T\\) 6.2.5.1 Project onto the x-axis \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} \\] For the example below, we plot a projection onto the x-axis 6.2.5.2 Project onto the y-axis \\[ \\mathbf{A} = \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] For the example below, we plot a projection onto the y-axis 6.2.6 Identity The identity transformation is the transformation that leaves the vector input unchanged. The identity matrix is typically written as \\(\\mathbf{I}\\) \\[ \\mathbf{I} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] 6.3 Properties of matrix transformations 6.3.1 One-to-one transformations Definition 6.2 A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is called one-to-one if every vector \\(\\mathbf{b}\\) in the image \\(\\mathcal{R}^m\\), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at most one solution in \\(\\mathcal{R}^n\\). The following statements are equivalent was of saying \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is one-to-one: For every \\(\\mathbf{b} \\in \\mathcal{R}^m\\) (for every vector in the image), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has either zero or one solution Every different input into the function \\(T(\\cdot)\\) has a different output If \\(T(\\mathbf{u}) = T(\\mathbf{v})\\) then \\(\\mathbf{u} = \\mathbf{v}\\) The following statements are equivalent was of saying \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is not one-to-one: There exists as least one \\(\\mathbf{b} \\in \\mathcal{R}^m\\) such that the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has more than one solution There are at least two different inputs into the function \\(T(\\cdot)\\) that have the same output There exist vectors \\(\\mathbf{u} \\neq \\mathbf{v} \\in \\mathcal{R}^n\\) such that \\(T(\\mathbf{u}) = T(\\mathbf{v})\\) Theorem 6.1 Let \\(\\mathbf{A}\\mathbf{x}\\) be the matrix representation of the linear transformation \\(T(\\mathbf{x})\\) for the \\(m \\times n\\) matrix \\(\\mathbf{A}\\). Then the following statements are equivalent: \\(T\\) is one-to-one. For every \\(\\mathbf{b} \\in \\mathcal{R}^m\\), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at most one solution. For every \\(\\mathbf{b} \\in \\mathcal{R}^m\\), the equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) has a unique solution or is inconsistent. The equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only a trivial solution. The columns of \\(\\mathbf{A}\\) are linearly independent. \\(\\mathbf{A}\\) has a pivot in every column. The range of \\(\\mathbf{A}\\) has dimension \\(n\\) Example: is the following matrix one-to-one? \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix} \\] Example: is the following matrix one-to-one? \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\end{pmatrix} \\] Note: Matrices that are wider than they are tall are not one-to-one transformations. (This does not mean that all tall matrices are one-to-one) 6.3.2 Onto transformations Definition 6.3 A transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is called onto if, for every vector \\(\\mathbf{b} \\in \\mathcal{R}^m\\), the equation \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at least one solution \\(\\mathbf{x} \\in \\mathcal{R}^n\\) The following are equivalent ways of saying that \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is onto: The range of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is equal to the codomain of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) Every vector in the codomain is the output of some input vector The following are equivalent ways of saying that \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is not onto: The range of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) is smaller than the codomain of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\). There exists a vector \\(\\mathbf{b} \\in \\mathcal{R}^m\\) such that the equation \\(T(\\mathbf{x})\\) does not have a solution. There is a vector in the codomain that is not the output of any input vector. Theorem 6.2 Let \\(\\mathbf{A}\\mathbf{x}\\) be the matrix representation of the linear transformation \\(T(\\mathbf{x})\\) for the \\(m \\times n\\) matrix \\(\\mathbf{A}\\). Then the following statements are equivalent: \\(T\\) is onto \\(T(\\mathbf{x}) = \\mathbf{b}\\) has at least one solution for every \\(\\mathbf{b} \\in \\mathcal{R}^m\\). The equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) is consistent for every \\(\\mathbf{b} \\in \\mathcal{R}^m\\). The columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^m\\) \\(\\mathbf{A}\\) has a pivot in every row The range of \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\) has dimension \\(m\\) Example: Example: is the following matrix onto? \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\end{pmatrix} \\] Example: is the following matrix one-to-one? \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] Note: Matrices that are taller than they are wide are not onto transformations. (This does not mean that all wide matrices are onto) "],["section-matrix-operations.html", "Chapter 7 Matrix operations 7.1 Properties of matrices", " Chapter 7 Matrix operations Note: add examples: 7.1 Properties of matrices 7.1.1 Matrix Addition Matrix Addition: If the matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of the same dimension (e.g., both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) have the same number of rows \\(m\\) and the same number of columns \\(n\\)), then \\[ \\begin{align*} \\tag{7.1} \\mathbf{A} + \\mathbf{B} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} + \\begin{pmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1n} \\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{m1} &amp; b_{m2} &amp; \\cdots &amp; b_{mn} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} a_{11} + b_{11} &amp; b_{12} + b_{12} &amp; \\cdots &amp; a_{1n} + b_{1n} \\\\ a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \\cdots &amp; a_{2n} + b_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \\cdots &amp; a_{mn} + b_{mn} \\end{pmatrix} \\\\ &amp; = \\left\\{ a_{ij} + b_{ij} \\right\\} \\end{align*} \\] … Another way to If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of the same dimension (same number of rows and columns) you can add the matrices together A &lt;- matrix(c(4, 1, 33, 2, 0, -4), 3, 2) B &lt;- matrix(c(7, -24, 3, 9, 11, -9), 3, 2) A ## [,1] [,2] ## [1,] 4 2 ## [2,] 1 0 ## [3,] 33 -4 B ## [,1] [,2] ## [1,] 7 9 ## [2,] -24 11 ## [3,] 3 -9 A + B ## [,1] [,2] ## [1,] 11 11 ## [2,] -23 11 ## [3,] 36 -13 We can also write this using for loops # initialize an empty matrix to fill C &lt;- matrix(0, 3, 2) for (i in 1:nrow(A)) { # loop over the rows for (j in 1:ncol(A)) { # loop over the columns C[i, j] &lt;- A[i, j] + B[i, j] } } C ## [,1] [,2] ## [1,] 11 11 ## [2,] -23 11 ## [3,] 36 -13 If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are of different dimensions (they differ in either the number of rows or columns), R will return an error warning you that the matrices are of different sizes and can’t be added A &lt;- matrix(c(4, 1, 33, 2, 0, -4), 3, 2) B &lt;- matrix(c(7, -24, 3, 9), 2, 2) A ## [,1] [,2] ## [1,] 4 2 ## [2,] 1 0 ## [3,] 33 -4 B ## [,1] [,2] ## [1,] 7 3 ## [2,] -24 9 A + B ## Error in A + B: non-conformable arrays Theorem 7.1 Let \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) be \\(m \\times n\\) matrices and let \\(a\\) and \\(b\\) be scalars, then: \\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\) \\((\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\) \\(\\mathbf{A} + \\mathbf{0} = \\mathbf{A}\\) \\(a (\\mathbf{A} + \\mathbf{B}) = a \\mathbf{A} + a \\mathbf{B}\\) \\((a + b)\\mathbf{A} = a \\mathbf{A} + b \\mathbf{A}\\) \\((ab)\\mathbf{A} = a (b\\mathbf{A})\\) 7.1.2 Matrix Multipliation Matrix Multiplication: If \\(\\mathbf{A} = \\left\\{ a_{ij} \\right\\}\\) is an \\(m \\times n\\) matrix and \\(\\mathbf{B} = \\left\\{ a_{jk} \\right\\}\\) is a \\(n \\times p\\) matrix, then the matrix product \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) is an \\(m \\times p\\) matrix where \\(\\mathbf{C} = \\left\\{ \\sum_{j=1}^p a_{ij} b{jk} \\right\\}\\) \\[ \\begin{align} \\tag{7.2} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\begin{pmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1p} \\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{n1} &amp; b_{n2} &amp; \\cdots &amp; b_{np} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} \\sum_{j=1}^n a_{1j} b_{j1} &amp; \\sum_{j=1}^n a_{1j} b_{j2} &amp; \\cdots &amp; \\sum_{j=1}^n a_{1j} b_{jp} \\\\ \\sum_{j=1}^n a_{2j} b_{j1} &amp;\\sum_{j=1}^n a_{2j} b_{j2} &amp; \\cdots &amp; \\sum_{j=1}^n a_{2j} b_{jp} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum_{j=1}^n a_{mj} b_{j1} &amp;\\sum_{j=1}^n a_{nj} b_{j2} &amp; \\cdots &amp; \\sum_{j=1}^n a_{mj} b_{jp} \\end{pmatrix} \\end{align} \\] Another way to define matrix multiplication is through inner product notation. Define the \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and the \\(n \\times p\\) matrix \\(\\mathbf{B}\\) as the partition \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{a}_{1}&#39; \\\\ \\mathbf{a}_{2}&#39; \\\\ \\vdots \\\\ \\mathbf{a}_{m}&#39; \\end{pmatrix} &amp; \\mbox{ and } &amp;&amp; \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{b}_{1} &amp; \\mathbf{b}_{2} &amp; \\cdots &amp; \\mathbf{b}_{p} \\end{pmatrix} \\end{align*} \\] where \\(\\mathbf{a}_i\\) and \\(\\mathbf{b}_k\\) are both \\(n\\)-vectors. Then, we have \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) can be written as \\[ \\begin{align*} \\mathbf{A} \\mathbf{B} = \\mathbf{A} \\begin{pmatrix} \\mathbf{b}_1 &amp; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{b}_p \\end{pmatrix} = \\begin{pmatrix} \\mathbf{A} \\mathbf{b}_1 &amp; \\mathbf{A} \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{A} \\mathbf{b}_p \\end{pmatrix} \\end{align*} \\] Note that in this representation, each column of the matrix \\(\\mathbf{A}\\mathbf{B}\\) is a linear combination the the columns of \\(\\mathbf{A}\\) with coefficients given by the corresponding column of \\(\\mathbf{B}\\). \\[ \\begin{align*} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{a}_1&#39; \\mathbf{b}_1 &amp; \\mathbf{a}_1&#39; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}_1&#39; \\mathbf{b}_q \\\\ \\mathbf{a}_2&#39; \\mathbf{b}_1 &amp; \\mathbf{a}_2&#39; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}_2&#39; \\mathbf{b}_q \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{a}_n&#39; \\mathbf{b}_1 &amp; \\mathbf{a}_n&#39; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}_n&#39; \\mathbf{b}_q \\end{pmatrix} \\\\ &amp; = \\left\\{ \\mathbf{a}_i&#39; \\mathbf{b}_k \\right\\}. \\end{align*} \\] Written in this notation, we arrive at the multiplication rule for \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) – the \\(ik\\)th element \\(c_{ik}\\) of \\(\\mathbf{C}\\) is the inner product of the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). 7.1.3 Properties of Matrix Multiplication Define the \\(m \\times m\\) identity matrix \\(\\mathbf{I}_m\\) with ones on the diagonal and zeros off diagonal as \\[ \\mathbf{I}_m = \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1\\end{pmatrix} \\] Let \\(\\mathbf{A}\\) be an \\(m \\times n\\) matrix, then: Let \\(\\mathbf{B}\\) be an \\(n \\times p\\) matrix and \\(\\mathbf{C}\\) a \\(p \\times q\\) matrix. Then \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\) is an \\(m \\times q\\) matrix. Let \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) be \\(n \\times p\\) matrices. Then \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\\) is an \\(m \\times p\\) matrix. Let \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) be \\(p \\times m\\) matrices. Then \\((\\mathbf{B} + \\mathbf{C})\\mathbf{A} = \\mathbf{B}\\mathbf{A} + \\mathbf{C}\\mathbf{A}\\) is an \\(p \\times m\\) matrix. Let \\(\\mathbf{B}\\) be an \\(p \\times m\\) matrix and \\(c\\) a scalar. Then \\(c(\\mathbf{A} \\mathbf{B}) = (c \\mathbf{A}) \\mathbf{B} = \\mathbf{A}(c\\mathbf{B})\\) is an \\(p \\times m\\) matrix. \\(\\mathbf{I}_m \\mathbf{A} = \\mathbf{A} \\mathbf{I}_n = \\mathbf{A}\\) Examples: in class Note: Matrix multiplication violates some of the rules of multiplication that you might be used to. Pay attention for the following: In general \\(\\mathbf{A} \\mathbf{B} \\neq \\mathbf{B} \\mathbf{A}\\) (sometimes these are equal, but usually are not) \\(\\mathbf{A}\\mathbf{B} = \\mathbf{A} \\mathbf{C}\\) does not imply \\(\\mathbf{B} = \\mathbf{C}\\) \\(\\mathbf{A}\\mathbf{B} = \\mathbf{0}\\) does not imply that \\(\\mathbf{A} = \\mathbf{0}\\) or \\(\\mathbf{B} = \\mathbf{0}\\) 7.1.4 Matrix Multiplication complexity (Big O notation) In the study of algorithms, the notation \\(O(n)\\) is used to describe the number of calculations that need to be done to evaluate the equation. As an example, consider \\(\\mathbf{A} = \\begin{pmatrix}3 &amp; 1 \\\\ 2 &amp; -3 \\end{pmatrix}\\), \\(\\mathbf{B} = \\begin{pmatrix} -2 &amp; 4 \\\\ -1 &amp; 2 \\end{pmatrix}\\), and \\(\\mathbf{x} = \\begin{pmatrix} -3 \\\\ 1 \\end{pmatrix}\\). By hand: Calculate \\((\\mathbf{A} \\mathbf{B}) \\mathbf{x}\\) \\(\\mathbf{A} (\\mathbf{B} \\mathbf{x})\\) Which was easier? Which required less calculation? Matrix-matrix multiplication of and \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and an \\(m \\times p\\) matrix \\(\\mathbf{B}\\) has complexity \\(O(m n p)\\). Matrix-vector multiplication of and \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and an \\(p\\)-vector \\(\\mathbf{x}\\) has complexity \\(O(n m)\\). From example above: \\(O(m n p)\\) matrix-matrix multiplication \\((\\mathbf{A} \\mathbf{B})\\) followed by \\(O(m n)\\) matrix-vector multiplication \\((\\mathbf{A} \\mathbf{B}) \\mathbf{x}\\) which has computational complexity \\(O(m n p) + O(m n)\\) \\(O(m n)\\) matrix-vector multiplication \\((\\mathbf{B} \\mathbf{x})\\) followed by \\(O(m n)\\) matrix-vector multiplication \\(\\mathbf{A} (\\mathbf{B} \\mathbf{x})\\) which has computational complexity \\(O(m n) + O(m n)\\) 7.1.5 Matrix powers Powers of a \\(n \\times n\\) (square) matrix are defined as the product of \\(\\mathbf{A}\\) multiplied \\(k\\) times \\[ \\mathbf{A}^k = \\underbrace{\\mathbf{A} \\cdots \\mathbf{A}}_k \\] 7.1.6 Matrix Transpose The matrix transpose is an operator that swaps the rows and columns of a matrix. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, then \\(\\mathbf{A}&#39;\\) is a \\(\\m \\times n\\) matrix (Note: some use \\(\\mathbf{A}^T\\) to denote a transpose; I prefer the \\(&#39;\\) notation as it is much simpler and cleaner notation). The matrix \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1p} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{np} \\end{pmatrix} \\end{align*} \\] has transpose \\[ \\begin{align*} \\mathbf{A}&#39; &amp; = \\begin{pmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{p1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{p2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1n} &amp; a_{2n} &amp; \\cdots &amp; a_{pn} \\end{pmatrix}, \\end{align*} \\] Theorem 7.2 Let \\(\\mathbf{A}\\) be an \\(m \\times n\\) matrix, then \\((\\mathbf{A}&#39;)&#39; = \\mathbf{A}\\). Let \\(\\mathbf{B}\\) be an \\(m \\times n\\) matrix, then \\((\\mathbf{A} + \\mathbf{B})&#39; = \\mathbf{A}&#39; + \\mathbf{B}&#39;\\). For any scalar \\(c\\), \\((c \\mathbf{A})&#39; = c \\mathbf{A}&#39;\\). Let \\(\\mathbf{B}\\) be an \\(n \\times p\\) matrix, then \\(( \\mathbf{A} \\mathbf{B})&#39; = \\mathbf{B}&#39; \\mathbf{A}&#39;\\) is an \\(p \\times m\\) matrix. Note: The power of video games: GPUs and modern CPUs are becoming more and more parallelized. Because the \\(ij\\)th element of \\(\\mathbf{A}\\mathbf{B}\\) requires only the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\), matrix multiplication is easily parallelized under modern computing architectures. Thanks to video games, this parallelization has been made faster than ever. Examples: in class "],["section-matrix-inverse.html", "Chapter 8 Matrix Inverses 8.1 Elementary matrices 8.2 Finding the inverse of \\(\\mathbf{A}\\) 8.3 The Invertible Matrix Theorem 8.4 Invertible Linear Transformations", " Chapter 8 Matrix Inverses library(dasc2594) library(tidyverse) For scalars, the multiplicative identity is \\[ a \\frac{1}{a} = a a^{-1} = a^{-1} a = 1 \\] where \\(a^{-1}\\) is the inverse of \\(a\\). The \\(n \\times n\\) square matrix \\(\\mathbf{A}\\) is said to be invertible if there exists a \\(n \\times n\\) matrix \\(\\mathbf{C}\\)( which we call \\(\\mathbf{A}^{-1}\\) once we verify the inverse exists) such that \\[ \\begin{align*} \\mathbf{C}\\mathbf{A} = \\mathbf{A} \\mathbf{C} &amp; = \\mathbf{I} \\\\ \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{-1} &amp; = \\mathbf{I} \\end{align*} \\] where \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix (the matrix with 1s on the diagonal and zeros everywhere else). In R, an identity matrix is easy to construct. An \\(n \\times n\\) identity matrix can be constructed using the diag() function n &lt;- 4 I &lt;- diag(n) I ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 Example: \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} 1 &amp; -1 \\\\ 2 &amp; -3 \\end{pmatrix} &amp;&amp; \\mathbf{B} = \\begin{pmatrix} 1 &amp; -1 \\\\ 2 &amp; -3 \\end{pmatrix} \\end{align*} \\] Theorem 8.1 Let \\(\\mathbf{A} = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\). If \\(ad - bc \\neq 0\\) then \\(\\mathbf{A}\\) is invertible and \\[ \\begin{align*} \\mathbf{A} = \\frac{1}{ad - bc} \\begin{pmatrix} d &amp; -b \\\\ -c &amp; a \\end{pmatrix} \\end{align*} \\] If \\(ad - bc = 0\\), then the matrix is not invertible. Question: why is the matrix not invertible when \\(ad - bc = 0\\)? Have you heard of “singular” or “singularity” before? Black holes are called singularities. Why is this? Square matrices that are not invertible are call “singular” Definition 8.1 For the \\(2 \\times 2\\) matrix \\(\\mathbf{A} = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\), the term \\(ad - bc\\) is called the determinant of the matrix \\(\\mathbf{A}\\) and is written as \\(\\operatorname{det}(\\mathbf{A})\\). Sometimes the determinant is written as \\(| \\mathbf{A}|\\) A consequence of the above theorem is that a \\(2 \\times 2\\) matrix is invertible only if its determinant is nonzero. Example: in class Determine if the following \\(2 \\times 2\\) matrix is invertible Theorem 8.2 If the \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is invertible, then for each \\(\\mathbf{b} \\in \\mathcal{R}^n\\), the matrix equation \\[ \\mathbf{A} \\mathbf{x} = \\mathbf{b} \\] has the unique solution \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\). Proof: in class Example in R: in class Theorem 8.3 (Invertible Matrix Theorem) 1) If \\(\\mathbf{A}\\) is an invertible matrix, then \\(\\mathbf{A}^{-1}\\) is invertible and \\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\) If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are \\(n \\times n\\) invertible matrices, then \\(\\mathbf{A} \\mathbf{B}\\) is also an invertible matrix whose inverse is \\[ (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1} \\] which is the inverse of the matrices in reverse order. If \\(\\mathbf{A}\\) is an invertible matrix, then the transpose \\(\\mathbf{A}&#39;\\) is also invertible and the inverse of \\(\\mathbf{A}&#39;\\) is the transpose of \\(\\mathbf{A}^{-1}\\). Equivalently, \\[ (\\mathbf{A}&#39;)^{-1} = (\\mathbf{A}^{-1})&#39; \\] Proof: in class *Note: The product of \\(k\\) invertible \\(n \\times n\\) matrices \\(\\mathbf{A}_1 \\mathbf{A}_2 \\cdots \\mathbf{A}_k\\) has inverse \\(\\mathbf{A}_k^{-1} \\mathbf{A}_{k-1}^{-1} \\cdots \\mathbf{A}_1^{-1}\\) 8.1 Elementary matrices Elementary matrices are matrices that perform basic row operations (i.e., we can write the reduced row echelon algorithm as a produce of elementary matrices). Recall the elementary row operations: swaps: swapping two rows. sums: replacing a row by the sum itself and a multiple of another row. scalar multiplication: replacing a row by a scalar multiple times itself. Example: Consider a \\(3 \\times 3\\) matrix A &lt;- matrix(c(4, 5, 9, -2, -4, 1, 4, 6, -2), 3, 3) \\(\\mathbf{A} = \\begin{pmatrix} 4 &amp; -2 &amp; 4 \\\\ 5 &amp; -4 &amp; 6 \\\\ 9 &amp; 1 &amp; -2 \\end{pmatrix}\\) What is the elementary matrix (let’s call it \\(\\mathbf{E}_1\\) that swaps the first and second rows of \\(\\mathbf{A}\\)? E_1 &lt;- matrix(c(0, 1, 0, 1, 0, 0, 0, 0, 1), 3, 3) \\(\\mathbf{E}_1 = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 5 -4 6 ## [3,] 9 1 -2 ## left multiple A by E_1 E_1 %*% A ## [,1] [,2] [,3] ## [1,] 5 -4 6 ## [2,] 4 -2 4 ## [3,] 9 1 -2 Thus, the matrix \\(\\mathbf{E}_1 = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) is the matrix that swaps the first and second row. What is the elementary matrix (let’s call it \\(\\mathbf{E}_2\\) that adds two times the first of \\(\\mathbf{A}\\) to the third row of \\(\\mathbf{A}\\)? E_2 &lt;- matrix(c(1, 0, 2, 0, 1, 0, 0, 0, 1), 3, 3) \\(\\mathbf{E}_2 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\end{pmatrix}\\) A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 5 -4 6 ## [3,] 9 1 -2 ## left multiple A by E_2 E_2 %*% A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 5 -4 6 ## [3,] 17 -3 6 Thus, the matrix \\(\\mathbf{E}_2 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\end{pmatrix}\\) is the matrix that adds two times the first of \\(\\mathbf{A}\\) to the third row of \\(\\mathbf{A}\\) What is the elementary matrix (let’s call it \\(\\mathbf{E}_3\\) that mutliples the second row of \\(\\mathbf{A}\\) by 3? E_3 &lt;- matrix(c(1, 0, 0, 0, 3, 0, 0, 0, 1), 3, 3) \\(\\mathbf{E}_3 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 5 -4 6 ## [3,] 9 1 -2 ## left multiple A by E_3 E_3 %*% A ## [,1] [,2] [,3] ## [1,] 4 -2 4 ## [2,] 15 -12 18 ## [3,] 9 1 -2 Thus, the matrix \\(\\mathbf{E}_3 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) is the matrix that mutliples the second row of \\(\\mathbf{A}\\) by 3. Question: Do you see any patterns with how the example elementary matrices look? \\[ \\begin{align*} \\mathbf{E_1} = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} &amp;&amp; \\mathbf{E_2} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\end{pmatrix} &amp;&amp; \\mathbf{E_3} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\end{align*} \\] The elementary matrices look like the identity matrix \\(\\mathbf{I}\\) with an elementary row operation applied to \\(\\mathbf{I}\\). In fact, this leads us to this general fact: Fact: If an elementary row matrix is applied to the \\(m \\times n\\) matrix \\(\\mathbf{A}\\), the result of this elementary row operation applied to \\(\\mathbf{A}\\) can be written as \\(\\mathbf{E} \\mathbf{A}\\) where \\(\\mathbf{E}\\) is the \\(m \\times m\\) identity matrix \\(\\mathbf{I}\\) with the respective elementary row operation applied to \\(\\mathbf{I}\\). Fact: Each elementary matrix \\(\\mathbf{E}\\) is invertible Example: in class The next theorem is quite important as the result gives an algorithm for calculating the inverse of a \\(n \\times n\\) matrix \\(\\mathbf{A}\\) which also makes it possible to solve matrix equations \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) Theorem 8.4 If an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is invertible, then \\(\\mathbf{A}\\) is row-equivalent to \\(\\mathbf{I}\\) (\\(\\mathbf{A} \\sim \\mathbf{I}\\); row-equivalent means \\(\\mathbf{A}\\) can be reduced to \\(\\mathbf{I}\\) using elementary row operations). The row-equivalency implies that there is a series of elementary row operations (e.g., elementary matrices \\(\\mathbf{E}_1, \\ldots, \\mathbf{E}_k\\)) that converts \\(\\mathbf{A}\\) to \\(\\mathbf{I}\\). In addition, the application of these row matrices to \\(\\mathbf{I}\\) transforms \\(\\mathbf{I}\\) to the matrix inverse \\(\\mathbf{A}^{-1}\\). Proof: in class 8.2 Finding the inverse of \\(\\mathbf{A}\\) The previous theorem states that for a \\(n \\times n\\) invertible matrix \\(\\mathbf{A}\\), the elementary row operations that covert \\(\\mathbf{A}\\) to \\(\\mathbf{I}\\) also convert \\(\\mathbf{I}\\) to \\(\\mathbf{A}^{-1}\\). This suggests an algorithm for finding the inverse \\(\\mathbf{A}^{-1}\\) of \\(\\mathbf{A}\\): Create the augmented matrix \\(\\begin{pmatrix} \\mathbf{A} &amp; \\mathbf{I} \\end{pmatrix}\\) and row reduce the augmented matrix. If the row-reduced augmented matrix is of the form \\(\\begin{pmatrix} \\mathbf{I} &amp; \\mathbf{A}^{-1} \\end{pmatrix}\\) then \\(\\mathbf{A}^{-1}\\) is the inverse of \\(\\mathbf{A}\\). If the leading matrix in the augmented matrix is not the identity matrix \\(\\mathbf{I}\\), then \\(\\mathbf{A}\\) is not row equivalent to \\(\\mathbf{I}\\) and is therefore not invertible. Example: Let \\(\\mathbf{A} = \\begin{pmatrix} -3 &amp; -3 &amp; -4 \\\\ -4 &amp; 2 &amp; -4 \\\\ 4 &amp; -4 &amp; 4 \\end{pmatrix}\\). Does \\(\\mathbf{A}\\) have an inverse, and if so, what is it? Example in R 8.3 The Invertible Matrix Theorem Theorem 8.5 (The Invertible Matrix Theorem) Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix. Then the following statements are equivalent (i.e., they are all either simultaneously true or false). \\(\\mathbf{A}\\) is an invertible matrix. \\(\\mathbf{A}\\) is row equivalent to the \\(n \\times n\\) identity matrix \\(\\mathbf{I}\\) (\\(\\mathbf{A} \\sim \\mathbf{I}\\)). \\(\\mathbf{A}\\) and \\(n\\) pivot columns. The homogeneous matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\). The columns of \\(\\mathbf{A}\\) are linearly independent. The linear transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) given by the matrix transformation \\(\\mathbf{x} \\rightarrow \\mathbf{A}\\mathbf{x}\\) is one-to-one. The inhomogeneous matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a unique solution for all \\(\\mathbf{b} \\in \\mathcal{R}^n\\). The columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^n\\). The linear transformation \\(\\mathbf{x} \\rightarrow \\mathbf{A} \\mathbf{x}\\) maps \\(\\mathcal{R}^n\\) onto \\(\\mathcal{R}^n\\). There is an \\(n \\times n\\) matrix \\(\\mathbf{C}\\) such that \\(\\mathbf{C}\\mathbf{A} = \\mathbf{I}\\). There is an \\(n \\times n\\) matrix \\(\\mathbf{D}\\) such that \\(\\mathbf{A}\\mathbf{D} = \\mathbf{I}\\). \\(\\mathbf{A}&#39;\\) is an invertible matrix. Proof: in class A result of the invertible matrix theorem is that if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are \\(n \\times n\\) matrices with \\(\\mathbf{A} \\mathbf{B} = \\mathbf{I}\\) then \\(\\mathbf{A} = \\mathbf{B}^{-1}\\) and \\(\\mathbf{B} = \\mathbf{A}^{-1}\\). 8.4 Invertible Linear Transformations Definition 8.2 A linear transformation \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) is said to be invertible if there exists a transformation \\(S:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) such that \\[ \\begin{align*} S(T(\\mathbf{x})) = \\mathbf{x} &amp;&amp; \\mbox{for all } \\mathbf{x} \\in \\mathcal{R}^n T(S(\\mathbf{x})) = \\mathbf{x} &amp;&amp; \\mbox{for all } \\mathbf{x} \\in \\mathcal{R}^n \\\\ \\end{align*} \\] Draw figure in class Theorem 8.6 Let \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) be a linear transformation and let \\(\\mathbf{A}\\) be the matrix representing the transformation \\(T\\). Then the transformation \\(T\\) is invertible if and only if the matrix \\(\\mathbf{A}\\) is invertible. Therefore, the matrix that represents \\(S:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\), the inverse transformation of \\(T\\), is unique and is represented by the matrix \\(\\mathbf{A}^{-1}\\). "],["section-block-matrices.html", "Chapter 9 Block Matrices 9.1 Block Matrix Addition 9.2 Block Matrix Multiplication 9.3 The column-row matrix product 9.4 Special Block Matrices", " Chapter 9 Block Matrices Another way to represent matrices is using a block (or partitioned) form. A block-representation of a matrix arises when the \\(n \\times p\\) matrix \\(\\mathbf{A}\\) is represented using smaller blocks as follows: \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} &amp; \\cdots &amp; \\mathbf{A}_{1K} \\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} &amp; \\cdots &amp; \\mathbf{A}_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{J1} &amp; \\mathbf{A}_{J2} &amp; \\cdots &amp; \\mathbf{A}_{JK} \\\\ \\end{pmatrix} \\\\ \\end{align*} \\] where \\(\\mathbf{A}_{ij}\\) is a \\(n_j \\times p_k\\) matrix where \\(\\sum_{j=1}^J n_j = n\\) and \\(\\sum_{k=1}^K p_k = p\\). For example, the matrix \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} 5 &amp; 7 &amp; 1 \\\\ 5 &amp; -22 &amp; 2 \\\\ -14 &amp; 5 &amp; 99 \\\\ 42 &amp; -3 &amp; 0\\end{pmatrix}, \\end{align*} \\] can be written in block matrix form with \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} \\end{pmatrix} \\\\ &amp; = \\begin{pmatrix} \\begin{bmatrix} 5 &amp; 7 \\\\ 5 &amp; -22 \\end{bmatrix} &amp; \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\\\ \\begin{bmatrix} -14 &amp; 5 \\\\ 42 &amp; -3 \\end{bmatrix} &amp; \\begin{bmatrix} 99 \\\\ 0 \\end{bmatrix} \\end{pmatrix}, \\end{align*} \\] where \\(\\mathbf{A}_{11} = \\begin{bmatrix} 5 &amp; 7 \\\\ 5 &amp; -22 \\end{bmatrix}\\) is a \\(2 \\times 2\\) matrix, \\(\\mathbf{A}_{12} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) is a \\(2 \\times 1\\) matrix, etc. A_11 &lt;- matrix(c(5, 5, 7, -22), 2, 2) A_12 &lt;- c(1, 2) A_21 &lt;- matrix(c(-14, 42, 5, -3), 2, 2) A_22 &lt;- c(99, 0) ## bind columns then rows rbind( cbind(A_11, A_12), cbind(A_21, A_22) ) ## A_12 ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 ## bind rows then columns cbind( rbind(A_11, A_21), c(A_12, A_22) ## rbind on vectors is different than c() ) ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 ## bind rows then columns cbind( rbind(A_11, A_21), ## convert the vectors to matrices for rbind rbind(as.matrix(A_12), as.matrix(A_22)) ) ## [,1] [,2] [,3] ## [1,] 5 7 1 ## [2,] 5 -22 2 ## [3,] -14 5 99 ## [4,] 42 -3 0 9.1 Block Matrix Addition If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m \\times n\\) block matrices with blocks in \\(r\\) rows and \\(c\\) columns where \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} &amp; \\cdots &amp; \\mathbf{A}_{1c}\\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} &amp;\\cdots &amp; \\mathbf{A}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{r1} &amp; \\mathbf{A}_{r2} &amp;\\cdots &amp; \\mathbf{A}_{rc} \\\\ \\end{pmatrix} &amp; \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{B}_{11} &amp; \\mathbf{B}_{12} &amp; \\cdots &amp; \\mathbf{B}_{1c}\\\\ \\mathbf{B}_{21} &amp; \\mathbf{B}_{22} &amp;\\cdots &amp; \\mathbf{B}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{B}_{r1} &amp; \\mathbf{B}_{r2} &amp;\\cdots &amp; \\mathbf{B}_{rc} \\\\ \\end{pmatrix} \\\\ \\end{align*} \\] and each block \\(\\mathbf{A}_{ij}\\) and \\(\\mathbf{B}_ij\\) have the same dimension, then \\[ \\begin{align} \\tag{9.1} \\mathbf{A} + \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} + \\mathbf{B}_{11} &amp; \\mathbf{A}_{12} + \\mathbf{B}_{12} &amp; \\cdots &amp; \\mathbf{A}_{1c} + \\mathbf{B}_{1c}\\\\ \\mathbf{A}_{21} + \\mathbf{B}_{21} &amp; \\mathbf{A}_{22} + \\mathbf{B}_{22} &amp; \\cdots &amp; \\mathbf{A}_{2c} + \\mathbf{B}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{r1} + \\mathbf{B}_{r1} &amp; \\mathbf{A}_{r2} + \\mathbf{B}_{r2} &amp; \\cdots &amp; \\mathbf{A}_{rc} + \\mathbf{B}_{rc} \\\\ \\end{pmatrix} \\end{align} \\] which is a matrix where each block is the sum of the other blocks. Notice that if each block was a scalar rather than a block matrix, this would be the usual definition of matrix addition (compare equation (7.1) above to (9.1)). The one requirement is that each of the blocks \\(\\mathbf{A}_{ij}\\) and \\(\\mathbf{B}_ij\\) have the same dimension. When this is true, we say that \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are conformable for block matrix addition. 9.2 Block Matrix Multiplication If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m \\times n\\) block matrices with blocks in \\(r\\) rows and \\(c\\) columns (same as above) where \\[ \\begin{align*} \\mathbf{A} &amp; = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} &amp; \\cdots &amp; \\mathbf{A}_{1c}\\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} &amp;\\cdots &amp; \\mathbf{A}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{r1} &amp; \\mathbf{A}_{r2} &amp;\\cdots &amp; \\mathbf{A}_{rc} \\\\ \\end{pmatrix} &amp; \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{B}_{11} &amp; \\mathbf{B}_{12} &amp; \\cdots &amp; \\mathbf{B}_{1c}\\\\ \\mathbf{B}_{21} &amp; \\mathbf{B}_{22} &amp;\\cdots &amp; \\mathbf{B}_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{B}_{r1} &amp; \\mathbf{B}_{r2} &amp;\\cdots &amp; \\mathbf{B}_{rc} \\\\ \\end{pmatrix} \\\\ \\end{align*} \\] and each row of blocks \\(\\mathbf{A}_{ij}\\) has the same number of columns as the block \\(\\mathbf{B}_ij\\) has rows, then the block matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are said to be conformable for block matrix multiplication. A consequence of this is that \\(r = c\\). When this is the case, the matrix products is \\[ \\begin{align*} \\tag{9.2} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} \\sum_{j = 1}^c \\mathbf{A}_{1j} \\mathbf{B}_{j1} &amp; \\sum_{j = 1}^c \\mathbf{A}_{1j} \\mathbf{B}_{j2} &amp; \\cdots &amp; \\sum_{j = 1}^c \\mathbf{A}_{1j} \\mathbf{B}_{jc} \\\\ \\sum_{j = 1}^c \\mathbf{A}_{2j} \\mathbf{B}_{j1} &amp; \\sum_{j = 1}^c \\mathbf{A}_{2j} \\mathbf{B}_{j2} &amp; \\cdots &amp; \\sum_{j = 1}^c \\mathbf{A}_{2j} \\mathbf{B}_{jc} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum_{j = 1}^c \\mathbf{A}_{rj} \\mathbf{B}_{j1} &amp; \\sum_{j = 1}^c \\mathbf{A}_{rj} \\mathbf{B}_{j2} &amp; \\cdots &amp; \\sum_{j = 1}^c \\mathbf{A}_{rj} \\mathbf{B}_{jc} \\end{pmatrix} \\end{align*} \\] which can be said in words as \"each block-element (the \\(ij\\)th element (\\(\\mathbf{A} \\mathbf{B}\\))_{ij}) of the block-matrix product \\(\\mathbf{A} \\mathbf{B}\\) is the sum of the \\(i\\)th block-row of \\(\\mathbf{A}\\) and the \\(j\\)th block column of \\(\\mathbf{B}\\) .Notice that if each block was a scalar rather than a block matrix, this would be the usual definition of matrix multiplication (compare equation (7.2) above to (9.2)). Example 9.1 in class 9.3 The column-row matrix product Theorem 9.1 The matrix product \\(\\mathbf{A}\\mathbf{B}\\) of an \\(m \\times n\\) matrix \\(\\mathbf{A} = \\begin{pmatrix} \\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\cdots &amp; \\mathbf{a}_n \\end{pmatrix}\\) with columns \\(\\{\\mathbf{a}_i\\}_{i=1}^n\\) and an \\(n \\times p\\) matrix \\(\\mathbf{B} = \\begin{pmatrix} \\mathbf{b}_1&#39; \\\\ \\mathbf{b}_2&#39; \\\\ \\vdots \\\\ \\mathbf{b}_n&#39; \\end{pmatrix}\\) with rows \\(\\{\\mathbf{b}_i&#39;\\}_{i=1}^n\\) can be written as the column-row expansion below: \\[ \\begin{align*} \\tag{9.2} \\mathbf{A} \\mathbf{B} &amp; = \\begin{pmatrix} \\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\cdots &amp; \\mathbf{a}_n \\end{pmatrix} \\begin{pmatrix} \\mathbf{b}_1&#39; \\\\ \\mathbf{b}_2&#39; \\\\ \\vdots \\\\ \\mathbf{b}_n&#39; \\end{pmatrix} \\\\ &amp; = \\mathbf{a}_1 \\mathbf{b}_1&#39; + \\mathbf{a}_2 \\mathbf{b}_2&#39; + \\cdots + \\mathbf{a}_n \\mathbf{b}_n&#39; \\end{align*} \\] Note: The notation \\(\\mathbf{b}_i&#39;\\) has a transpose because a vector is defined in the vertical orientation (column vector). Therefore, to formally define a row vector, we take a vertical vector of the values in the row and take its transpose to turn the column vector into a row vector. Example 9.2 in class 9.4 Special Block Matrices There are many different forms of block matrices. Two that deserve special mention here include block diagonal matrices and block triangular matrices. Definition 9.1 The matrix \\(\\mathbf{A}\\) is said to be block diagonal if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_1 &amp; \\mathbf{O} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{A}_2 &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{A}_3 &amp; \\cdots &amp; \\mathbf{0} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{A}_n \\\\ \\end{pmatrix} \\end{align*} \\] Definition 9.2 The matrix \\(\\mathbf{A}\\) is said to be block (upper) triangular if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} &amp; \\mathbf{A}_{13} &amp; \\cdots &amp; \\mathbf{A}_{1n} \\\\ \\mathbf{0} &amp; \\mathbf{A}_{22} &amp; \\mathbf{A}_{23} &amp; \\cdots &amp; \\mathbf{A}_{2n} \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{A}_{33} &amp; \\cdots &amp; \\mathbf{A}_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{A}_{nn} \\\\ \\end{pmatrix} \\end{align*} \\] \\(\\mathbf{A}\\) is block (lower) triangular if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{A}_{21} &amp; \\mathbf{A}_{22} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{A}_{31} &amp; \\mathbf{A}_{32} &amp; \\mathbf{A}_{33} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{m1} &amp; \\mathbf{A}_{m2} &amp; \\mathbf{A}_{m3} &amp; \\cdots &amp; \\mathbf{A}_{mn} \\\\ \\end{pmatrix} \\end{align*} \\] Example 9.3 Assume that \\(\\mathbf{A}\\), which has the form \\[ \\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_{11} &amp; \\mathbf{A}_{12} \\\\ \\mathbf{0} &amp; \\mathbf{A}_{22} \\end{pmatrix}, \\] is an invertible matrix with \\(\\mathbf{A}_{11}\\) a \\(p \\times p\\) matrix, \\(\\mathbf{A}_{12}\\) a \\(p \\times q\\) matrix, and $_{22} a \\(q \\times q\\) matrix. Solve for $^{-1} "],["section-matrix-factorizations.html", "Chapter 10 Matrix Factorizations 10.1 The LU factorization 10.2 Obtaining the LU factorization 10.3 The Cholesky factor", " Chapter 10 Matrix Factorizations library(tidyverse) library(dasc2594) library(mvnfast) In scalar mathematics, a factorization is an expression that writes a scalar \\(a\\) as a product of two or more scalars. For example, the scalar 2 has a square-root factorization of \\(2 =\\sqrt{2} * \\sqrt{2}\\) and 15 has a prime factorization of \\(15 = 3 * 5\\). A matrix factorization is a similar concept where a matrix \\(\\mathbf{A}\\) can be represented by a product or two or more matrices (e.g., \\(\\mathbf{A} = \\mathbf{B} \\mathbf{C}\\)). In data science, matrix factorizations are fundamental to working with data. 10.1 The LU factorization First, we define lower and upper triangular matrices. Definition 10.1 The matrix \\(\\mathbf{A}\\) is said to be lower triangular if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} a_{11} &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ a_{21} &amp; a_{22} &amp; 0 &amp; \\cdots &amp; 0 \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\cdots &amp; a_{nn} \\\\ \\end{pmatrix} \\end{align*} \\] Similarly, the matrix \\(\\mathbf{A}\\) is said to be upper triangular if \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\cdots &amp; a_{1n} \\\\ 0 &amp; a_{22} &amp; a_{23} &amp; \\cdots &amp; a_{2n} \\\\ 0 &amp; 0 &amp; a_{33} &amp; \\cdots &amp; a_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; a_{nn} \\\\ \\end{pmatrix} \\end{align*} \\] The LU factorization of a matrix \\(\\mathbf{A}\\) reduces the matrix \\(\\mathbf{A}\\) into two components. The first component \\(\\mathbf{L}\\) is a lower-triangular matrix and the second component \\(\\mathbf{U}\\) is an upper triangular matrix. Using the LU factorization, the matrix factorization \\(\\mathbf{A} = \\mathbf{L} \\mathbf{U}\\) can be used in the matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{L} \\mathbf{U}\\mathbf{x} = \\mathbf{b}\\) by first solving the sub-equation \\(\\mathbf{L} \\mathbf{y} = \\mathbf{b}\\) and then solving the second sub-equation \\(\\mathbf{U} \\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\). Thus, the matrix factorization applied to the matrix equation gives the pair of equations \\[ \\begin{align*} \\tag{10.1} \\mathbf{L} \\mathbf{y} &amp; = \\mathbf{b} \\\\ \\mathbf{U} \\mathbf{x} &amp; = \\mathbf{y} \\end{align*} \\] At first glance, this seems like we are trading the challenge of solving one system of equations \\(\\mathbf{A}\\mathbf{x}\\) (4.1) for the two equations in (10.1). However, the computational benefits arise due to the fact that \\(\\mathbf{L}\\) and \\(\\mathbf{U}\\) are triangular matrices and solving matrix equations with triangular matrices is much faster. Example 10.1 in class: Let \\(\\mathbf{A} = \\begin{pmatrix} -2 &amp; 4 &amp; -6 &amp; 2 \\\\ -2 &amp; 4 &amp; -6 &amp; 2 \\\\ -3 &amp; 0 &amp; -1 &amp; -3 \\\\ -3 &amp; 6 &amp; -5 &amp; -1 \\end{pmatrix}\\) which has the LU decomposition \\[ \\begin{align*} \\mathbf{A} = \\begin{pmatrix} -2 &amp; 4 &amp; -6 &amp; 2 \\\\ -2 &amp; 4 &amp; -6 &amp; 2 \\\\ -3 &amp; 0 &amp; -1 &amp; -3 \\\\ -3 &amp; 6 &amp; -5 &amp; -1 \\end{pmatrix} = \\begin{pmatrix} -2 &amp; 0 &amp; 0 &amp; 0 \\\\ -2 &amp; 0 &amp; 0 &amp; 0 \\\\ -3 &amp; 2 &amp; 2 &amp; 0 \\\\ -3 &amp; 0 &amp; 2 &amp; -1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; -2 &amp; 3 &amp; -1 \\\\ 0 &amp; -3 &amp; 2 &amp; -1 \\\\ 0 &amp; 0 &amp; 2 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\end{align*} \\] solve \\(\\mathbf{L} \\mathbf{y} = \\mathbf{b}\\) using augmented matrix solve \\(\\mathbf{U} \\mathbf{x} = \\mathbf{y}\\) using augmented matrix * compare to the solution \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) Exercise 10.1 in lab: Solve some large systems of equations by brute force which shows how the LU decomposition is faster. 10.1.1 Geometric interpretation of the LU factorization Draw image in class 10.2 Obtaining the LU factorization Notice that the upper-triangular matrix \\(\\mathbf{U}\\) is in echelon form. Congratulations! you know how to construct a matrix \\(\\mathbf{U}\\) by reducing the matrix \\(\\mathbf{A}\\) to an echelon form \\(\\mathbf{U}\\) using elementary matrices \\(\\mathbf{E}_1, \\ldots \\mathbf{E}_k\\). Now, we only need to find the lower triangular matrix \\(\\mathbf{L}\\). Combining the LU factorization and the fact that we can find an upper triangular matrix \\(\\mathbf{U}\\) using elementary row matrices, we have \\[ \\begin{align} \\tag{10.2} \\mathbf{A} &amp; = \\mathbf{L} \\mathbf{U} \\\\ \\mathbf{E}_k \\cdots \\mathbf{E}_1 \\mathbf{A} &amp; = \\mathbf{U}. \\end{align} \\] We also know that each of the elementary row matrices \\(\\mathbf{E}_j\\) are invertible (you can always re-swap rows, subtract instead of add rows, etc.) which says that each inverse \\(\\mathbf{E}_j^{-1}\\) exists. Thus, the product \\(\\mathbf{E}_k \\cdots \\mathbf{E}_1\\) must have an inverse which is \\[ \\begin{align*} (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1} &amp; = \\mathbf{E}_1^{-1} \\cdots \\mathbf{E}_k^{-1}. \\end{align*} \\] Plugging this inverse into (10.2) gives (left multiplying by \\((\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\) on both sides) \\[ \\begin{align*} (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1} (\\mathbf{E}_k \\cdots \\mathbf{E}_1) \\mathbf{A} &amp; = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\mathbf{U} \\\\ \\mathbf{A} &amp; = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\mathbf{U} \\\\ &amp; = \\mathbf{L} \\mathbf{U} \\end{align*} \\] where \\(\\mathbf{L} = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\) Algorithm for finding the LU decomposition Given the matrix \\(\\mathbf{A}\\) Find elementary matrices \\(\\mathbf{E}_1, \\ldots, \\mathbf{E}_k\\) such that \\(\\mathbf{E}_k \\cdots \\mathbf{E}_1 \\mathbf{A}\\) is in row echelon form (if this is possible, otherwise an LU factorization does not exist). Call this matrix \\(\\mathbf{U}\\), the upper triangular component of the LU factorization. The, the lower triangular \\(\\mathbf{L} = (\\mathbf{E}_k \\cdots \\mathbf{E}_1)^{-1}\\). Notice that the algorithm does not say to find a specific matrix \\(\\mathbf{U}\\). In general, any row echelon form matrix \\(\\mathbf{U}\\) will work. 10.3 The Cholesky factor A Cholesky decomposition is special type of LU decomposition. A Cholesky decomposition is an LU decomposition on a symmetric, positive-definite square matrix. Definition 10.2 * A matrix \\(\\mathbf{A}\\) is said to by symmetric if \\(\\mathbf{A} = \\mathbf{A}&#39;\\) A \\(n \\times n\\) matrix is said to be positive definite if for all \\(\\mathbf{x} \\in \\mathcal{R}^n\\), the quadratic form \\(\\mathbf{x}&#39; \\mathbf{A }\\mathbf{x} \\geq 0\\) Note: the condition of positive definiteness is actually impossible to check. Can you show this is true for all vectors? Luckily, a \\(n \\times n\\) symmetric matrix is positive definite if and only if the matrix \\(\\mathbf{A}\\) is invertible (which we know about by the invertible matrix theorem 8.5). Definition 10.3 Let \\(\\mathbf{A}\\) be a symmetric, positive definite matrix (by this, \\(\\mathbf{A}\\) is a \\(n \\times n\\) square matrix). Then \\[ \\begin{align*} \\mathbf{A} = \\mathbf{L} \\mathbf{L}&#39; \\end{align*} \\] is the Cholesky decomposition of \\(\\mathbf{A}\\) if \\(\\mathbf{L}\\) is a lower-triangular matrix. Also, the lower triangular Cholesky matrix \\(\\mathbf{L}\\) is unique. What makes the Cholesky factor special? The decomposition \\(\\mathbf{A} = \\mathbf{L} \\mathbf{U}\\) has the property that \\(\\mathbf{U} = \\mathbf{L}&#39;\\) so that the computer only has to store one of the matrix components (reduce memory demands). As about half of the elements of \\(\\mathbf{L}\\) are 0, matrix multiplication is much less computationally demanding as about half of the flops are not required to be evaluated (x * 0 = 0). The Cholesky factor is unique. There is only one Cholesky factor for each symmetric positive definite matrix. The Cholesky has properties related to multivariate normal distributions. Let \\(\\mathbf{y} \\sim \\operatorname{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})\\), and \\(\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}&#39;\\). Then, if \\(\\mathbf{z} \\sim \\operatorname{N}(\\mathbf{0}, \\mathbf{I})\\), then \\(\\mathbf{L} \\mathbf{z} \\sim \\operatorname{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})\\). We say the \\(\\mathbf{y}\\) and \\(\\mathbf{L}\\mathbf{z}\\) are equal in distribution. # simulate N 2-dimensional random normal vectors N &lt;- 5000 mu &lt;- rep(0, 2) Sigma &lt;- matrix(c(2, 1.5, 1.5, 2), 2, 2) y &lt;- rmvn(N, mu, Sigma) # calculate the Cholesky factor L &lt;- t(chol(Sigma)) # R calculates the upper (right) Cholesky factor by default z &lt;- rmvn(N, mu, diag(2)) Lz &lt;- t(L %*% t(z)) # pay attention to the dimensions of L and z here... data.frame( observation = 1:N, x1 = c(y[, 1], z[, 1], Lz[, 1]), x2 = c(y[, 2], z[, 2], Lz[, 2]), variable = factor(rep(c(&quot;y&quot;, &quot;z&quot;, &quot;Lz&quot;), each = N), levels = c(&quot;y&quot;, &quot;z&quot;, &quot;Lz&quot;)) ) %&gt;% ggplot(aes(x = x1, y = x2, color = variable)) + geom_point(alpha = 0.1) + geom_density2d() + facet_wrap(~ variable) "],["section-subspaces-Rn.html", "Chapter 11 Subspaces of \\(\\mathcal{R}^n\\) 11.1 Special subspaces: column space and null space 11.2 The basis of a subspace", " Chapter 11 Subspaces of \\(\\mathcal{R}^n\\) First, let’s recall the definition of a subset. A set \\(A\\) is a subset of a set \\(B\\) if all elements of \\(A\\) are also members of \\(B\\). For example, the integers \\(\\mathcal{Z}\\) are a subset of the real numbers \\(\\mathbf{R}\\) (\\(\\mathcal{Z} \\subset \\mathcal{R}\\)) and the real numbers are a subset of the complex numbers \\(\\mathcal{C}\\) (\\(\\mathcal{R} \\subset \\mathcal{C}\\)). Subspaces are a generalization of the idea of subsets that are useful for understanding vector spaces. Definition 11.1 A subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) is a set that has the properties The zero vector \\(\\mathbf{0} \\in \\mathcal{H}\\) For each \\(\\mathbf{u}, \\mathbf{v} \\in \\mathcal{H}\\), the sum \\(\\mathbf{u} + \\mathbf{v}\\) is in \\(\\mathcal{H}\\) For each \\(\\mathbf{u} \\in \\mathcal{H}\\) and scalar \\(c\\), the scalar multiple \\(c \\mathbf{u}\\) is in \\(\\mathcal{H}\\) Example 11.1 Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors in \\(\\mathcal{R}^n\\). Then the vector space defined by span\\(\\{\\mathbf{u}, \\mathbf{v} \\}\\) is a subspace of \\(\\mathcal{R}^n\\) Show this in class. Exercise 11.1 * Is a line through the origin a subspace? * Is a line not through the origin a subspace? Note: For any vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_k \\in \\mathcal{R}^n\\), the span\\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}\\) is a subspace of \\(\\mathcal{R}^n\\). 11.1 Special subspaces: column space and null space Definition 11.2 The column space, denoted \\(\\operatorname{col}(\\mathbf{A})\\), of a \\(m \\times n\\) matrix \\(\\mathbf{A}\\) which has columns \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_n \\in \\mathcal{R}^m\\) is the set of vectors that are linear combinations of the columns of \\(\\mathbf{A}\\) which is equivalent to the span\\(\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}\\). Example 11.2 in class Definition 11.3 The null space, denoted \\(\\operatorname{null}(\\mathbf{A})\\), of a matrix \\(\\mathbf{A}\\) is the set of all solutions to the homogeneous matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\). While the idea of a null space seems unclear, the null space is the set of all vectors which the matrix transformation defined by \\(\\mathbf{A}\\) maps to \\(\\mathbf{0}\\). Theorem 11.1 The null space of a n \\(m \\times m\\) matrix \\(\\mathbf{A}\\) is a subspace of \\(\\mathcal{R}^n\\). Proof. Do in class Example: give \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) and determine if \\(\\mathbf{x}\\) is in the null space of \\(\\mathbf{A}\\) using R 11.2 The basis of a subspace Definition 11.4 A basis for a subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) is a linearly independent set in \\(\\mathcal{H}\\) that spans \\(\\mathbf{H}\\). Equivlently, a baisis is a set of linearly independent vectors \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\) such that span\\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\} = \\mathcal{H}\\). The requirement that the vectors of a basis are linearly independent while spanning a subspace \\(\\mathcal{H}\\) means that a basis is a minimal spanning set for the subspace \\(\\mathcal{H}\\) Question: Is a basis unique? Definition 11.5 The standard basis for \\(\\mathcal{R}^n\\) Example 11.3 Basis for \\(\\mathcal{R}^3\\) Example 11.4 Do the following set of vectors form a basis for \\(\\mathcal{R}^3\\)? in class Example 11.5 Using R, find a basis for the null space of the matrix \\[ \\mathbf{A} = \\begin{pmatrix} 2 &amp; 4 &amp; 1 &amp; 3 \\\\ -1 &amp; -2 &amp; 6 &amp; 5 \\\\ 1 &amp; 2 &amp; -3 &amp; 2 \\end{pmatrix} \\] Theorem 11.2 The pivot columns of a matrix \\(\\mathbf{A}\\) for a basis for the column space of \\(\\mathbf{A}\\). Note: Use the columns of \\(\\mathbf{A}\\), not the columns of the matrix in echelon form. corollary Corollary 11.1 Example 11.6 Using R, find a basis for the column space of the matrix \\[ \\mathbf{A} = \\begin{pmatrix} 3 &amp; 1 &amp; 2 &amp; -3 \\\\ 4 &amp; 1 &amp; -3 &amp; -2 \\\\ 4 &amp; -1 &amp; -3 &amp; 1 \\end{pmatrix} \\] "],["section-dimension-and-rank.html", "Chapter 12 Dimension and Rank 12.1 Coordinate systems 12.2 Dimension of a subspace", " Chapter 12 Dimension and Rank 12.1 Coordinate systems Recall the idea of polynomials (e.g., a polynomial of order \\(p\\) is \\(a_1x^p + a_2x^{p-1} + \\ldots + a_p x^1 + a_{p+1} x^0\\)) where the polynomials \\(x^p, x^{p-1}, \\ldots, x^1, x^0\\) form a set of powers up to the power \\(p\\) of \\(x\\) from which the coefficients \\(a_p, \\ldots, a_{p+1}\\) can be used to make any polynomial of order \\(p\\). It can be said that the powers of \\(x\\) (\\(x^p, x^{p-1}, \\ldots, x^1, x^0\\)) form a basis for all polynomials of order \\(p\\). In the previous section, we extended this analogy to vector spaces using the concept of a minimal spanning set. Consider the basis \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) for a subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) where span\\(\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\} = \\mathcal{H}\\). Because the set \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) is a basis, the set of vectors is linearly independent. Then, because the set \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) is a basis, we have the following result. Theorem 12.1 For each vector \\(\\mathbf{x}\\) in the subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\), and a basis \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\), there is a unique set of coefficients \\(a_1, \\ldots, a_k\\) such that \\[\\begin{align*} \\mathbf{x} &amp; = a_1 \\mathbf{b}_1 + \\cdots + a_k \\mathbf{b}_k \\end{align*}\\] Proof. In class: assume contradiction that there are two ways \\(a_1, \\ldots, a_k\\) and \\(c_1, \\ldots, c_k\\)… Definition 12.1 Let \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\}\\) be a basis for a subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\). Then, for each \\(\\mathbf{x} \\in \\mathcal{H}\\), the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\) are the set of coefficients \\(\\{a_1, \\ldots, a_k\\}\\) where \\[\\begin{align*} \\mathbf{x} &amp; = a_1 \\mathbf{b}_1 + \\cdots + a_k \\mathbf{b}_k. \\end{align*}\\] Example 12.1 Let \\(\\mathcal{B} = \\left\\{ \\mathbf{b}_1 = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1\\end{pmatrix}, \\mathbf{b}_2 = \\begin{pmatrix} 2 \\\\ -3 \\\\ 1\\end{pmatrix} \\right\\}\\) and \\(\\mathbf{x} = \\begin{pmatrix} 5 \\\\ 6 \\\\ 1\\end{pmatrix}\\). What are the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\)? Solution. In class: write out vector equation with coefficients, put in augmented matrix form, find consistent solution 12.2 Dimension of a subspace Definition 12.2 The dimension \\(\\operatorname{dim}(\\mathcal{H})\\) of a nonzero subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^n\\) is the number of (nonzero) vectors that make up a basis \\(\\mathcal{B}\\) for \\(\\mathcal{H}\\). The dimension of the subspace \\(\\mathcal{H} = \\{\\mathbf{0}\\}\\) is 0. Note that under this definition, the basis \\(\\mathcal{B}\\) is not unique. For example, the following bases for the 2-dimensional subspace \\(\\mathcal{H}\\) of \\(\\mathcal{R}^3\\) both have two linearly independent vectors. \\[\\begin{align*} \\mathcal{B}_1 = \\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\} &amp;&amp; \\mathcal{B}_2 = \\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\} \\end{align*}\\] For example, let \\(\\mathbf{x} = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix}\\). Then under the basis \\(\\mathcal{B}_1\\), the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}_1\\) are \\(a_1 = 3\\) and \\(a_2 = 4\\) because \\[\\begin{align*} \\mathbf{x} = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + 4 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\end{align*}\\] while the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}_2\\) are \\(a_1 = 3\\) and \\(a_2 = 1\\) because \\[\\begin{align*} \\mathbf{x} = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}. \\end{align*}\\] Exercise 12.1 What is the dimension of a basis for \\(\\mathcal{R}^n\\)? Exercise 12.2 What is the dimension of a subspace that is a plane in 3 dimensions? Definition 12.3 The rank \\(\\operatorname{rank}(\\mathbf{A})\\) of a matrix \\(\\mathbf{A}\\) is the dimension of the column space of \\(\\mathcal{A}\\). Recall that the pivot columns of \\(\\mathbf{A}\\) form a basis for the column space of \\(\\mathbf{A}\\). Hence, the number of pivot columns in the matrix \\(\\mathbf{A}\\) is the rank of the matrix \\(\\mathbf{A}\\). Example 12.2 Determine the rank of the following matrix: in class example Theorem 12.2 (The Rank Theorem) If a matrix \\(\\mathbf{A}\\) has \\(n\\) columns, then \\(\\operatorname{rank}(\\mathbf{A}) + \\operatorname{dim}(\\operatorname{null}(\\mathbf{A})) = n\\) Proof. in class: sketch– rank(A) is number of linearly independent columns. null(A) is number of linearly dependent columns (solutions to Ax=0) The following theorem states that any \\(p\\) vectors in \\(\\mathcal{R}^p\\) that are linearly independent must span \\(\\mathcal{R}^p\\). Theorem 12.3 (The Basis Theorem) Let \\(\\mathcal{H}\\) be a p-dimensional subspace of \\(\\mathcal{R}^n\\). Then any linearly independent set of \\(p\\) elements in \\(\\mathcal{H}\\) is a basis for \\(\\mathcal{H}\\). Equivalently, any set of \\(p\\) elements of \\(\\mathcal{H}\\) that span \\(\\mathcal{H}\\) is a basis for \\(\\mathcal{H}\\) Theorem 12.4 (Invertible Matrix Theorem (again)) Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) matrix. The the following statements are equivalent to \\(\\mathbf{A}\\) being an invertible matrix: The columns of \\(\\mathbf{A}\\) form a basis for \\(\\mathcal{R}^n\\) \\(\\operatorname{col}(\\mathbf{A}) = \\mathcal{R}^n\\) \\(\\operatorname{dim}(\\operatorname{col}(\\mathbf{A})) = n\\) \\(\\operatorname{rank}(\\mathbf{A}) = n\\) \\(\\operatorname{null}(\\mathbf{A}) = \\{\\mathbf{0}\\}\\) \\(\\operatorname{dim}(\\operatorname{null}(\\mathbf{A})) = 0\\) "],["section-determinants.html", "Chapter 13 Determinants 13.1 Determinants of \\(2 \\times 2\\) matrices 13.2 Determinants of \\(n \\times n\\) matrices 13.3 Properties of determinants 13.4 Cramer’s Rule and Determinants", " Chapter 13 Determinants library(tidyverse) library(dasc2594) The determinant \\(\\operatorname{det}(\\mathbf{A})\\) of a square \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is a real number. Definition 13.1 The determinant is a fuction of a square \\(n \\times n\\) matrix that satisfies the following properties If a scalar multiple of one row of \\(\\mathbf{A}\\) is added to another row of \\(\\mathbf{A}\\), then the determinant \\(\\operatorname{det}(\\mathbf{A})\\) is unchanged. Scaling a row of \\(\\mathbf{A}\\) by a constant \\(c\\) multiples the determinant by \\(c\\) Swapping two rows of a matrix \\(\\mathbf{A}\\) multiplies the determinant by -1 The determinant of the \\(n \\times n\\) identity matrix \\(\\mathbf{I}\\) is equal to 1 The determinant is the unique function mapping square matrices to the real number line that satisfies the above definition. Example 13.1 Let \\(\\mathbf{A} = \\begin{pmatrix} 5 &amp; 3 \\\\ 1 &amp; -3 \\end{pmatrix}\\). Then, we can calculate the determinant \\(\\operatorname{det}(\\mathbf{A})\\) using row operations to get to reduced row echelon form. \\[\\begin{align*} &amp;&amp; \\begin{pmatrix} 5 &amp; 3 \\\\ 1 &amp; -3 \\end{pmatrix} &amp;&amp; \\operatorname{det} = x \\\\ \\mbox{swap row 1 and row 2} &amp;&amp; \\begin{pmatrix} 1 &amp; -3 \\\\ 5 &amp; 3 \\end{pmatrix} &amp;&amp; \\operatorname{det} = -x \\\\ \\mbox{row 2 = -5 * row 1 + row 2} &amp;&amp; \\begin{pmatrix} 1 &amp; -3 \\\\ 0 &amp; 18 \\end{pmatrix} &amp;&amp; \\operatorname{det} = -x \\\\ \\mbox{row 2 \\div 18} &amp;&amp; \\begin{pmatrix} 1 &amp; -3 \\\\ 0 &amp; 1 \\end{pmatrix} &amp;&amp; \\operatorname{det} = -\\frac{x}{18} \\\\ \\mbox{row 1 = row 1 + 3 * row 2} &amp;&amp; \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} &amp;&amp; \\operatorname{det} = -\\frac{x}{18} \\end{align*}\\] where the last matrix is in reduced row echelon form and is the identity matrix which has determinant \\(\\operatorname{det}(\\mathbf{I}) = 1\\). Therefore \\(-\\frac{x}{18} = 1\\) which implies that \\(x = -18\\) so that \\(\\operatorname{det}(\\mathbf{A}) = -18\\) Let’s check our answer in R A &lt;- matrix(c(5, 1, 3, -3), 2, 2) det(A) ## [1] -18 Example 13.2 In class 13.1 Determinants of \\(2 \\times 2\\) matrices Definition 13.2 If \\(\\mathbf{A} = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\) is a \\(2 \\times 2\\) matrix, the determinant \\(\\operatorname{det}(\\mathbf{A}) = ad - bc\\) Example 13.3 Let \\(\\mathbf{A} = \\begin{pmatrix} 5 &amp; 3 \\\\ 1 &amp; -3 \\end{pmatrix}\\), then \\(\\det(\\mathbf{A}) = (5 * -3) - (3 * 1) = -18\\) Example 13.4 Let \\(\\mathbf{A} = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\). Then If \\(a\\) = 0 \\(a \\neq 0\\) show steps in class 13.2 Determinants of \\(n \\times n\\) matrices To better understand determinants of \\(n \\times n\\) matrices, we need to define the two concepts of a matrix minor and cofactor. Definition 13.3 For an \\(n \\times n\\) matrix \\(\\mathbf{A}\\), The (i, j) minor \\(\\mathbf{A}_{-i-j}\\) is the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)th row and the \\(j\\) column from \\(\\mathbf{A}\\) The (i, j) cofactor \\(c_{ij}\\) is defined using the determinant of the minor where \\[\\begin{align*} c_{ij} = \\mathbf(-1)^{i + j} \\det{\\mathbf{A}_{-i-j}} \\end{align*}\\] Note: The cofactor of a scalar \\(a\\) (a \\(1 \\times 1\\) matrix) is defined as \\(\\mathbf{C}_{ij} = (-1)^{1 + 1} \\det(a) = a\\). Note: The leading term in the cofactor definition \\(\\mathbf(-1)^{i + j}\\) defines a checkerboard pattern shown below \\[\\begin{align*} \\begin{pmatrix} + &amp; - &amp; + &amp; - &amp; \\cdots \\\\ - &amp; + &amp; - &amp; + &amp; \\cdots \\\\ + &amp; - &amp; + &amp; - &amp; \\cdots \\\\ - &amp; + &amp; - &amp; + &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\\\ + &amp; - &amp; + &amp; - &amp; \\cdots \\\\ \\end{pmatrix} \\end{align*}\\] Example 13.5 Let \\(\\mathbf{A}\\) be the \\(3 \\times 3\\) matrix… (in class) Find the minor \\(\\mathbf{A}_{-2-3}\\) Find the cofactor \\(c_{23}\\) Note that in the cofactor definition of a \\(n \\times n\\) matrix it is assumed that you can calculate the determinant of the \\(n-1 \\times n-1\\) minor \\(\\mathbf{A}_{-i-j}\\). From this we see that each of the \\(n \\times n\\) cofactors of \\(\\mathbf{A}\\) are themselves the (signed) determinants of \\(n-1 \\times n-1\\) submatrices (the matrix minors). Thus, solving for all cofactors in general requires a recursive definition where smaller and smaller submatrices are evaluated. Theorem 13.1 (Cofactor exapansion) Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix with \\(ij\\)th elements \\(a_{ij}\\). Then The cofactor expansion along the \\(i\\)th row (for any fixed row \\(i\\)) is \\[\\begin{align*} \\det(\\mathbf{A}) = \\sum_{j=1}^n a_{ij} c_{ij} = a_{i1} c_{i1} + a_{i2} c_{i2} + \\cdots + a_{in} c_{in} \\end{align*}\\] The cofactor expansion along the \\(j\\)th column (for any fixed column \\(j\\)) is \\[\\begin{align*} \\det(\\mathbf{A}) = \\sum_{i=1}^n a_{ij} c_{ij} = a_{1j} c_{1j} + a_{2j} c_{2j} + \\cdots + a_{nj} c_{nj} \\end{align*}\\] Proof. This is quite complex. For those interested, an example is available here The above theorem states that there are actually \\(2n\\) ways to calcuate the determinant–one for each row and column of \\(\\mathbf{A}\\). Example 13.6 in class: Use the minor/cofactor defintion to calculate the determinant of tha \\(2 \\times 2\\) matrix Example 13.7 in class: Use the minor/cofactor defintion to calculate the determinant of tha \\(3 \\times 3\\) matrix Example 13.8 in class: determinant of a matrix with a row (or column) of zeros Example 13.9 in class: determinant of a matrix where at least one row is a linear combination of the other rows Theorem 13.2 in class: determinant of a matrix is equal to the determinant of its transpose Proof. Follows directly from cofactor expansion theorem 13.3 Properties of determinants Theorem 13.3 A \\(n \\times n\\) square matrix \\(\\mathbf{A}\\) is invertible if and only if \\(\\det(\\mathbf{A}) \\neq 0\\) Proof. From the invertible matrix theorem link to this in notes, we know that the matrix \\(\\mathbf{A}\\) is invertible if and only if every column of \\(\\mathbf{A}\\) is a pivot column. Therefore, each column is linearly independent from the other columns. Based on the example above, if the rows were not linearly independent, the determinant would be equal to 0 (as row operations could create a row/column of all zeros). Thus, if the determinant is not 0, the columns of \\(\\mathbf{A}\\) are linearly independent and the matrix is invertible. Theorem 13.4 If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are \\(n \\times n\\) matrices, \\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix, and \\(c\\) is a scalar, we have \\(\\det(\\mathbf{I}) = 1\\) \\(\\det(\\mathbf{A}) = \\det(\\mathbf{A}&#39;)\\) \\(\\det(\\mathbf{A}^{-1}) = 1 / \\det(\\mathbf{A})\\) if \\(\\det(\\mathbf{A}) \\neq 0\\) (\\(\\mathbf{A}\\) is invertible) \\(\\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B})\\) \\(\\det(c\\mathbf{A}) = c^n \\det(\\mathbf{A})\\) 13.4 Cramer’s Rule and Determinants While commonly used for theoretical results, Cramer’s rule is not commonly used in applied linear algebra. As such, we will mention Cramer’s rule but not focus on it. Theorem 13.5 (Cramer’s Rule) Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) invertible matrix. Define \\(\\mathbf{A}_i(\\mathbf{b})\\) as the matrix \\(\\mathbf{A}\\) with the \\(i\\)th column replace by the vector \\(\\mathbf{b}\\). For example, \\(\\mathbf{A}_i(\\mathbf{b}) = \\begin{pmatrix} \\mathbf{a}_1 &amp; \\cdots &amp; \\mathbf{a}_{i-1} &amp; \\mathbf{b} &amp; \\mathbf{a}_{i+1} &amp; \\cdots &amp; \\mathbf{a}_n \\end{pmatrix}\\). Then, for any \\(\\mathbf{b} \\in \\mathcal{R}^n\\), the unique solution to \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has entries given by \\[\\begin{align*} x_i = \\frac{\\det(\\mathbf{A}_i(\\mathbf{b}))}{\\det(\\mathbf{A})} &amp; \\mbox{ for } i = 1, 2, \\ldots, n \\end{align*}\\] Example 13.10 In Cramer’s rule, why do we know the solution is unique for any \\(\\mathbf{b}\\)? the determinant \\(\\det(\\mathbf{A}) \\neq 0\\)? "],["section-determinants-and-volumes.html", "Chapter 14 Determinants and volumes 14.1 Volumes of Parallelpipeds 14.2 Volumes of Linear Transformations", " Chapter 14 Determinants and volumes library(tidyverse) library(dasc2594) Definition 14.1 The parallelpiped defined by \\(n\\) vectors \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n \\in \\mathcal{R}^n\\) with coefficients \\(x_1, x_2, \\ldots, x_n\\) is the subset \\[\\begin{align*} \\mathcal{P} = \\{x_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n | 0 \\leq x_1, x_2, \\ldots, x_n \\leq 1 \\} \\end{align*}\\] The determinant is a function that takes the vectors \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\) that make up the columns of \\(\\mathbf{A}\\) and returns the volume of the parallelpiped \\(\\mathcal{P}\\) defined above add link Example 14.1 The unit cube: in class–use standard vectors \\(\\mathbf{e}_1, \\mathbf{e}_2\\), and \\(\\mathbf{e}_3\\) Example 14.2 parallelograms in \\(\\mathcal{R}^2\\): the unit square df_vector &lt;- data.frame(x = c(1, 0), y = c(0, 1)) df_polygon &lt;- data.frame(x = c(0, 1, 1, 0), y = c(0, 0, 1, 1)) p1 &lt;- ggplot() + geom_segment(aes(x = 0, xend = df_vector$x[1], y = 0, yend = df_vector$y[1]), arrow = arrow()) + geom_segment(aes(x = 0, xend = df_vector$x[2], y = 0, yend = df_vector$y[2]), arrow = arrow()) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_polygon(data = df_polygon, aes(x = x, y = y), fill = &quot;grey&quot;, alpha = 0.5) + ggtitle(&quot;Area of unit cube&quot;) p1 Which implies that if \\(\\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\) has \\(\\det(\\mathbf{A}) = 1\\) because \\(\\mathbf{A} = \\mathbf{I}\\) the identity matrix. det(matrix(c(1, 0, 0, 1), 2, 2)) ## [1] 1 Example 14.3 parallelograms in \\(\\mathcal{R}^2\\): A larger square df_vector &lt;- data.frame(x = c(2, 0), y = c(0, 2)) df_polygon &lt;- data.frame(x = c(0, 2, 2, 0), y = c(0, 0, 2, 2)) p1 &lt;- ggplot() + geom_segment(aes(x = 0, xend = df_vector$x[1], y = 0, yend = df_vector$y[1]), arrow = arrow()) + geom_segment(aes(x = 0, xend = df_vector$x[2], y = 0, yend = df_vector$y[2]), arrow = arrow()) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + geom_polygon(data = df_polygon, aes(x = x, y = y), fill = &quot;grey&quot;, alpha = 0.5) + ggtitle(&quot;Area of unit cube&quot;) p1 Which implies that if \\(\\mathbf{A} = \\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix}\\) has \\(\\det(\\mathbf{A}) = 4\\) because \\(\\mathbf{A} = 2 \\mathbf{I}\\) and the rule is for a constant \\(c, \\det(c\\mathbf{A}) = c^n \\det(\\mathbf{A})\\) det(matrix(c(2, 0, 0, 2), 2, 2)) ## [1] 4 The Shiny app below allows you to plot the vector for any \\((x, y)\\) pair of your choosing. library(shiny) runGitHub(rep = &quot;multivariable-math&quot;, username = &quot;jtipton25&quot;, subdir = &quot;shiny-apps/chapter-14/determinants-volume/&quot;) Theorem 14.1 (Determinants and Volume) Let \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\) be vectors in \\(\\mathcal{R}^n\\), let \\(\\mathcal{P}\\) be the parallelpiped determined by these vectors, and let \\(\\mathbf{A}\\) be the matrix with columns \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\). Then, the absolute value of the determinant of \\(\\mathbf{A}\\) is the volume of the parallelpiped \\(\\mathcal{P}\\): \\[\\begin{align*} |\\det(\\mathbf{A})| = \\operatorname{volume}(\\mathcal{P}) \\end{align*}\\] Proof. Recall the 4 defining properties (Definition 13.1) that characterize the determinant. These properties also characterize the absolute value of the determinant. From the 4 defining properties, the absolute value of the determinant \\(|\\det|\\) is a function on square \\(n \\times n\\) matrices that satisfies the properties Row replacement (e.g., row i = row i + c * row j) of \\(\\mathbf{A}\\) does not change \\(|\\det(\\mathbf{A})|\\) Scaling a row of \\(\\mathbf{A}\\) by a scalar \\(c\\) changes \\(|\\det(\\mathbf{A})|\\) by multiplication by \\(|c|\\) Swapping two rows of \\(\\mathbf{A}\\) does not change \\(|\\det(\\mathbf{A})|\\) The determinant of the identity matrix \\(\\mathbf{I}\\) is 1 Like the determinant and its 4 defining characteristics (Definition 13.1), the absolute value of the determinant is the only function that satisfies this relationship. Define \\(vol(\\mathcal{P}_A)\\) as the volume of the parallelpiped defined by the rows of the square matrix \\(\\mathbf{A}\\). In what follows, we will show that \\(vol(\\mathcal{P}_A)\\) also satisfies the 4 defining characteristics of the absolute value of the determinant and, by the uniqueness of the function of the absolute value of the determinant, is equivalent to the absolute value of the determinant. c) We start with showing that row swaps have no impact of the volume of the parallelpiped. Swapping two rows of \\(\\mathbf{A}\\) just reorders the vectors \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\) and the order has no impact of the calculation of the volume (e.g., area in 2d = length * width = width * length) a) Consider a row replacement of \\(\\mathbf{a}_i \\leftarrow \\mathbf{a}_i + c \\mathbf{a}_j\\) for some \\(j \\neq i\\). Because reordering has no effect on the volume of the parallelpiped, assume WLOG (without loss of generality–fancy math speak for this one case works for all the other possible cases) that we are replacing the last row (\\(\\mathbf{a}_n \\leftarrow \\mathbf{a}_n + c \\mathbf{a}_j\\)). Then, the area of the parallelpiped is defined as the base times the height. Let the “base” be the set of vectors \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots \\mathbf{a}_{n-1}\\) which are the same for the original matrix and the row-replaced matrix. Therefore, if there is a difference in volume of the parallelpiped, it must be due to a difference in height. By definition, \\(\\mathbf{a}_j \\in \\operatorname{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_{n-1}\\}\\) so \\(c \\mathbf{a}_j\\) is a vector that points in the same direction as the “base” which implies that translation of \\(\\mathbf{a}_n\\) by \\(c \\mathbf{a}_j\\) is parallel to the “base”. As this is a parallel translation, the distance from the “base” to \\(\\mathbf{a}_n\\) must be equal to the distance from the “base” to \\(\\mathbf{a}_n + c \\mathbf{a}_j\\) (the definition of parallel) which means the height is unchanged and therefore the volume of \\(vol(\\mathcal{P}_A)\\) is unchanged by row replacement. Draw an example here b) WLOG assume we are scaling the last row \\(\\mathbf{a}_n\\) (we can always swap rows without changing volume so this is ok). Scaling \\(\\mathbf{a}_n\\) by a scalar \\(c\\) leaves the “base” of the parallelpiped unchanged (the “base” is defined as \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_{n-1}\\) which are unchanged). Therefore, the only question is whether the height from the base is changed when scaling \\(\\mathbf{a}_n\\) by \\(|c|\\). In fact, scaling the vector \\(\\mathbf{a}_n\\) by \\(|c|\\) changes the height by \\(|c|\\) and therefore the volume \\(vol(\\mathcal{P}_A)\\) is scaled by \\(|c|\\). Draw an example in class d) The identity matrix having volume 1 is easy. The vectors of the identity matrix \\(\\mathbf{I}\\) define a unit cube (technically a hypercube) which has area equal to the product of the lengths of each of their sides, which are each 1. Because the absolute value of the determinant \\(|\\det|\\) is the only function that satisfies, these properties, we have \\[\\begin{align*} vol(\\mathcal{P}_A) = |\\det(\\mathbf{A})| \\end{align*}\\] Note: Because \\(\\det(\\mathbf{A}) = \\det(\\mathbf{A}&#39;)\\), the absolute value of the determinant is equal to the volume of the parallelpiped defined by the columns of \\(\\mathbf{A}\\) (we could just have easily done all the calculations on the columns of \\(\\mathbf{A}\\) as the rows of \\(\\mathbf{A}\\)). Example 14.4 1 by 1 matrix \\(a\\) (length) has \\(\\det(a) = a\\) Example 14.5 2 by 2 matrix \\(a b c d\\) in class Example 14.6 Find the area of a parallelogram with sides defined by the vectors \\(\\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix}\\) and \\(\\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}\\) Example 14.7 area of a triangle – choose two sides and find area of parallelogram and divide by 2 Example 14.8 Recall that in data science, a probability distribution is function that has volume under the surface of one. For common distributions, particularly the normal/Gaussian distribution, the determinant is the factor that scales the function so that the volume under the surface is one. The vector \\(\\mathbf{y}\\) is said to have a multivariate normal distribution with mean \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\) if the probability density function of \\(\\mathbf{y}\\) is \\[\\begin{align*} f(\\mathbf{y}) = (2 \\pi)^{-n/2} |\\det(\\boldsymbol{\\Sigma})|^{-1/2} e^{- \\frac{1}{2} (\\mathbf{y} - \\boldsymbol{\\mu})&#39; \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu})} \\end{align*}\\] Notice in this equation that the determinant \\(\\det(\\boldsymbol{\\Sigma})\\) plays a key role in the definition of the probability distribution. This is because \\[\\begin{align*} \\int_{\\mathbf{y}} (2 \\pi)^{-n/2} e^{- \\frac{1}{2} (\\mathbf{y} - \\boldsymbol{\\mu})&#39; \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu})} \\, d\\mathbf{y} = |\\det(\\boldsymbol{\\Sigma})|^{-1/2} \\end{align*}\\] which implies that \\[\\begin{align*} \\frac{\\int_{\\mathbf{y}} (2 \\pi)^{-n/2} e^{- \\frac{1}{2} (\\mathbf{y} - \\boldsymbol{\\mu})&#39; \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu})} \\, d\\mathbf{y}}{|\\det(\\boldsymbol{\\Sigma})|^{-1/2}} = 1 \\end{align*}\\] In probability and statistics, the denominator is known as the “normalizing constant.” 14.1 Volumes of Parallelpipeds Proposition 14.1 Let \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\) be nonzero vectors. Then, for any scalar \\(c\\), the area of the parallelpiped defined by \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\) is the same as the area of the parallelpiped defined by the vectors \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2 + c \\mathbf{a}_1\\) (an elementary column operation). Example 14.9 Draw a parallelpiped in class. recall that areas of a parallelpiped are defined (in 2 dimensions) as the length of the base times the height perpindicular to the base. In 3 dimensions, the volume of a parallelpiped is the base times the width (the area of the base) times the height. 14.2 Volumes of Linear Transformations Recall linear transformations \\(T:\\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) (Section 6) where for any \\(\\mathbf{x} \\in \\mathcal{R}^n\\) (the domain), \\(T(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} \\in \\mathcal{R}^n\\) (the codomain). Theorem 14.2 Let \\(\\mathcal{S}\\) be a set in the domain that has a volume \\(vol(\\mathcal{S})\\). Then, the volume of the image of the set under the transformation \\(T(\\mathcal{S})\\) is \\(vol(T(\\mathcal{S})) = |\\det(\\mathbf{A})|vol(\\mathcal{S})\\) "],["section-vector-spaces-and-subspaces.html", "Chapter 15 Vector Spaces and Subspaces 15.1 Null space and column space", " Chapter 15 Vector Spaces and Subspaces Recall the definition of a subspace: Definition 15.1 A subspace \\(\\mathcal{H}\\) of a vector space \\(\\mathcal{V}\\) is a subset of \\(\\mathcal{V}\\) such that \\(\\mathcal{H}\\) contains the zero vector – \\(\\mathbf{0} \\in \\mathcal{H}\\) \\(\\mathcal{H}\\) is closed under vector addition. Therefore, for \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathcal{H}\\), the sum \\(\\mathbf{u} + \\mathbf{v}\\) is in \\(\\mathcal{H}\\) \\(\\mathcal{H}\\) is closed under scalar multiplication. Therefore, for \\(\\mathbf{u}\\) in \\(\\mathcal{H}\\) and a scalar \\(a\\), the product \\(a \\mathbf{u}\\) is in \\(\\mathcal{H}\\) A consequence of this definition is that a subspace \\(\\mathcal{H}\\) is closed under linear combinations. 15.1 Null space and column space Also, recall the special subspaces of the column space and the null space. 15.1.1 Null space Definition 15.2 The null space null(\\(\\mathbf{A}\\)) of an \\(m \\times n\\) \\(\\mathbf{A}\\) is the set of all solutions of the homogeneous equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\) Another way to write null(\\(\\mathbf{A}\\)) is \\[\\begin{align*} \\mbox{null}(\\mathbf{A}) = \\{\\mathbf{x} : \\mathbf{x} \\in \\mathcal{R}^n \\mbox{ and } \\mathbf{A} \\mathbf{x} = \\mathbf{0} \\} \\end{align*}\\] Theorem 15.1 The null space of an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) is a subspace of \\(\\mathcal{R}^n\\). Proof. Do in class \\(\\mathbf{0}\\) vector sum of vectors scalar multiplication As a consequence, there will exist a set of vectors that span the null space null(\\(\\mathbf{A}\\)). However, the null space of \\(\\mathbf{A}\\) is defined implicitly. This means that the null space of \\(\\mathbf{A}\\) is not obvious given the vectors of \\(\\mathbf{A}\\) and must be checked/calculated. In class Find a spanning set for null(\\(\\mathbf{A}\\)) where \\[\\begin{align*} \\mathbf{A} = \\begin{pmatrix} 7 &amp; -2 &amp; 7 &amp; -4 &amp; 5 \\\\ 2 &amp; 0 &amp; 3 &amp; 3 &amp; 9 \\\\ -5 &amp; 2 &amp; -5 &amp; 7 &amp; -2 \\end{pmatrix} \\end{align*}\\] Find solution to system of homogeneous system of equations A &lt;- matrix(c(7, 2, -5, -2, 0, 2, 7, 3, -5, -4, 3, 7, 5, 9, -2), 3, 5) rref(cbind(A, 0)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 1.50 -4.50 0 ## [2,] 0 1 0 7.25 2.75 0 ## [3,] 0 0 1 0.00 6.00 0 Take the general solution and write as a linear combination of vectors where the coefficients are the free variables. general solution \\(x_1 = -1.5 x_4 + 4.5 x_5\\), \\(x_2 = -7.25 x_4 - 2.75 x_5\\), \\(x_3 = -6 x_5\\) and both \\(x_4\\) and \\(x_5\\) are free. Write out the general solution in vector form. \\[\\begin{align*} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{pmatrix} = \\begin{pmatrix} -1.5 x_4 + 4.5 x_5\\\\ -7.25 x_4 - 2.75 x_5 \\\\ -6 x_5 \\\\ x_4 \\\\ x_5 \\end{pmatrix} = x_4 \\begin{pmatrix} -1.5 \\\\ -7.25 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + x_5 \\begin{pmatrix} 4.5 \\\\ 2.75 \\\\ -6 \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{align*}\\] From above, the free variables \\(x_4\\) and \\(x_5\\) are multiplied by the vectors \\(\\mathbf{u} = \\begin{pmatrix} -1.5 \\\\ -7.25 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} 4.5 \\\\ 2.75 \\\\ -6 \\\\ 0 \\\\ 1 \\end{pmatrix}\\) where \\(\\{ \\mathbf{u}, \\mathbf{v} \\}\\) are a spanning set for the null(\\(\\mathbf{A}\\)) In class – do another Find a spanning set for null(\\(\\mathbf{A}\\)) where 15.1.2 Column space Definition 15.3 The columns space col(\\(\\mathbf{A}\\)) of an \\(m \\times n\\) \\(\\mathbf{A}\\) is the set of all linear combinations of the columns of \\(\\mathbf{A}\\). If \\(\\{ \\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}\\) are the columns of \\(\\mathbf{A}\\), then \\[\\begin{align*} \\mbox{col}(\\mathbf{A}) = \\mbox{span}(\\mathbf{A}) \\end{align*}\\] this can be written in set notation as \\[\\begin{align*} \\mbox{col}(\\mathbf{A}) = \\{ \\mathbf{b} : \\mathbf{A} \\mathbf{x} = \\mathbf{b} \\mbox{ for some } $\\mathbf{x} \\in \\mathcal{R}^n \\} \\end{align*}\\] Theorem 15.2 The column space of an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) is a subspace of \\(\\mathcal{R}^n\\). Proof. Do in class \\(\\mathbf{0}\\) vector sum of vectors scalar multiplication Compared to the null space, the column space is defined explicitly–it is the span of the columns of \\(\\mathbf{A}\\). The definition of the column space results in the fact that col(\\(\\mathbf{A}\\)) is the range of the linear transformation \\(\\mathbf{x} \\rightarrow \\mathbf{A} \\mathbf{x}\\). In class Find a spanning set for col(\\(\\mathbf{A}\\)) where \\[\\begin{align*} \\mathbf{A} = \\begin{pmatrix} 6 &amp; 0 &amp; 4 \\\\ 5 &amp; -1 &amp; -9 \\\\ -4 &amp; 7 &amp; 4 \\\\ 6 &amp; 2 &amp; 9 \\end{pmatrix} \\end{align*}\\] 15.1.3 Understanding the differerneces between the column space and the null space "],["section-linearly-independent-sets-and-bases.html", "Chapter 16 Linearly independent sets and bases 16.1 Bases for null(\\(\\mathbf{A}\\)) and col(\\(\\mathbf{A}\\))", " Chapter 16 Linearly independent sets and bases library(tidyverse) library(dasc2594) Recall that a set of vectors \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) is linearly independent if the only solution to the system of equations \\[\\begin{align*} x_1 \\mathbf{v}_1 + \\cdots + x_n \\mathbf{v}_n = \\mathbf{0} \\end{align*}\\] is the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\). In other words, it is not possible to write any of the vectors in the set \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) as a linear combination of the other vectors. Definition 16.1 Let \\(\\mathcal{H}\\) be a subspace of a vector space \\(\\mathcal{V}\\). Then, the set of vectors \\(\\mathcal{B} = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_n \\}\\) is a basis for \\(\\mathcal{H}\\) if The set of vectors \\(\\mathcal{B}\\) are linearly independent The subspace spanned by \\(\\mathcal{B}\\) is \\(\\mathcal{H}\\). In other words \\[\\begin{align*} \\mbox{span}(\\mathbf{v}_1, \\ldots, \\mathbf{v}_n) = \\mathcal{H} \\end{align*}\\] in class–standard basis \\(\\mathbf{e}_1, \\ldots \\mathbf{e}_n\\) which are the columns of the \\(n \\times n\\) identity matrix\\(\\mathbf{I}\\). in class–pick 3 vectors of length 3. Are they a basis for \\(\\mathcal{R}^3\\)? What about \\(\\mathcal{R}^4\\)? Theorem 16.1 (The Spanning Set Theorem) Let \\(\\mathcal{S} = \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) be vectors in the vector space \\(\\mathcal{V}\\) and let \\(\\mathcal{H}\\) = span(\\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\)) If one of the vectors, say \\(\\mathbf{v}_k\\), of \\(\\mathcal{S}\\) is a linear combination of the remaining vectors of \\(\\mathcal{S}\\), then the set formed by removing the vector \\(\\mathbf{v}_k\\) still spans \\(\\mathcal{H}\\) If \\(\\mathcal{H} \\neq \\{\\mathbf{0}\\}\\), then some subset of \\(\\mathcal{S}\\) spans \\(\\mathcal{H}\\). Proof. a) WLOG assume \\(\\mathbf{v}_n\\) is a linear combination of \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_{n-1}\\) (if not, permute the labels to make the linearly dependent vector the \\(n\\)th vector). Then \\[\\begin{align} \\tag{16.1} \\mathbf{v}_n = x_1 \\mathbf{v}_1 + \\cdots + x_{n-1} \\mathbf{v}_{n-1} \\end{align}\\] Because the vectors \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) span \\(\\mathcal{H}\\), any vector \\(\\mathbf{b}\\in \\mathcal{H}\\) can be written as \\[\\begin{align} \\tag{16.2} \\mathbf{b} = c_1 \\mathbf{v}_1 + \\cdots + c_{n} \\mathbf{v}_{n} \\end{align}\\] for scalars \\(c_1, \\ldots, c_n\\). Plugging the result from (16.1) into (16.2) shows that any vector \\(\\mathbf{b}\\) in \\(\\mathcal{H}\\) can be written only using the vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_{n-1}\\) As the vectors in \\(\\mathcal{S}\\) span \\(\\mathcal{H}\\), if there is a linearly dependent vector in \\(\\mathcal{S}\\), this vector can be removed from \\(\\mathcal{S}\\) and the span of this subset of \\(\\mathcal{S}\\) will still span \\(\\mathcal{H}\\). As long as \\(\\mathbf{H} \\neq \\{\\mathbf{0}\\}\\), there must be a least one nonzero vector in \\(\\mathbf{S}\\) so the removing of linearly dependent vectors will stop with at least one vector. As all of the linearly dependent vectors have been removed, the subset of \\(\\mathcal{S}\\) created in this manner will be a set of linearly independent vectors that span \\(\\mathcal{H}\\). 16.1 Bases for null(\\(\\mathbf{A}\\)) and col(\\(\\mathbf{A}\\)) Example 16.1 Find a basis for the col(\\(\\mathbf{A}\\)) where set.seed(2021) A &lt;- matrix(sample(-9:9, 15, replace = TRUE), 5, 3) \\[\\begin{align*} \\mathbf{A} = \\begin{pmatrix} -3 &amp; -4 &amp; 5 \\\\ -4 &amp; -4 &amp; -3 \\\\ 4 &amp; -4 &amp; -1 \\\\ -3 &amp; 4 &amp; 2 \\\\ 2 &amp; -5 &amp; 9 \\end{pmatrix} \\end{align*}\\] Calculate row echelon form and identify the pivot columns. The vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\) that make up the columns of \\(\\mathbf{A}\\) that are in the pivot columns form a basis for \\(\\mathbf{A}\\) Why is this? Think about the relationship between the columns of \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\) in \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) that result in a consistent solution. Example 16.2 Find a basis for the null(\\(\\mathbf{A}\\)) where set.seed(2021) A &lt;- matrix(sample(-9:9, 15, replace = TRUE), 5, 3) \\[\\begin{align*} \\mathbf{A} = \\begin{pmatrix} -3 &amp; -4 &amp; 5 \\\\ -4 &amp; -4 &amp; -3 \\\\ 4 &amp; -4 &amp; -1 \\\\ -3 &amp; 4 &amp; 2 \\\\ 2 &amp; -5 &amp; 9 \\end{pmatrix} \\end{align*}\\] Calculate solutions to homogeneous system of equations, write solution in vector equation form. Vectors form a basis for null(\\(\\mathbf{A}\\)) note: Facts about the basis for the null space null(\\(\\mathbf{A}\\)) The spanning set produced using the method above produces a linearly independent set because the free variables are weights on the spanning vectors. When null(\\(\\mathbf{A}\\)) contains nonzero vectors, the number of vectors in the spanning set for null(\\(\\mathbf{A}\\)) is the number of free variables in the solution of \\(\\mathbf{A} \\mathbf{x} = \\mathbf{0}\\). Example 16.3 The matrix \\(4 \\times 5\\) \\(\\mathbf{A}\\) has columns given by the vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_5\\) and is row equivalent to the matrix \\[\\begin{align*} \\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 &amp; 3 &amp; -2 &amp; 1 \\\\ 0 &amp; 0 &amp; 3 &amp; -2 &amp; 5 \\\\ 0 &amp; 0 &amp; 0 &amp; -1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\end{align*}\\] What is a basis for col(\\(\\mathbf{A}\\)) in terms of the vectors \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_5\\) Note that two matrices that are row equivalent have the same linear dependence relationsihps between their vectors (but the basis for their column space is different) Example 16.4 The matrix \\(\\mathbf{A}\\) is row equivalent to the matrix \\(\\mathbf{B}\\) A &lt;- matrix(c(1, 3, 2, 5, 4, 12 , 8, 20, 0, 1, 1, 2, 2, 5, 3, 8, -1, 5, 2, 8), 4, 5) B &lt;- rref(A) \\[\\begin{align*} \\mathbf{A} = \\begin{pmatrix} 1 &amp; 4 &amp; 0 &amp; 2 &amp; -1 \\\\ 3 &amp; 12 &amp; 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 8 &amp; 1 &amp; 3 &amp; 2 \\\\ 5 &amp; 20 &amp; 2 &amp; 8 &amp; 8 \\end{pmatrix} &amp; \\mathbf{B} = \\begin{pmatrix} 1 &amp; 4 &amp; 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\\\ \\end{align*}\\] What is a basis for col(\\(\\mathbf{A}\\))? What is a basis for col(\\(\\mathbf{B}\\))? What is span(\\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_5\\))? What is span(\\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_5\\))? Are the spaces spanned by the columns of \\(\\mathbf{A}\\) and the columns of \\(\\mathbf{B}\\) the same space? Theorem 16.2 The pivot columns of a matrix \\(\\mathbf{A}\\) for a basis for col(\\(\\mathbf{A}\\)) Proof. sketch: \\(\\mathbf{B}\\) rref of \\(\\mathbf{A}\\), linearly independent columns of \\(\\mathbf{B}\\) are same as linearly independent columns in \\(\\mathbf{A}\\). Other (non-pviot) columns are linearly dependent. By spanning set theorem, non-pivot columns can be removed from the spanning set without changing the span, leaving only the pivot columns of \\(\\mathbf{A}\\) as a basis for col($) "],["section-coordinate-systems-and-dimension.html", "Chapter 17 Coordinate Systems and Dimension 17.1 Coordinates in \\(\\mathcal{R}^n\\) 17.2 Dimension of a vector space 17.3 Subspaces of finite dimension 17.4 Dimensions of null(\\(\\mathbf{A}\\)) and col(\\(\\mathbf{A}\\))", " Chapter 17 Coordinate Systems and Dimension library(tidyverse) library(dasc2594) We already know about the cartesian coordinate system (x, y, z) which has the set of basis vectors \\[\\begin{align*} \\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} &amp;&amp; \\mathbf{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} &amp;&amp; \\mathbf{e}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{align*}\\] However, using the concept of a basis for a subspace \\(\\mathcal{H}\\) of some vector space \\(\\mathcal{V}\\), we might want to use a different basis. Luckily, we have learned how to construct bases for col(\\(\\mathbf{A}\\)) and null(\\(\\mathbf{A}\\)). You might be wondering why we want to create different bases. The usual cartesian basis has been good enough for me so far (unless you have used polar coordinates). In data science, the data often live in a high dimensional space (i.e., there are a number of data variables). However, while the data might have many variables, some of these variables are partially dependent and thus the space in which the data are embedded might be well approximated using a subspace of the original variables which can increase computation speed (less computation with fewer variables – recall from lab how the inverse of \\(\\mathbf{X}&#39;\\mathbf{X}\\) took much much longer with larger numbers of variables). Thus, understanding different coordinate systems and how to change coordinate systems can lead to more efficient data representation and model fitting. Theorem 17.1 (The unique representation Theorem) Let \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n \\}\\) be a basis for the vector space \\(\\mathcal{V}\\). Then, for each \\(\\mathbf{x}\\) in \\(\\mathcal{B}\\), there exists a unique set of coefficients \\(c_1, \\ldots, c_n\\) such that \\[\\begin{align*} \\mathbf{x} = c_1 \\mathbf{b}_1 + \\ldots + c_n \\mathbf{b}_n \\end{align*}\\] Proof. sketch: vectors of \\(\\mathcal{B}\\) span \\(\\mathcal{V}\\) so there exists a set of coefficienct \\(c_1, \\ldots, c_n\\) such that \\[\\begin{align*} \\mathbf{x} = c_1 \\mathbf{b}_1 + \\ldots + c_n \\mathbf{b}_n \\end{align*}\\] is true. Assume another set of coefficients \\(d_1, \\ldots, d_n\\) exists such that \\[\\begin{align*} \\mathbf{x} = d_1 \\mathbf{b}_1 + \\ldots + d_n \\mathbf{b}_n. \\end{align*}\\] Subtract these two equations to get \\[\\begin{align*} \\mathbf{0} = \\mathbf{x} - \\mathbf{x} = (c_1 - d_1) \\mathbf{b}_1 + \\ldots + (c_n - d_n) \\mathbf{b}_n. \\end{align*}\\] Because \\(\\mathcal{B}\\) is linearly independent by definition, all the weights in the equation above must be 0 (linear independence means the only solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) is the trivial solution). Therefore \\(c_i = d_i\\) for \\(i = 1, \\ldots, n\\). Definition 17.1 Suppose \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n \\}\\) be a basis for the vector space \\(\\mathcal{V}\\) and \\(\\mathbf{x} \\in \\mathcal{V}\\). The coordinates of \\(\\mathbf{x}\\) with respect to \\(\\mathcal{B}\\) are the coefficients \\(c_1, \\ldots, c_n\\) such that \\[\\begin{align*} \\mathbf{x} = c_1 \\mathbf{b}_1 + \\ldots + c_n \\mathbf{b}_n \\end{align*}\\] Example 17.1 In class using standard basis in 2-dimensions and vector \\(\\mathbf{x} = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\\) Example 17.2 In class using basis in 2-dimensions \\(b_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) and \\(b_2 = \\begin{pmatrix} 0.5 \\\\ 1 \\end{pmatrix}\\) and vector \\(\\mathbf{x} = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\\) transformation_matrix &lt;- tribble( ~ x, ~ y, 1, 0.5, 0, 1) %&gt;% as.matrix() p &lt;- plot_transformation(transformation_matrix) + geom_point(aes(x = 3, y = 2)) p + facet_wrap(~ time, labeller = labeller(time = c(&quot;1&quot; = &quot;Standard cooridinates&quot;, &quot;2&quot; = &quot;Shear cooridnates&quot;))) 17.1 Coordinates in \\(\\mathcal{R}^n\\) Let \\(\\mathbf{x}\\) be defined with the standard coordinates. Let \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) be a basis in \\(\\mathcal{R}^n\\). Define \\(\\mathbf{P}_B = \\begin{pmatrix} \\mathbf{b}_1 &amp; \\cdots &amp; \\mathbf{b}_n \\end{pmatrix}\\) as the matrix with columns the vectors of the basis. Then, the coordinates \\([\\mathbf{x}]_\\mathcal{B} = \\begin{pmatrix} [x_1]_\\mathcal{B}, \\ldots, [x_n]_\\mathcal{B} \\end{pmatrix}&#39;\\) of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\) can be found by solving the matrix equation \\[\\begin{align*} \\mathbf{P}_B [\\mathbf{x}]_\\mathcal{B} = \\mathbf{x} \\end{align*}\\] The matrix \\(\\mathbf{P}_B\\) is called the change-of-coordinates matrix from \\(\\mathcal{B}\\) to the standard basis in \\(\\mathcal{R}^n\\). The solution set (the coefficients) \\([\\mathbf{x}]_\\mathcal{B}\\) can be found using row operations or by using the fact that because the columns of \\(\\mathbf{P}_B\\) spans \\(\\mathcal{R}^n\\) the matrix \\(\\mathbf{P}_B\\) is invertible. Then, the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\) is \\[\\begin{align*} [\\mathbf{x}]_\\mathcal{B} = \\mathbf{P}_B^{-1} \\mathbf{x} \\end{align*}\\] Theorem 17.2 Let \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) be a basis for the vector space \\(\\mathcal{V}\\). Then, the coordinate mapping \\(\\mathbf{x} \\rightarrow \\mathbf{P}^{-1} \\mathbf{x}\\) is a one-to-one and onto transformation from \\(\\mathcal{V}\\) to \\(\\mathcal{R}^n\\) Proof. First we want to show that multiplication by \\(\\mathbf{P}_B^{-1}\\) defines a linear transformation. First, take two vectors \\[\\begin{align*} \\mathbf{u} &amp; = c_1 \\mathbf{b}_1 + \\ldots + c_n \\mathbf{b}_n \\end{align*}\\] and \\[\\begin{align*} \\mathbf{v} = d_1 \\mathbf{b}_1 + \\ldots + d_n \\mathbf{b}_n \\end{align*}\\] First, we show the mapping preserves vector addition \\[\\begin{align*} \\mathbf{u} + \\mathbf{v} \\rightarrow \\mathbf{P}_B^{-1} (\\mathbf{u} + \\mathbf{v}) = \\mathbf{P}_B^{-1} \\mathbf{u} + \\mathbf{P}_B^{-1} \\mathbf{v} \\end{align*}\\] which preserves vector addition Next, we show the mapping preserves scalar multiplication. Given scalar \\(a\\), \\[\\begin{align*} a\\mathbf{u} \\rightarrow \\mathbf{P}_B^{-1} (a \\mathbf{u}) = a \\mathbf{P}_B^{-1} \\mathbf{u} \\end{align*}\\] which preserves scalar multiplication. Therefore, this is a linear transformation. one-to-one and onto come from fact that \\(\\mathbf{P}_B\\) is and \\(n \\times n\\) matrix with \\(n\\) pivot columns (\\(n\\) linearly independent vectors because it is a basis for \\(\\mathcal{R}^n\\)) Example 17.3 in class–Give basis in \\(\\mathcal{R}^4\\), find coefficients with respect to this basis for the vector \\(\\mathbf{x}\\) 17.2 Dimension of a vector space In some sense, we already know about the dimension of a vector space through the concept of a span. The span of a set of vectors defines the dimension of the vector space. Theorem 17.3 In a vector space \\(\\mathcal{V}\\) with basis \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\), any set in \\(\\mathcal{V}\\) containing more than \\(n\\) vectors must be linearly dependent. Proof. Let \\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_p \\}\\) be a set of vectors in \\(\\mathcal{V}\\) with \\(p &gt; n\\). The coordinate vectors \\(\\{\\mathbf{P}_B \\mathbf{u}_1, \\ldots, \\mathbf{P}_B \\mathbf{u}_p\\}\\) form a linearly dependent set in \\(\\mathcal{R}^n\\) because there are more vectors (\\(p\\)) than entries (\\(n\\)) in each vector. Thus, there exist scalars \\(c_1, \\ldots, c_p\\), some nonzero, such that \\[\\begin{align*} c_1 \\mathbf{P}_B \\mathbf{u}_1 + \\ldots + c_p \\mathbf{P}_B \\mathbf{u}_p = \\mathbf{0}. \\end{align*}\\] which by linearity implies \\[\\begin{align*} \\mathbf{P}_B (c_1 \\mathbf{u}_1 + \\ldots + c_p \\mathbf{u}_p) = \\mathbf{0} \\end{align*}\\] Because the matrix \\(\\mathbf{P}_{B}\\) is a \\(n \\times n\\) matrix with n linearly independent columns, the only way the equation above can equal \\(\\mathbf{0}\\) is if the vector \\(c_1 \\mathbf{u}_1 + \\ldots + c_p \\mathbf{u}_p = \\mathbf{0}\\) (by the invertible matrix theorem). Therefore, the set of vectors \\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_p \\}\\) is linearly dependent because there are coefficients that allow the vectors to sum to \\(\\mathbf{0}\\). Thus, we know that for a vector space \\(\\mathcal{V}\\) that has a basis \\({\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n \\}}\\) that consists on \\(n\\) vectors, then every linearly independent set of vectors in \\(\\mathcal{V}\\) contains at most \\(n\\) vectors. Theorem 17.4 If a vector space \\({\\mathcal{V}}\\) has a basis with \\(n\\) vectors, then every other basis of \\({\\mathcal{V}}\\) must also contain exactly \\(n\\) vectors. Proof. Let \\(\\mathcal{B}_1\\) be a basis of \\({\\mathcal{V}}\\) containing \\(n\\) vectors and let \\(\\mathcal{B}_2\\) be any other basis of \\({\\mathcal{V}}\\). Because \\(\\mathcal{B}_1\\) and \\(\\mathcal{B}_2\\) are both bases, they both contain sets of linearly independent vectors. As such, the previous theorem states that each of these bases contain at most \\(n\\) vectors (otherwise the sets wouldn’t be linearly independent). Because \\(\\mathcal{B}_2\\) is a basis and the basis \\(\\mathcal{B}_1\\) contains \\(n\\) vectors, \\(\\mathcal{B}_2\\) must contain at least \\(n\\) vectors. These results combined are only satisfied when \\(\\mathcal{B}_2\\) contains \\(n\\) vectors. Like the span defined by the columns of a matrix \\(\\mathbf{A}\\), there is an abstract concept called dimension which measures the “size” of a vector space. Definition 17.2 If \\({\\mathcal{V}}\\) is spanned by a finite set of vectors, then \\({\\mathcal{V}}\\) is said to be finite dimensional. If \\({\\mathcal{V}}\\) is not spanned by a finite set of vectors, \\({\\mathcal{V}}\\) is said to be infinite dimensional. The smallest set of vectors that spans \\({\\mathcal{V}}\\) is a basis for \\({\\mathcal{V}}\\) and the number of vectors in this basis is called the dimension of \\({\\mathcal{V}}\\) and written as dim(\\({\\mathcal{V}}\\)). If \\({\\mathcal{V}}= \\{\\mathbf{0}\\}\\), then dim(\\({\\mathcal{V}}\\)) is said to be 0. Example 17.4 in class - span of 2 or 3 linearly independent vectors span, dim, and geometry Example 17.5 in class - span of 2 or 3 linearly dependent vectors span, dim, and geometry 17.3 Subspaces of finite dimension Theorem 17.5 Let \\(\\mathcal{H}\\) be a subspace of a finite-dimensional vector space \\({\\mathcal{V}}\\). Then, any linearly independent set in \\(\\mathcal{H}\\) can be expanded, if necessary to form a basis for \\(\\mathcal{H}\\). As \\(\\mathcal{H}\\) is a subspace of the finite-dimensional vector space \\({\\mathcal{V}}\\), \\(\\mathcal{H}\\) is a finite-dimensional vector space with \\[\\begin{align*} \\mbox{dim}(\\mathcal{H}) \\leq \\mbox{dim}(\\mathcal{V}) \\end{align*}\\] For a vector space of known dimension \\(p\\), finding a basis can be simplified by finding a linearly independent set of size \\(p\\). Theorem 17.6 (The Basis Theorem) Let \\({\\mathcal{V}}\\) be a \\(p\\) dimensional vector space with \\(p \\geq 1\\). Any linearly independent subset of \\(p\\) vectors is a basis for \\({\\mathcal{V}}\\). Equivalently, any set of \\(p\\) vectors that span \\({\\mathcal{V}}\\) is automatically a basis for \\({\\mathcal{V}}\\). 17.4 Dimensions of null(\\(\\mathbf{A}\\)) and col(\\(\\mathbf{A}\\)) The dimension of null(\\(\\mathbf{A}\\)) are the number of free variables in \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) and the dimension of col(\\(\\mathbf{A}\\)) is the number of pivot columns of \\(\\mathbf{A}\\). "],["section-rank.html", "Chapter 18 Rank 18.1 Rank", " Chapter 18 Rank library(tidyverse) library(dasc2594) Definition 18.1 Given an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), the row space, row(\\(\\mathbf{A}\\)) is the set of linearly independent rows of \\(\\mathbf{A}\\). Thus, the row space is the span of the rows of \\(\\mathbf{A}\\). Note: the row space of \\(\\mathbf{A}\\) is the column space of the transposed matrix \\(\\mathcal{A}&#39;\\). \\[\\begin{align*} row(\\mathbf{A}) = col(\\mathbf{A}&#39;) \\end{align*}\\] Example 18.1 in class Example 18.2 Find basis for row space, column space, and null space of \\(\\mathbf{A}\\) 18.1 Rank Definition 18.2 The rank of a matrix \\(\\mathbf{A}\\), rank(\\(\\mathbf{A}\\)) is the dimension of the column space col(\\(\\mathbf{A}\\)) Theorem 18.1 (The Rank Theorem) Let \\(\\mathbf{A}\\) be an \\(m \\times n\\) matrix. Then the dimension of are equal. The rank of \\(\\mathbf{A}\\) equals the number of pivot columns of \\(\\mathbf{A}\\) and \\[\\begin{align*} rank (\\mathbf{A}) + dim(null(\\mathbf{A})) = n \\end{align*}\\] Proof. rank(\\(\\mathbf{A}\\)) is the number of pivot columns. dim(null(\\(\\mathbf{A}\\))) is the number of non-pivot columns. The pivot columns + non-pivot columns are the number of columns. Example 18.3 in class \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix with dim(null(\\(\\mathbf{A}\\))) = p. What is rank(\\(\\mathbf{A}\\)) Example 18.4 \\(\\mathbf{A}\\) is a 6x9 matrix. Is it possible for null(\\(\\mathbf{A}\\)) = 2? Theorem 18.2 (Invertible Matrix Theorm + Rank) This is an extension of the prior statement of the invertible matrix theorem 8.5 Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix. Then the following statements are equivalent (i.e., they are all either simultaneously true or false). 13) The columns of \\(\\mathbf{A}\\) form a basis of \\(\\mathcal{R}^n\\) 14) col(\\(\\mathbf{A}\\)) = \\(\\mathcal{R}^n\\) 15) dim(col(\\(\\mathbf{A}\\))) = \\(n\\) 16) rank(\\(\\mathbf{A}\\)) = \\(n\\) 17) null(\\(\\mathbf{A}\\)) = \\(\\{\\mathbf{0}\\}\\) 18) dim(null(\\(\\mathbf{A}\\))) = 0 "],["section-change-of-basis.html", "Chapter 19 Change of basis", " Chapter 19 Change of basis Consider two bases \\(\\mathcal{B} = \\{ \\mathbf{b}_1, \\ldots, \\mathbf{b}_n \\}\\) and \\(\\mathcal{C} = \\{ \\mathbf{c}_1, \\ldots, \\mathbf{c}_n \\}\\) for a vector space \\(\\mathcal{V}\\). If we have a vector \\([\\mathbf{x}]_\\mathcal{B}\\) with coordinates in \\(\\mathcal{B}\\), what are the coordinates of \\([\\mathbf{x}]_\\mathcal{C}\\) with respect to \\(\\mathcal{C}\\)? Recall: we know how to change from the standard coordinates to the basis \\(\\mathcal{B}\\). If \\(\\mathbf{x}\\) is a vector in the standard coordinates and \\(\\mathbf{P}_B = \\begin{pmatrix} \\mathbf{b}_1 &amp; \\ldots &amp; \\mathbf{b}_n \\end{pmatrix}\\) is a matrix with columns given by the basis \\(\\mathbf{B}\\), the coordinates of \\([\\mathbf{x}]_\\mathcal{B}\\) of the vector \\(\\mathbf{x}\\) with respect to the basis \\(\\mathcal{B}\\) are \\[\\begin{align*} [\\mathbf{x}]_\\mathcal{B} = \\mathbf{P}_B^{-1} \\mathbf{x} \\end{align*}\\] and, as a consequence, given a vector \\([\\mathbf{x}]_\\mathcal{B}\\) with coordinates with respect to the basis \\(\\mathcal{B}\\), the vector of coefficients \\(\\mathbf{x}\\) with standard coordinates is given by \\[\\begin{align*} \\mathbf{x} = \\mathbf{P}_B [\\mathbf{x}]_\\mathcal{B}. \\end{align*}\\] Notice that change of coordinates is a linear transformation from \\(\\mathcal{B}\\) to \\(\\mathcal{C}\\) with transformation matrix \\(\\mathbf{A}\\). Despite the more complex notation, this is just another linear transformation [link]. Draw diagram Now, we can combine these ideas. Given a vector \\([\\mathbf{x}]_\\mathcal{B}\\) written with coordinates with respect to the basis \\(\\mathcal{B}\\), we can find the coordinates of \\([\\mathbf{x}]_\\mathcal{C}\\) with respect to the basis \\(\\mathcal{C}\\). First, we find the coordinates of the vector \\(\\mathbf{x}\\) with respect to the standard basis then find the coordinates of \\([\\mathbf{x}]_\\mathcal{C}\\) with respect to the basis \\(\\mathcal{C}\\). Let \\(\\mathbf{P}_B = \\begin{pmatrix} \\mathbf{b}_1 &amp; \\ldots &amp; \\mathbf{b}_n \\end{pmatrix}\\) and \\(\\mathbf{P}_C = \\begin{pmatrix} \\mathbf{c}_1 &amp; \\ldots &amp; \\mathbf{c}_n \\end{pmatrix}\\), then given a vector \\([\\mathbf{x}]_\\mathcal{B}\\) with coordinates with respect to the basis \\(\\mathcal{B}\\), the coordinates \\([\\mathbf{x}]_\\mathcal{C}\\) of this vector with respect to the basis \\(\\mathcal{C}\\) is \\[\\begin{align*} [\\mathbf{x}]_\\mathcal{C} = \\mathbf{P}_C^{-1} \\mathbf{P}_B [\\mathbf{x}]_\\mathcal{B}. \\end{align*}\\] Draw diagram Example 19.1 2-d change of basis Example 19.2 3-d change of basis "],["section-eigenvectors-and-eigenvalues.html", "Chapter 20 Eigenvectors and Eigenvalues 20.1 Eigenspaces", " Chapter 20 Eigenvectors and Eigenvalues library(tidyverse) library(dasc2594) set.seed(2021) We have just learned about change of basis in an abstract sense. Now, we will learn about a special change of basis that is “data-driven” called an eigenvector. Eigenvectors and the corresponding eigenvalues are a vital tool in data science for data compression and modeling. Definition 20.1 An eigenvector of an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is a nonzero vector \\(\\mathbf{x}\\) such that the matrix equation \\[\\begin{align*} \\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x} \\end{align*}\\] for some scalar \\(\\lambda\\). If there exists some \\(\\lambda \\neq 0\\) (a non-trivial solutions), then \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf{A}\\) corresponding to the eigenvector \\(\\mathbf{x}\\). Example 20.1 It is easy to check if a vector is an eigenvalue: Let \\(\\mathbf{A} = \\begin{pmatrix} 0 &amp; 6 &amp; 8 \\\\ 1/2 &amp; 0 &amp; 0 \\\\ 0 &amp; 1/2 &amp; 0 \\end{pmatrix}\\), \\(\\mathbf{u} = \\begin{pmatrix} 16 \\\\ 4 \\\\ 1 \\end{pmatrix}\\), and \\(\\mathbf{v} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}\\). Determine if \\(\\mathbf{u}\\) or \\(\\mathbf{v}\\) are eigenvectors of \\(\\mathbf{A}\\). If they are eigenvectors, what are the associated eigenvalues. Now, plot \\(\\mathbf{u}\\), \\(\\mathbf{A} \\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{A} \\mathbf{v}\\) to show this relationship geometrically. An eigenvector is a (nonzero) vector \\(\\mathbf{x}\\) that gets mapped to a scalar multiple of itself \\(\\lambda \\mathbf{x}\\) by the matrix transformation \\(\\mathbf{A}\\mathbf{x} \\rightarrow \\lambda \\mathbf{x}\\). As such, we say that \\(\\mathbf{x}\\) and \\(\\mathbf{A} \\mathbf{x}\\) are collinear with the origin (\\(\\mathbf{0}\\)) in the sense that these points lie on the same line that goes through the origin. Note: The matrix \\(\\mathbf{A}\\) must be an \\(n \\times n\\) square matrix. A similar decomposition (called the singular value decomposition) can be used for rectangular matrices. Example 20.2 Example: reflection Draw images: https://textbooks.math.gatech.edu/ila/eigenvectors.html Theorem 20.1 (The distinct eigenvalues theorem) Let \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\) be eigenvectors of a matrix \\(\\mathbf{A}\\) and suppose the corresponding eigenvalues are \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\) are all distinct (different values). Then, the set of vectors \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) is linearly independent. Proof. Suppose the set \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) is linearly dependent. Then, there is some \\(j\\) such that \\(\\mathbf{v}_j = \\sum_{k = 1}^{j-1} x_k \\mathbf{v}_k\\). If we choose the first linearly dependent vector as \\(j\\), we know that the subset of vectors \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_{j-1}\\}\\) is linearly independent and \\[\\begin{align*} \\mathbf{v}_j &amp; = x_1 \\mathbf{v}_1 + \\cdots x_{j-1} + \\mathbf{v}_{j-1} \\end{align*}\\] for some scalars \\(x_1, \\ldots, x_{j-1}\\). Multiplying the equation above on the left by \\(\\mathbf{A}\\) on both sides gives \\[\\begin{align*} \\mathbf{A}\\mathbf{v}_j &amp; = \\mathbf{A} (x_1 \\mathbf{v}_1 + \\cdots + x_{j-1} \\mathbf{v}_{j-1}) \\\\ \\lambda_j \\mathbf{v}_j &amp; = x_1 \\mathbf{A} \\mathbf{v}_1 + \\cdots + x_{j-1} \\mathbf{A} \\mathbf{v}_{j-1} \\\\ &amp; = x_1 \\lambda_1 \\mathbf{v}_1 + \\cdots x_{j-1} \\lambda_{j-1} + \\mathbf{v}_{j-1} \\\\ \\end{align*}\\] Multiplying the first equation by \\(\\lambda_j\\) and subtracting this from the second equation gives \\[\\begin{align*} \\mathbf{0} = \\lambda_j \\mathbf{v}_j - \\lambda_j \\mathbf{v}_j &amp; = x_1 (\\lambda_1 - \\lambda_j) \\mathbf{v}_1 + \\cdots x_{j-1} + (\\lambda_{j-1} - \\lambda_j) \\mathbf{v}_{j-1} \\\\ \\end{align*}\\] Because \\(\\lambda_k \\neq \\lambda_j\\) for all \\(k &lt; j\\), the equation above implies a linear dependence among the set of vectors \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_{j-1}\\}\\) which is a contradiction. Therefore, our assumption that there exists a linearly dependent vector \\(\\mathbf{v}_j\\) is violated and all the \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) are linearly independent. 20.1 Eigenspaces Given a square \\(n \\times n\\) matrix \\(\\mathbf{A}\\), we know how to check if a given vector \\(\\mathbf{x}\\) is an eigenvector and then how to find the eigenvalue associated with that eigenvector. Next, we want to check if a given number is an eigenvalue of \\(\\mathbf{A}\\) and to find all the eigenvectors corresponding to that eigenvalue. Given a square \\(n \\times n\\) matrix \\(\\mathbf{A}\\) and a scalar \\(\\lambda\\), the eigenvectors of \\(\\mathbf{A}\\) associated with the scalar \\(\\lambda\\) (if there are eigenvectors associated with \\(\\lambda\\)) are the nonzero solutoins to the equation \\(\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}\\). This can be written as \\[\\begin{align*} \\mathbf{A} \\mathbf{x} &amp; = \\lambda \\mathbf{x} \\\\ \\mathbf{A} \\mathbf{x} -\\lambda \\mathbf{x} &amp; = \\mathbf{0} \\\\ \\mathbf{A} \\mathbf{x} -\\lambda \\mathbf{I} \\mathbf{x} &amp; = \\mathbf{0} \\\\ \\left( \\mathbf{A} -\\lambda \\mathbf{I} \\right) \\mathbf{x} &amp; = \\mathbf{0}. \\\\ \\end{align*}\\] Therefore, the eigenvectors of \\(\\mathbf{A}\\) associated with \\(\\lambda\\), if there are any, are the nontrivial solutions of the homogeneous matrix equation \\(\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}\\). In other words, the eigenvectors are the nonzero vectors in the null space null\\(\\left( \\mathbf{A} -\\lambda \\mathbf{I} \\right)\\). If there is not a nontrivial solution (solution \\(\\mathbf{x} \\neq \\mathbf{0}\\)), then \\(\\lambda\\) is not an eigenvalue of \\(\\mathbf{A}\\). Hey, we know how to find solutions to homogeneous systems of equations! Thus, we know how to find the eigenvectors of \\(\\mathbf{A}\\). All we have to do is solve the system of linear equations \\(\\left( \\mathbf{A} -\\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}\\) for a given \\(\\lambda\\) (actually, for all \\(\\lambda\\)s, which we can’t do). If only there was some way to find eigenvalues \\(\\lambda\\) (hint: there is and it is coming next chapter). Example 20.3 Let \\(\\mathbf{A} = \\begin{pmatrix} 3 &amp; 6 &amp; -8 \\\\ 0 &amp; 0 &amp; 6 \\\\ 0 &amp; 0 &amp; 2 \\end{pmatrix}\\). Then an eigenvector with eigenvector \\(\\lambda\\) is a nontrival solution to \\[\\begin{align*} \\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} &amp; = \\mathbf{0} \\end{align*}\\] which can be written as \\[\\begin{align*} \\begin{pmatrix} 3 - \\lambda &amp; 6 &amp; -8 \\\\ 0 &amp; 0 - \\lambda &amp; 6 \\\\ 0 &amp; 0 &amp; 2 - \\lambda \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} &amp; = \\mathbf{0} \\end{align*}\\] which can be solved for a given \\(\\lambda\\) using an augmented matrix form and row operations to reduce to reduced row echelon form. Letting \\(\\lambda = 3\\), we have \\[\\begin{align*} \\begin{pmatrix} 3 - 3 &amp; 6 &amp; -8 \\\\ 0 &amp; 0 - 3 &amp; 6 \\\\ 0 &amp; 0 &amp; 2 - 3 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} &amp; = \\mathbf{0} \\end{align*}\\] which has solution lambda &lt;- 3 rref(cbind(A - lambda * diag(nrow(A)), 0)) ## [,1] [,2] [,3] [,4] ## [1,] 0 1 0 0 ## [2,] 0 0 1 0 ## [3,] 0 0 0 0 Definition 20.2 Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix and let \\(\\lambda\\) be an eigenvalue of \\(\\mathbf{A}\\). Then, the \\(\\lambda\\)-eigenspace of \\(\\mathbf{A}\\) is the solution set of the matrix equation \\(\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) \\mathbf{x} = \\mathbf{0}\\) which is the subspace null(\\(\\mathbf{A} - \\lambda \\mathbf{I}\\)). Therefore, the \\(\\lambda\\)-eigenspace is a subspace (the null space of any matrix is a subspace) that contains the zero vector \\(\\mathbf{0}\\) and all the eigenvectors of \\(\\mathbf{A}\\) with corresponding eigenvalue \\(\\lambda\\). Example 20.4 For \\(\\lambda\\) = -2, 1, and 3, decide if \\(\\lambda\\) is a eigenvalue of the matrix \\(\\mathbf{A} = \\begin{pmatrix} 2 &amp; -4 \\\\ -1 &amp; 1 \\end{pmatrix}\\) and if so, compute a basis for the \\(\\lambda\\)-eigenspace. A &lt;- matrix(c(2, -1, -4, 1), 2, 2) Calculate using rref on \\(\\mathbf{A} - \\lambda \\mathbf{I}\\) augmented with \\(\\mathbf{0}\\) and solve in parametric form After doing all three \\(\\lambda\\)s, draw a graphic showing the \\(\\lambda\\)-eignenspace basis. 20.1.1 Computing Eigenspaces Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) matrix and let \\(\\lambda\\) be a scalar. \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{A}\\) if and only if \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\) has a non-trivial solution. The matrix equation \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\) has a non-trivial solution if and only if null\\((\\mathbf{A} - \\lambda \\mathbf{I}) \\neq \\{\\mathbf{0} \\}\\) Finding a basis for the \\(\\lambda\\)-eigenspace of \\(\\mathbf{A}\\) is equivalent to finding a basis for null\\((\\mathbf{A} - \\lambda \\mathbf{I})\\) which can be done by finding parametric forms of the solutions of the homogeneous system of equations \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\). The dimension of the \\(\\lambda\\)-eigenspace of \\(\\mathbf{A}\\) is equal to the number of free variables in the system of equations \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\) which is the number of non-pivot columns of \\(\\mathbf{A} - \\lambda \\mathbf{I}\\). The eigenvectors with eigenvalue \\(\\lambda\\) are the nonzero vectors in null\\((\\mathbf{A} - \\lambda \\mathbf{I})\\) which are equivalent to the nontrivial solutions of \\((\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\). Note that this leads of a fact about the \\(0\\)-eigenspace. Definition 20.3 Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix. Then The number 0 is an eigenvalue of \\(\\mathbf{A}\\) if and only if \\(\\mathbf{A}\\) is not invertible. If 0 is an eigenvalue of \\(\\mathbf{A}\\), then the 0-eigenspace of \\(\\mathbf{A}\\) is null\\((\\mathbf{A})\\). Proof. 0 is an eigenvalue of \\(\\mathbf{A}\\) if and only if null\\((\\mathbf{A} - 0 \\mathbf{I})\\) = null\\((\\mathbf{A})\\). By the invertible matrix theorem, \\(\\mathbf{A}\\) is invertible if and only if null\\((\\mathbf{A}) = \\{\\mathbf{0}\\}\\) but we know that the 0-eigenspace of \\(\\mathbf{A}\\) is not the trivial set \\(\\{\\mathbf{0}\\}\\) because 0 is an eigenvalue. Theorem 20.2 (Invertible Matrix Theorm + eigenspaces) This is an extension of the prior statement of the invertible matrix theorem 8.5 Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix and \\(T: \\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\) be the linear transformation given by \\(T(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\). Then the following statements are equivalent (i.e., they are all either simultaneously true or false). \\(\\mathbf{A}\\) is invertible. \\(\\mathbf{A}\\) has n pivot columns. null\\((\\mathbf{A}) = \\{\\nathbf{0}\\}\\). The columns of \\(\\mathbf{A}\\) are linearly independent. The columns of \\(\\mathbf{A}\\) span \\(\\mathcal{R}^n\\). The matrix equation \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a uniqu solution for each \\(\\mathbf{b} \\in \\mathcal{R}^n\\). The transormation \\(T\\) is invertible. The transormation \\(T\\) is one-to-one. The transormation \\(T\\) is onto. det\\((\\mathbf{A}) \\neq 0\\) 0 is not an eigenvalue of \\(\\mathbf{A}\\) "],["section-the-characteristic-equation.html", "Chapter 21 The Characteristic Equation 21.1 Similarity 21.2 The geometric interpetation of similar matrices", " Chapter 21 The Characteristic Equation library(tidyverse) library(dasc2594) The characteristic equation/polynomial encodes information about the eigenvalues of the characteristic equation. In the previous chapter, we showed how we can decide if a scalar \\(\\lambda\\) is an eigenvalue of a matrix and how to find the vectors associated with the eigenvalue. However, we did not learn how to find eigenvalues (other than to just randomly try \\(\\lambda\\)). The characteristic equation/polynomial allows for determining the eigenvalues \\(\\lambda\\). Definition 21.1 Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) matrix. The characteristic equation/polynomial of \\(\\mathbf{A}\\) is the function \\(f(\\lambda)\\) given by \\[\\begin{align*} f(\\lambda) = det(\\mathbf{A} - \\lambda \\mathbf{I}) \\end{align*}\\] While not obvious, the function \\(f(\\lambda)\\) is a polynomial of \\(\\lambda\\) but requires computing the determinant of the matrix \\(\\mathbf{A} - \\lambda \\mathbf{I}\\) which contains an unknown value \\(\\lambda\\). Example 21.1 Find the characteristic equation of the matrix \\(\\mathbf{A} = \\begin{pmatrix} 3 &amp; 5 \\\\ 2 &amp; -1 \\end{pmatrix}\\) do in class Example 21.2 Find the characteristic equation of the matrix \\(\\mathbf{A} = \\begin{pmatrix} 0 &amp; 6 &amp; 8 \\\\ \\frac{1}{2} &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{2} &amp; 0 \\end{pmatrix}\\) do in class (expand cofactors along the third column) Once the characteristic equation is defined, we can use the equation to solve for the eigenvalues. Theorem 21.1 Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) matrix and let \\(f(\\lambda) = det(\\mathbf{A} - \\lambda \\mathbf{I})\\) be a characteristic polynomial. Then, the number \\(\\lambda_0\\) is an eigenvalue of \\(\\mathbf{A}\\) if and only if \\(f(\\lambda_0) = 0\\). Proof. By the invertible matrix theorem, the matrix \\((\\mathbf{A} - \\lambda_0 \\mathbf{I}) \\mathbf{x} = \\mathbf{0}\\) has a nontrivial solution if and only if \\(det(\\mathbf{A} - \\lambda_0 \\mathbf{I}) \\mathbf{x} = \\mathbf{0}\\). Therefore, the following statements are equivalent: \\(\\lambda_0\\) is an eigenvalue of \\(\\mathbf{A}\\) \\(\\mathbf{A} \\mathbf{x} = \\lambda_0 \\mathbf{x}\\) has a nontrivial solution \\((\\mathbf{A} - \\lambda_0 \\mathbf{I}) \\mathbf{x} = \\mathbf{0}\\) has a nontrivial solution \\(\\mathbf{A} - \\lambda_0 \\mathbf{I}\\) is not invertible \\(det(\\mathbf{A} - \\lambda_0 \\mathbf{I}) = \\mathbf{0}\\) \\(f(\\lambda_0) = 0\\) Example 21.3 Using the characteristic equation of the matrix \\(\\mathbf{A} = \\begin{pmatrix} 3 &amp; 5 \\\\ 2 &amp; -1 \\end{pmatrix}\\), solve for the eigenvalues and find a basis for the \\(\\lambda\\)-eigenspaces do in class Example 21.4 Using the characteristic equation of the matrix \\(\\mathbf{A} = \\begin{pmatrix} 0 &amp; 6 &amp; 8 \\\\ \\frac{1}{2} &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{2} &amp; 0 \\end{pmatrix}\\), solve for the eigenvalues and find a basis for the \\(\\lambda\\)-eigenspaces do in class (expand cofactors along the third column) 21.1 Similarity The idea behind similar matrices is to understand how the linear transformations implied by the transformation behave. Two matrices are similar if their transformation behavior (rotation, expansion/contraction, etc.) is the same but the coordinates on which the matrix operates are different. Definition 21.2 The matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are said to be similar if there exists an invertible matrix \\(\\mathbf{P}\\) where \\[\\begin{align*} \\mathbf{A} = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} \\end{align*}\\] or equivalently \\[\\begin{align*} \\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P}= \\mathbf{B} \\end{align*}\\] Therefore, it is possible to change \\(\\mathbf{A}\\) into \\(\\mathbf{B}\\) with an invertible (one-to-one and onto) transformation. Example 21.5 Consider the following example with matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{P}\\) defined as below: A &lt;- matrix(c(3, 0, 0, -2), 2, 2) A ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 -2 B &lt;- matrix(c(-12, -10, 15, 13), 2, 2) B ## [,1] [,2] ## [1,] -12 15 ## [2,] -10 13 P &lt;- matrix(c(-2, 1, 3,-1), 2, 2) P ## [,1] [,2] ## [1,] -2 3 ## [2,] 1 -1 P %*% B %*% solve(P) ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 -2 solve(P) %*% A %*% P ## [,1] [,2] ## [1,] -12 15 ## [2,] -10 13 Theorem 21.2 If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are \\(n \\times n\\) similar matrices, then \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) will have the same characteristic polynomial and therefore the same eigenvalues. Proof. If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are similar, then there exists an invertible matrix \\(\\mathbf{P}\\) such that \\[\\begin{align*} \\mathbf{A} = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} \\end{align*}\\] Therefore \\[\\begin{align*} \\mathbf{A} - \\lambda \\mathbf{I} &amp; = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} - \\lambda \\mathbf{I} \\\\ &amp; = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} - \\lambda \\mathbf{P} \\mathbf{P}^{-1} \\\\ &amp; = \\mathbf{P} \\left( \\mathbf{B} \\mathbf{P}^{-1} - \\lambda \\mathbf{P}^{-1} \\right) \\\\ &amp; = \\mathbf{P} \\left( \\mathbf{B} - \\lambda \\mathbf{I} \\right) \\mathbf{P}^{-1}\\\\ \\end{align*}\\] To get the characteristic equation, we need to solve for the determinant \\[\\begin{align*} det\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) &amp; = det\\left( \\mathbf{P} \\left( \\mathbf{B} - \\lambda \\mathbf{I} \\right) \\mathbf{P}^{-1} \\right) \\\\ &amp; = det\\left( \\mathbf{P} \\right) det\\left( \\mathbf{B} - \\lambda \\mathbf{I} \\right) det\\left(\\mathbf{P}^{-1} \\right) \\\\ \\end{align*}\\] We know that \\(det\\left(\\mathbf{P}^{-1} \\right)\\) = \\(\\frac{1}{det\\left(\\mathbf{P} \\right)}\\) (or, equivalently \\(det\\left(\\mathbf{P} \\right) det\\left(\\mathbf{P}^{-1} \\right) = det\\left(\\mathbf{P} \\mathbf{P}^{-1} \\right) = det(\\mathbf{I}) = 1\\)), we have \\(det\\left( \\mathbf{A} - \\lambda \\mathbf{I} \\right) = det\\left( \\mathbf{B} - \\lambda \\mathbf{I} \\right)\\) so that \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) have the same characteristic polynomial (and the same eigenvalues). 21.2 The geometric interpetation of similar matrices In general, similar matrices do similar things in different spaces (different spaces in terms of different bases). Example here "],["section-diagonalization.html", "Chapter 22 Diagonalization", " Chapter 22 Diagonalization library(tidyverse) library(dasc2594) set.seed(2021) Definition 22.1 A \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is diagonalizable if the matrix \\(\\mathbf{A}\\) is similar to a diagonal matrix. This is equivalent to saying there exists some invertible \\(n \\times n\\) matrix \\(\\mathbf{P}\\) and diagonal matrix \\(\\mathbf{D}\\) such that \\[\\begin{align*} \\mathbf{A} &amp; = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\end{align*}\\] Example 22.1 Any diagonal matrix \\(\\mathbf{D}\\) is diagonalizable becuase it is self-similar. Theorem 22.1 (The Diagonalization Theorem) A \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is diagonalizable if and only if the matrix \\(\\mathbf{A}\\) has \\(n\\) linearly independent eigenvectors. In addition, the \\(n \\times n\\) matrix \\(\\mathbf{A} = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}\\) with diagonal matrix \\(\\mathbf{D}\\) if and only if the columns of \\(\\mathbf{P}\\) are the lienarly independent eigenvectors of \\(\\mathbf{A}\\). Then, the diagonal elements of \\(\\mathbf{D}\\) are the eigenvalues of \\(\\mathbf{A}\\) that correspond to the eigenvectors in \\(\\mathbf{P}\\). Proof. This comes directly from Theorem 20.1 where if a \\(n \\times n\\) matrix has \\(n\\) distinct eigenvalues \\(\\lambda_1 \\neq \\lambda_2 \\neq \\cdots \\neq \\lambda_n\\), the the corresponding eigenvalues \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) are linearly independent. This theorem implies that the matrix \\(\\mathbf{A}\\) is diagonalizable if and only if the eigenvectors of \\(\\mathbf{A}\\) form a basis for \\(\\mathcal{R}^n\\). When this is the case, the set of eigenvectors is called an eigenbasis. Example 22.2 Consider the following example of a diagonalizable matrix \\(\\mathbf{A}\\) A &lt;- matrix(c(9, 2, 0, -3, 2, -4, 1, 0, 3), 3, 3) A ## [,1] [,2] [,3] ## [1,] 9 -3 1 ## [2,] 2 2 0 ## [3,] 0 -4 3 eigen_A &lt;- eigen(A) str(eigen_A) ## List of 2 ## $ values : num [1:3] 7.63 4.52 1.86 ## $ vectors: num [1:3, 1:3] -0.905 -0.322 0.278 -0.407 -0.324 ... ## - attr(*, &quot;class&quot;)= chr &quot;eigen&quot; P &lt;- eigen_A$vectors P ## [,1] [,2] [,3] ## [1,] -0.9050468 -0.4069141 -0.01938647 ## [2,] -0.3217259 -0.3235720 0.27433148 ## [3,] 0.2781774 0.8542377 0.96143976 D &lt;- diag(eigen_A$values) D ## [,1] [,2] [,3] ## [1,] 7.626198 0.000000 0.000000 ## [2,] 0.000000 4.515138 0.000000 ## [3,] 0.000000 0.000000 1.858664 P %*% D %*% solve(P) ## [,1] [,2] [,3] ## [1,] 9.000000e+00 -3 1 ## [2,] 2.000000e+00 2 0 ## [3,] -9.992007e-16 -4 3 all.equal(A, P %*% D %*% solve(P)) ## [1] TRUE Theorem 22.2 Let \\(\\mathbf{A}\\) be a \\(n \\times n\\) diagonalizable matrix with \\(\\mathbf{A} = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}\\). Then, the matrix power \\(\\mathbf{A}^p\\) is \\[\\begin{align*} \\mathbf{A}^p = \\mathbf{P} \\mathbf{D}^p \\mathbf{P}^{-1} \\end{align*}\\] Proof. In class Example 22.3 In this example, we apply the diagonalization theorem to the matrix \\(\\mathbf{A}\\) Consider the matrix \\(\\mathbf{A} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix}\\) which has eigenvalues 1, 2, 3. Then the standard basis \\(\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\), \\(\\mathbf{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\), and \\(\\mathbf{e}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\) are corresponding eigenvectors (check the definition \\(\\mathbf{A} \\lambda = \\mathbf{v} \\lambda\\)) because \\[\\begin{align*} \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} &amp; = 1 * \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\\\ \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} &amp; = 2 * \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\\\ \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} &amp; = 3 * \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{align*}\\] Thus, by the diagonlaization theorem, we have \\(\\mathbf{A} = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}\\) where \\(\\mathbf{P}\\) is the identity matrix and \\(\\mathbf{D}\\) is the diagonal matrix with entries 1, 2, 3. \\[\\begin{align*} \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix} &amp; = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}^{-1} \\end{align*}\\] which gives us that \\(\\mathbf{A}\\) is similar to itself. However, there is nothing in the diagonalization theorem that says that we must put the eigenvalues in the order 1, 2, 3. If we put the eigenvalues in the order 3, 2, 1, then the corresponding eigenvectors are \\(\\mathbf{e}_3\\), \\(\\mathbf{e}_2\\), and \\(\\mathbf{e}_1\\). Using the diagonlaization theorem, we have \\(\\mathbf{A} = \\tilde{\\mathbf{P}} \\tilde{\\mathbf{D}} \\tilde{\\mathbf{P}}^{-1}\\) where \\(\\tilde{\\mathbf{P}}\\) is the matrix with columns \\(\\mathbf{e}_3\\), \\(\\mathbf{e}_2\\), and \\(\\mathbf{e}_1\\) and \\(\\tilde{\\mathbf{D}}\\) is the diagonal matrix with entries 3, 2, 1 which results in \\[\\begin{align*} \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix} &amp; = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{pmatrix}^{-1} \\end{align*}\\] which implies that the matrices \\(\\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) are similar to each other "],["section-inner-product-length-and-orthogonality.html", "Chapter 23 Inner product, length, and orthogonality 23.1 Distance 23.2 Orthogonal vectors 23.3 Angles between vectors 23.4 Orthogonal sets 23.5 Orthogonal projections", " Chapter 23 Inner product, length, and orthogonality library(tidyverse) library(dasc2594) set.seed(2021) Definition 23.1 Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors in \\(\\mathcal{R}^n\\). Then, the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is \\(\\mathbf{u}&#39; \\mathbf{v}\\). The vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are \\(n \\times 1\\) matrices where \\(\\mathbf{u}&#39;\\) is a \\(1 \\times n\\) matrix and the inner product \\(\\mathbf{u}&#39; \\mathbf{v}\\) is a scalar (\\(1 \\times 1\\) matrix). The inner product is also sometimes called the dot product and written as \\(\\mathbf{u} \\cdot \\mathbf{v}\\). If the vectors \\[\\begin{align*} \\mathbf{u} = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{pmatrix} &amp; &amp; \\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix} \\end{align*}\\] then \\(\\mathbf{u}&#39; \\mathbf{v} = u_1 v_1 + u_2 v_2 + \\cdots u_n v_n\\) Example 23.1 Find the inner product \\(\\mathbf{u}&#39;\\mathbf{v}\\) and \\(\\mathbf{v}&#39;\\mathbf{u}\\) of \\[\\begin{align*} \\mathbf{u} = \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} &amp; &amp; \\mathbf{v} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 3 \\end{pmatrix} \\end{align*}\\] do by hand u &lt;- c(2, -3, 1) v &lt;- c(4, -2, 3) # u&#39;v sum(u*v) ## [1] 17 t(u) %*% v ## [,1] ## [1,] 17 # v&#39;u sum(v*u) ## [1] 17 t(v) %*% u ## [,1] ## [1,] 17 The properties of inner products are defined with the following theorem. Theorem 23.1 Let \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) be vectors in \\(\\mathcal{R}^n\\) and let \\(c\\) be a scalar. Then \\(\\mathbf{u}&#39;\\mathbf{v} = \\mathbf{v}&#39;\\mathbf{u}\\) \\((\\mathbf{u} + \\mathbf{v})&#39; \\mathbf{w} = \\mathbf{u}&#39; \\mathbf{w} + \\mathbf{v}&#39; \\mathbf{w}\\) \\(( c \\mathbf{u} )&#39; \\mathbf{v} = c ( \\mathbf{v}&#39;\\mathbf{u} )\\) \\(\\mathbf{u}&#39;\\mathbf{u} \\geq 0\\) with \\(\\mathbf{u}&#39;\\mathbf{u} = 0\\) only when \\(\\mathbf{u} = \\mathbf{0}\\) Based on the theorem above, the inner product of a vector with itself (\\(\\mathbf{u}&#39;\\mathbf{u}\\)) is strictly non-negative. Thus, we can define the length of the vector \\(\\mathbf{u}\\) (also called the norm of the vector \\(\\mathbf{u}\\)). Definition 23.2 The length of a vector \\(\\mathbf{v} \\in \\mathcal{R}^n\\), also called the vector norm \\(\\| \\mathbf{v} \\|\\) is defined as \\[\\begin{align*} \\| \\mathbf{v} \\| &amp; = \\sqrt{\\mathbf{v}&#39;\\mathbf{v}} = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\end{align*}\\] Example 23.2 Let \\(\\mathbf{v} = \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\in \\mathcal{R}^2\\). Show that the definition of the norm satisfies the Pythagorean theorem. Another property of the norm is how the norm changes based on scalar multiplication. Let \\(\\mathbf{v} \\in \\mathcal{R}^n\\) be a vector and let \\(c\\) be a scalar. Then \\(\\|c \\mathbf{v}\\| = |c|\\|\\mathbf{v}\\|\\) Definition 23.3 A vector \\(\\mathbf{v} \\in \\mathcal{R}^n\\) whose length/norm is 1 is called a unit vector. Any vector can be made into a unit vector through normalization by multiplying the vector \\(\\mathbf{v}\\) by \\(\\frac{1}{\\|\\mathbf{v}\\|}\\) to get a unit vector \\(\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\) in the same direction as \\(\\mathbf{v}\\). 23.1 Distance In two dimensions, the Euclidean distance between the points \\((x_1, y_1)\\) and \\((x_2, y_2)\\) is defined as \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\). In higher dimensions, a similar definition holds. Definition 23.4 Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors in \\(\\mathcal{R}^n\\). Then the distance \\(dist(\\mathbf{u}, \\mathbf{v})\\) between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is \\[\\begin{align*} dist(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\| \\end{align*}\\] Example 23.3 Distance between two 3-dimensional vectors u &lt;- c(3, -5, 1) v &lt;- c(4, 3, -2) sqrt(sum((u-v)^2)) ## [1] 8.602325 23.2 Orthogonal vectors The equivalent of perpendicular lines in \\(\\mathcal{R}^n\\) are known as orthogonal vectors. Definition 23.5 The two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathcal{R}^n\\) are orthogonal if \\[\\begin{align*} \\mathbf{u}&#39; \\mathbf{v} = 0 \\end{align*}\\] 23.3 Angles between vectors Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors \\(\\mathcal{R}^n\\). Then, the angle between the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is defined as the angle \\(\\theta\\) in the relationship \\[\\begin{align*} \\mathbf{u}&#39; \\mathbf{v} = \\| \\mathbf{u} \\| \\| \\mathbf{v} \\| cos(\\theta) \\end{align*}\\] see example: angles-as-n-gets-large.R 23.4 Orthogonal sets The set of vectors \\(\\mathcal{S} = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) in \\(\\mathcal{R}^n\\) is said to be an orthogonal set if every pair of vectors is orthogonal. In other words, for all \\(i \\neq j\\), \\(\\mathbf{v}_i&#39; \\mathbf{v}_j = 0\\). The set is called an orthonormal set if the set of vectors are orthogonal and for \\(i = 1, \\ldots, p\\), each vector \\(\\mathbf{v}_i\\) in the set has length \\(\\| \\mathbf{v}_i \\| = 1\\). Example 23.4 Show the set of vectors \\(\\left\\{ \\mathbf{v}_1 = \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\mathbf{v}_2 = \\begin{pmatrix} -\\frac{1}{2} \\\\ -2 \\\\ \\frac{7}{2} \\end{pmatrix}, \\mathbf{v}_3 = \\begin{pmatrix} -1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\right\\}\\) is orthogonal Show these are orthogonal using R If the set of vectors \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) are an orthogonal set, then the set of vectors \\(\\left\\{ \\frac{\\mathbf{v}_1}{\\|\\mathbf{v}_1\\|}, \\ldots, \\frac{\\mathbf{v}_p}{\\|\\mathbf{v}_p\\|} \\right\\}\\) is an orthonormal set. Note that for each \\(i\\), the length of the vector \\(\\frac{\\mathbf{v}_i} {\\|\\mathbf{v}_i \\|} = 1\\) Theorem 23.2 Let the set \\(\\mathcal{S} = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) be an orthogonal set of nonzero vectors in \\(\\mathcal{R}^n\\). Then, the set of vectors in \\(\\mathcal{S}\\) are linearly independent and therefore are a basis for the space spanned by \\(\\mathcal{S}\\). Proof. Assume the set of vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\) are linearly dependent. Then, there exist coefficients \\(c_1, \\ldots, c_p\\) such that \\[\\begin{align*} \\mathbf{0} &amp; = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\end{align*}\\] Then, multiplying both equations on the left by \\(\\mathbf{v}_1&#39;\\) gives \\[\\begin{align*} 0 = \\mathbf{v}_1&#39; \\mathbf{0} &amp; = \\mathbf{v}_1&#39; (c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p) \\\\ &amp; = c_1 \\mathbf{v}_1&#39; \\mathbf{v}_1 + c_2 \\mathbf{v}_1&#39; \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_1&#39; \\mathbf{v}_p \\\\ &amp; = c_1 \\mathbf{v}_1&#39; \\mathbf{v}_1 + c_2 0 + \\cdots + c_p 0 \\\\ &amp; = c_1 \\mathbf{v}_1&#39; \\mathbf{v}_1 \\end{align*}\\] which is only equal to 0 when \\(c_1\\) is equal to 0 because \\(\\mathbf{v}_1\\) is a nonzero vector. The above left multiplication could be repeated for each vector \\(\\mathbf{v}_i\\) which gives all \\(c_i\\) must equal 0. As the only solution to the starting equation has all 0 coefficients, the set of vectors \\(\\mathcal{S}\\) must be linearly independent. A set of orthogonal vectors is called an orthogonal basis. Theorem 23.3 Let \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) be an orthogonal basis of the subspace \\(\\mathcal{W}\\) of \\(\\mathcal{R}^n\\). Then for each \\(\\mathbf{x} \\in \\mathcal{W}\\), the coefficients for the linear combination of basis vectors \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) for the vector \\(\\mathbf{x}\\) are \\[\\begin{align*} \\mathbf{x} &amp; = \\frac{\\mathbf{x}&#39;\\mathbf{v}_1}{\\mathbf{v}_1&#39;\\mathbf{v}_1} \\mathbf{v}_1 + \\frac{\\mathbf{x}&#39;\\mathbf{v}_2}{\\mathbf{v}_2&#39;\\mathbf{v}_2} \\mathbf{v}_2 + \\cdots + \\frac{\\mathbf{x}&#39;\\mathbf{v}_p}{\\mathbf{v}_p&#39;\\mathbf{v}_p} \\mathbf{v}_p \\\\ &amp; = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\\\ \\end{align*}\\] where \\(c_j = \\frac{\\mathbf{x}&#39;\\mathbf{v}_j}{\\mathbf{v}_j&#39;\\mathbf{v}_j}\\). In other words, the coordinates of the vector \\(\\mathbf{x}\\) with respect to the orthogonal basis \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) are the linear projection of the vector \\(\\mathbf{x}\\) on the respective vectors \\(\\mathbf{v}_j\\). Proof. The orthogonality of the basis \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) gives \\[\\begin{align*} \\mathbf{x}&#39;\\mathbf{v}_j &amp; = \\left(c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\right)&#39; \\mathbf{v}_j \\\\ &amp; = c_j \\mathbf{v}_j&#39; \\mathbf{v}_j \\end{align*}\\] Because we know that \\(\\mathbf{v}_j&#39;\\mathbf{v}_j\\) is not zero (a vector can’t be orthogonal to itself), we can divide the above equality by \\(\\mathbf{v}_j&#39; \\mathbf{v}_j\\) and solve for \\(c_j = \\frac{\\mathbf{x}&#39;\\mathbf{v}_j}{\\mathbf{v}_j&#39;\\mathbf{v}_j}\\) Thus, for a vector \\(\\mathbf{x}\\) in the standard basis, the coordinates of \\(\\mathbf{x}\\) with respect to an orthogonal basis can be easily calculated using dot products (rather than matrix inverses) which is an easier computation. In fact, this is exactly the idea of using least squares estimation (linear regression, spline regression, etc.). 23.5 Orthogonal projections Definition 23.6 Let \\(\\mathbf{x}\\) be a vector in \\(\\mathcal{R}^n\\) and let \\(\\mathcal{W}\\) be a subspace of \\(\\mathcal{R}^n\\). Then the vector \\(\\mathbf{x}\\) can be written as the orthogonal decomposition \\[\\begin{align*} \\mathbf{x} = \\mathbf{x}_{\\mathcal{W}} + \\mathbf{x}_{\\mathcal{W}^\\perp} \\end{align*}\\] where \\(\\mathbf{x}_{\\mathcal{W}}\\) is the vector in \\(\\mathcal{W}\\) that is closest to \\(\\mathbf{x}\\) and is called the orthogonal projection of \\(\\mathbf{x}\\) onto \\(\\mathcal{W}\\) and \\(\\mathbf{x}_{\\mathcal{W}^\\perp}\\) is the orthogonal projection of \\(\\mathbf{x}\\) onto \\(\\mathcal{W}^{\\perp}\\), the subspace \\(\\mathcal{W}^\\perp\\) of \\(\\mathcal{R}^n\\) that is complementary to \\(\\mathcal{W}\\) and is called the orthogonal complement. Draw picture in class - W is a plane, orthogonal projection of a vector onto the plane This leads to the projection theorem that decomposes a vector \\(\\mathbf{x} \\in \\mathcal{R}^n\\) into components that are Theorem 23.4 Let \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) be an orthogonal basis of the subspace \\(\\mathcal{W}\\) of \\(\\mathcal{R}^n\\). Then for each \\(\\mathbf{x} \\in \\mathcal{R}^n\\), the orthogonal projection of \\(\\mathbf{x}\\) onto \\(\\mathcal{W}\\) is given by \\[\\begin{align*} \\mathbf{x}_{\\mathcal{W}} &amp; = \\frac{\\mathbf{x}&#39;\\mathbf{v}_1}{\\mathbf{v}_1&#39;\\mathbf{v}_1} \\mathbf{v}_1 + \\frac{\\mathbf{x}&#39;\\mathbf{v}_2}{\\mathbf{v}_2&#39;\\mathbf{v}_2} \\mathbf{v}_2 + \\cdots + \\frac{\\mathbf{x}&#39;\\mathbf{v}_p}{\\mathbf{v}_p&#39;\\mathbf{v}_p} \\mathbf{v}_p \\\\ &amp; = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_p \\mathbf{v}_p \\\\ \\end{align*}\\] where the coefficient \\(c_j\\) corresponding to the vector \\(\\mathbf{v}_j\\) of the linear combination of vectors \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) is given by \\(c_j = \\frac{\\mathbf{x}&#39;\\mathbf{v}_j}{\\mathbf{v}_j&#39;\\mathbf{v}_j}\\). In other words, the coordinates of the vector \\(\\mathbf{x}\\) with respect to the orthogonal basis \\(\\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_p \\}\\) are the linear projection of the vector \\(\\mathbf{x}\\) on the respective vectors \\(\\mathbf{v}_j\\). You might be wondering what use orthogonal projections are. In fact, linear regression (and most common regression models) use orthogonal projections to fit a line (or surface) of best fit. This leads to the important theorem that allows us to project a vector \\(\\mathbf{y} \\in \\mathcal{R}^n\\) onto the column space of a \\(n \\times p\\) matrix \\(\\mathbf{X}\\) (which is exactly the linear regression of \\(\\mathbf{y}\\) onto \\(\\mathbf{X}\\)). Theorem 23.5 (Orthogonal Projection Theorem) Let \\(\\mathbf{X}\\) be a \\(n \\times p\\) matrix, let \\(\\mathcal{W} =\\) col(\\(\\mathbf{X}\\)), and let \\(\\mathbf{y}\\) be a vector in \\(\\mathcal{R}^n\\). Then the matrix equation \\[\\begin{align*} \\mathbf{X}&#39;\\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}&#39; \\mathbf{y} \\end{align*}\\] with respect to the unknown coefficients \\(\\boldsymbol{\\beta}\\) is consistent and \\(\\mathbf{y}_{\\mathcal{W}} = \\mathbf{X}\\boldsymbol{\\beta}\\) for any solution \\(\\boldsymbol{\\beta}\\). Proof. In addition, if the columns of \\(\\mathbf{X}\\) are linearly independent, then the coefficients \\(\\boldsymbol{\\beta}\\) are given by \\[\\begin{align*} \\boldsymbol{\\beta} = \\left( \\mathbf{X}&#39;\\mathbf{X} \\right)^{-1} \\mathbf{X}&#39; \\mathbf{y} \\end{align*}\\] which is the least squares solution to the linear regression problem. For example, let X and y be defined as below X &lt;- cbind(1, c(2, -1, 3, -4, 5, 7, -2, 3)) y &lt;- c(5, 3, 4, -9, 11, 12, -5, 6) Plotting this data shows the strong positive linear relationship # The first column is a basis for a constant term (the intercept) data.frame(x = X[, 2], y = y) %&gt;% ggplot(aes(x = x, y = y)) + geom_point() We can solve for the coefficients beta using the linear project theorem beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y and using this solution, solve for the projection \\(\\mathbf{y}_{\\mathcal{W}}\\) of \\(\\mathbf{y}\\) onto \\(\\mathbf{X}\\) y_W &lt;- X %*% beta Plotting the projection \\(\\mathbf{y}_{\\mathcal{W}}\\) gives # The first column is a basis for a constant term (the intercept) data.frame(x = X[, 2], y = y, y_W = y_W) %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = y_W)) The complement of the project (called the residuals in statistics) is given by \\(\\mathbf{y}_{\\mathcal{W}^\\perp} = \\mathbf{y} - \\mathbf{y}_{\\mathcal{W}}\\) y_W_perp &lt;- y - y_W and can be visualized as the orthogonal projection using segments # The first column is a basis for a constant term (the intercept) data.frame(x = X[, 2], y = y, y_W = y_W) %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = y_W)) + geom_segment(aes(x = x, y = y_W, xend = x, yend = y_W + y_W_perp), color = &quot;blue&quot;) Recall that the orthogonal projection gives the “closest” vector \\(\\mathbf{y}_\\mathcal{W}\\) to \\(\\mathbf{y}\\) that is in the subspace \\(\\mathcal{W}\\) that is the span of the column space of \\(\\mathbf{X}\\). See https://www.enchufa2.es/archives/least-squares-as-springs-the-shiny-app.html for an example. "],["section-graphs-and-limits.html", "Chapter 24 Graphs and Limits 24.1 Graphs and level curves 24.2 Limits 24.3 Continuity", " Chapter 24 Graphs and Limits library(tidyverse) library(dasc2594) set.seed(2021) Here we start a transition to topics in vector calculus. We will start with a discussion of functions of two variables (although the functions are not assumed to be linear). We define a function of two variables explicitly as \\(z = f(x, y)\\). Definition 24.1 Like with linear functions, we can define the domain and range for general functions of two variables. A function \\(f(x, y)\\) assigns each point \\((x, y)\\) in some domain \\(\\mathcal{D}\\) in \\(\\mathcal{R}^2\\) to a unique number \\(z\\) in a subset of \\(\\mathcal{R}\\). The set of inputs \\(\\mathcal{D}\\) is called the domain of the function and the range is the set of real numbers \\(z\\) that are the output of the function over all the inputs in \\(\\mathcal{D}\\) Example 24.1 Let \\(f(x, y) = \\sqrt{1 - x^2 - y^2}\\). The domain of \\(f\\) is the set of points \\((x, y)\\) such that \\(x^2 + y^2 \\leq 1\\) which is the unit circle (draw picture). The range is the unit interval \\([0, 1]\\) 24.1 Graphs and level curves Example 24.2 The parabola Consider the function of two variables \\[\\begin{align*} f(x, y) = x^2 + y^2 \\end{align*}\\] which defines the surface The 3-dimensional surface can be represented in 2-dimensions using level curves (think of a topographic map) data.frame(expand.grid(x, y)) %&gt;% rename(x = Var1, y = Var2) %&gt;% mutate(z = parabola(x, y)) %&gt;% ggplot(aes(x = x, y = y, z = z)) + geom_contour() + coord_fixed(ratio = 1) where each curve in the (x, y) plane has exactly the same value of \\(f(x, y)\\). Alternatively, this can be represented using filled level curves data.frame(expand.grid(x, y)) %&gt;% rename(x = Var1, y = Var2) %&gt;% mutate(z = parabola(x, y)) %&gt;% ggplot(aes(x = x, y = y, z = z)) + geom_contour_filled() + coord_fixed(ratio = 1) Notice that although the original parabola was continuous, these 2-d representations simplify the diagram by representing the contours as discrete values. Example 24.3 A saddle function Consider the function of two variables \\[\\begin{align*} f(x, y) = x^2 - y^2 \\end{align*}\\] which defines the surface The 3-dimensional surface can be represented in 2-dimensions using level curves (think of a topographic map) data.frame(expand.grid(x, y)) %&gt;% rename(x = Var1, y = Var2) %&gt;% mutate(z = saddle(x, y)) %&gt;% ggplot(aes(x = x, y = y, z = z)) + geom_contour() + coord_fixed(ratio = 1) where each curve in the (x, y) plane has exactly the same value of \\(f(x, y)\\). Alternatively, this can be represented using filled level curves data.frame(expand.grid(x, y)) %&gt;% rename(x = Var1, y = Var2) %&gt;% mutate(z = saddle(x, y)) %&gt;% ggplot(aes(x = x, y = y, z = z)) + geom_contour_filled() + coord_fixed(ratio = 1) Notice that although the original saddle was continuous, these 2-d representations simplify the diagram by representing the contours as discrete values. 24.2 Limits For functions of several variables, we have to define limits and continuity for these multivariable settings. For now, we focus on two variable functions as the multivariable case follows similar from the two variable case. Let \\(P(x, y) \\rightarrow P_0(a, b)\\) be a path in the \\(x-y\\) plane that starts at the point \\(P(x, y)\\) and ends at the point \\(P_0(a, b)\\) with coordinates \\((a, b)\\). Thus, we can understand the limit as the fixed value of \\(f(x, y)\\) for which all paths that connect the points \\(P(x, y)\\) that are “close” to \\(P_0(a, b)\\) converge to. For one-dimensional limits, “close” was defined as distance. Thus, for multivariable functions, “close” is defined as the Euclidean distance defined by a “ball” of radius \\(\\delta\\) and the limits examines the function output as the radius \\(\\delta\\) goes to 0. Recall that the distance \\(dist((x, y), (a, b))\\) between two points \\((x, y)\\) and \\((a, b)\\) is \\[\\begin{align*} dist((x, y), (a, b)) = \\sqrt{(x-a)^2 + (y-b)^2} \\end{align*}\\] ** Draw images** Definition 24.2 (Limit of a function of two variables) The function f(x, y) has limit \\(L\\) as \\(P(x, y) \\rightarrow P_0(a, b)\\), written \\[\\begin{align*} \\lim_{(x, y) \\rightarrow (a, b)} f(x, y) = \\lim_{P \\rightarrow P_0} f(x, y) = L, \\end{align*}\\] if, for any \\(\\epsilon &gt; 0\\) (the radius of the ball that defines the “closeness” of the point), there exists a \\(\\delta &gt; 0\\) such that \\[\\begin{align*} |f(x, y) - L| &lt; \\epsilon \\end{align*}\\] whenever \\((x, y)\\) is in the domain of \\(f\\) and \\[\\begin{align*} 0 &lt; \\sqrt{(x-a)^2 + (y-b)^2} &lt; \\delta. \\end{align*}\\] In the definition, as the value of \\(\\delta\\) is getting smaller, the distance between the set of points \\(P(x, y)\\) within radius \\(\\delta\\) of the point \\(P_0(a, b)\\) is getting smaller. As a consequence, the limit in the definition above exists only if \\(f(x, y)\\) approaches the value \\(L\\) along all possible paths in the domain of \\(f\\). Example 24.4 In class notes * Future work: write out hand-written examples Theorem 24.1 Let \\(L\\) and \\(M\\) be real numbers and let \\(\\lim_{(x, y) \\rightarrow (a, b)}f(x, y) = L\\) and \\(\\lim_{(x, y) \\rightarrow (a, b)}g(x, y) = M\\) for functions \\(f(x, y)\\) and \\(g(x, y)\\). Let \\(c\\) be a constant and \\(n&gt;0\\), then: Sum of limits: \\[\\begin{align*} \\lim_{(x, y) \\rightarrow (a, b)} \\left( f(x, y) + g(x, y) \\right) = L + M \\end{align*}\\] Difference of limits: \\[\\begin{align*} \\lim_{(x, y) \\rightarrow (a, b)} \\left( f(x, y) - g(x, y) \\right) = L - M \\end{align*}\\] Scalar multiple of the limit: \\[\\begin{align*} \\lim_{(x, y) \\rightarrow (a, b)} c f(x, y) = c L \\end{align*}\\] Product of limits: \\[\\begin{align*} \\lim_{(x, y) \\rightarrow (a, b)} f(x, y) g(x, y) = LM \\end{align*}\\] Quotient of limits: As long as \\(M&gt;0\\) we have \\[\\begin{align*} \\lim_{(x, y) \\rightarrow (a, b)} \\frac{f(x, y)}{g(x, y)} = \\frac{L}{M} \\end{align*}\\] Power of the limit: \\[\\begin{align*} \\lim_{(x, y) \\rightarrow (a, b)} f(x, y)^n = L^n \\end{align*}\\] Root of the limit: If \\(n\\) is even, we assume \\(L &gt; 0\\) \\[\\begin{align*} \\lim_{(x, y) \\rightarrow (a, b)} f(x, y)^{1/n} = L^{1/n} \\end{align*}\\] Example 24.5 Use the rules above to evaluate the limit \\(\\lim_{(x, y) \\rightarrow (2, 3)} 4x^3y + \\sqrt{xy}\\) 24.2.1 Boundary points Definition 24.3 Define a region \\(\\mathcal{D}\\) in \\(\\mathcal{R}^2\\). An interior point \\(P\\) of \\(\\mathcal{D}\\) is a point that lies entirely in the region \\(\\mathcal{D}\\). Mathematically, a point \\(P\\) is an interior point of \\(\\mathcal{D}\\) if it is possible to define a ball of radius \\(\\epsilon&gt;0\\) centered at \\(P\\) such that this ball only contains points within \\(\\mathcal{D}\\). A boundary point \\(P\\) of \\(\\mathcal{D}\\) is a point that lies on the edge of the region \\(\\mathcal{D}\\). Mathematically, a point \\(P\\) is an boundary point of \\(\\mathcal{D}\\) if every ball of radius \\(\\epsilon&gt;0\\) centered at \\(P\\) contains at least one point in \\(\\mathcal{D}\\) and one point outside \\(\\mathcal{D}\\). Example 24.6 Let \\(f(x, y) = \\sqrt{1 - x^2 - y^2}\\). The boundary points are all the points on the unit circle an the interior points are all the points on the interior of the unit disk. draw picture Definition 24.4 A region \\(\\mathcal{D}\\) is said to be open if it only contains interior points (i.e., \\(\\mathcal{D}\\) has no boundary points). A region \\(\\mathcal{D}\\) is said to be closed if the region contains all its boundary points. Figure: limit paths along the boundary Example 24.7 Consider the \\(\\lim_{(x, y) \\rightarrow (4, 4)} \\frac{x^2 - y^2}{x - y}\\). Because the point \\((4, 4)\\) is not a valid point in the domain (can’t divide by \\(4-4=0\\)), the point \\((4, 4)\\) is a boundary point of the domain. The boundary of the domain that is not contained in the domain is the set of points \\(x = y\\). Assuming we are not taking a path along the boundary, we know that \\(x \\neq y\\) in the interior of the domain. Hence, \\[\\begin{align*} \\lim_{(x, y) \\rightarrow (4, 4)} \\frac{x^2 - y^2}{x - y} &amp; = \\lim_{(x, y) \\rightarrow (4, 4)} \\frac{(x - y)(x + y)}{x - y} \\\\ &amp; = \\lim_{(x, y) \\rightarrow (4, 4)} x + y = 4 + 4 = 8\\\\ \\end{align*}\\] for all paths that do not cross the line \\(y = x\\). Example 24.8 nonexistence of limit in class 24.3 Continuity A very important property of functions is continuity. In a general sense, a function is continuous if two nearby input values result in nearby output values. As a graph, this means that there are no hops, skips, or jumps. Definition 24.5 The function \\(f(x, y)\\) is said to be continuous at the point \\((a, b)\\) if the following are true 1) \\(f(a, b)\\) is defined at \\((a, b)\\) 2) \\(\\lim_{(x, y) \\rightarrow (a, b)} f(x, y)\\) exists 2) \\(\\lim_{(x, y) \\rightarrow (a, b)} f(x, y) = f(a, b)\\) Example 24.9 checking continuity in class \\(f(x, y) = \\begin{cases} \\frac{x^2 - y^2}{x - y} &amp; \\mbox{ if } x \\neq y \\\\ x + y &amp; \\mbox{ if } x = y \\\\ \\end{cases}\\) "],["section-partial-derivatives.html", "Chapter 25 Partial Derivatives 25.1 Higher-order partial derivatives", " Chapter 25 Partial Derivatives library(tidyverse) library(plotly) library(dasc2594) set.seed(2021) Recall that for a function of one variable, the derivative gives the rate of change of the function with respect to that variable. The function \\(f(x)\\) has an instantaneous rate of change \\(\\frac{d}{dx}f(x)\\), assuming the derivative \\(\\frac{d}{dx}f(x)\\) exists. This concept can be extended to functions of multivariables where we now have to specify a direction in which the function changes. For example, consider a mountain which is very steep in the north/south direction but is much less steep in the east/west direction. Thus, the directional derivative in the north/south direction will have a larger absolute value (higher rate of change) than the directional derivative in the east/west direction. For example, consider the function mountain &lt;- function(x, y) { 4 - 9 * x^2 - y^2 } dat &lt;- expand_grid(x = seq(-4, 4, length.out = 20), y = seq(-4, 4, length.out = 20)) %&gt;% mutate(z = mountain(x, y)) dat %&gt;% ggplot(aes(x = x, y = y, z = z)) + geom_contour() + coord_fixed(ratio = 1) plot_ly(z = matrix(dat$z, 20, 20)) %&gt;% add_surface( contours = list( z = list( show=TRUE, usecolormap=TRUE, highlightcolor=&quot;#ff0000&quot;, project=list(z=TRUE) ) ) ) Note that the partial derivative asks the question “What is the rate of change of one variable holding all the other variables constant?” Thus, the question can be phrased as what is the derivative of the function \\(f(x, y)\\) at the point \\((a, b)\\) where we only let one variable change? To make the notation of a partial derivative clear, a special symbol is used where \\(\\frac{\\partial}{\\partial x}\\) is the partial derivative with respect to the \\(x\\) variable (holding the y variable constant). draw surfaces with marginal slices Definition 25.1 The partial derivative of the function \\(f(x, y)\\) with respect to \\(x\\) at the point \\((a, b)\\) is \\[\\begin{align*} \\frac{\\partial}{\\partial x} f(x, y) = f_x(x, y) &amp; = \\lim_{h \\rightarrow 0} \\frac{f(a + h, b) - f(a, b)} {h}. \\end{align*}\\] The partial derivative of the function \\(f(x, y)\\) with respect to \\(y\\) at the point \\((a, b)\\) is \\[\\begin{align*} \\frac{\\partial}{\\partial y} f(x, y) = f_y(x, y) &amp; = \\lim_{h \\rightarrow 0} \\frac{f(a, b + h) - f(a, b)} {h}, \\end{align*}\\] as long as these limits exist. Example 25.1 partial derivatives using limit definition Example 25.2 Let \\(f(x, y) = 3x^2 - 4 y^3 + 3\\), Compute \\(\\frac{\\partial}{\\partial x}f(x, y)\\) and \\(\\frac{\\partial}{\\partial y}f(x, y)\\). Then evaluate each derivative at \\((-2, 3)\\). Notice that you can find partial derivatives by holding all the other variables constant and then finding the equivalent univariate derivative. 25.1 Higher-order partial derivatives We can calculate the partial derivatives of partial derivatives. The derivatives could be with respect to the same variable repeatedly or the derivatives could be with respect to different variables in which case we call these mixed partial derivatives. Notation for higher order partial derivatives is \\(\\frac{\\partial^2}{\\partial x \\partial y} f(x, y) = f_{xy}(x, y)\\) which says first take the partial derivative of with respect to \\(y\\) then take the partial derivative with respect to \\(x\\). The possible sets of second-order partial derivatives for functions of two variables are shown in the table below Notation 1 Notation 2 \\(\\frac{\\partial}{\\partial x}\\frac{\\partial}{\\partial x} f(x, y) = \\frac{\\partial^2}{\\partial x^2} f(x, y)\\) \\(f_{xx}(x, y)\\) \\(\\frac{\\partial}{\\partial y}\\frac{\\partial}{\\partial y} f(x, y) = \\frac{\\partial^2}{\\partial y^2} f(x, y)\\) \\(f_{yy}(x, y)\\) \\(\\frac{\\partial}{\\partial x}\\frac{\\partial}{\\partial y} f(x, y) = \\frac{\\partial^2}{\\partial x \\partial y} f(x, y)\\) \\(f_{xy}(x, y)\\) \\(\\frac{\\partial}{\\partial y}\\frac{\\partial}{\\partial x} f(x, y) = \\frac{\\partial^2}{\\partial y \\partial x} f(x, y)\\) \\(f_{yx}(x, y)\\) Example 25.3 Find the four second-order partial derivatives of \\(f(x, y) = 3x^2y^3 + 4xy - 3x^2\\) Note that the order in which mixed partial derivatives are taken can sometimes change the result. However, it is often the case that the order of the partial derivatives can be switched. Theorem 25.1 Let the function \\(f(x, y)\\) be defined on an open domain \\(\\mathcal{D}\\) of \\(\\mathcal{R}^2\\) and assume that \\(f_{xy}\\) and \\(f_{yx}\\) are continuous over the domain \\(\\mathcal{D}\\). Then, \\(f_{xy} = f_{yx}\\) for all points in the domain \\(\\mathcal{D}\\). Many of the commonly used functional forms in data science meet the criteria above. Thus, for many of the commonly used functions in data science, the order of evaluation of partial derivatives often does not matter. In practice, it is always good practice to verify this though. "],["section-the-chain-rule.html", "Chapter 26 The chain rule 26.1 The chain rule with one independent variable 26.2 The chain rule with several independent variables 26.3 The chain rule in matrix notation", " Chapter 26 The chain rule library(tidyverse) library(plotly) library(dasc2594) set.seed(2021) Recall the univariate chain rule: If \\(y = f(x)\\) is a function of \\(x\\) and \\(z = g(y)\\) is a function of \\(y\\), a question of interest is \"What is the change in \\(z\\) relative to change in \\(x\\)? We can write \\(z = g(y) = g(f(x))\\) and using this notation, the change in \\(z\\) with respect to the variable \\(x\\) is \\(\\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx} = \\frac{df(y)}{dy}\\frac{dg(x)}{dx}\\). Written in functional form \\[\\begin{align*} (g(f(x)))&#39; = (g \\circ f)&#39;(x) = g&#39;(f(x)) f&#39;(x) \\end{align*}\\] Example 26.1 Let \\(z = f(y) = y^3\\) and let \\(y = g(x) = e^{x}\\) what is \\(\\frac{dz}{dx}\\)? 26.1 The chain rule with one independent variable Drawing in class Definition 26.1 (Chain rule for one independent variable) Let \\(z\\) be a differentiable function of two variables \\(x\\) and \\(y\\) so that \\(z = f(x, y)\\) and let \\(x=g(t)\\) be a function of \\(t\\) and \\(y=h(t)\\) a function of \\(t\\). Written in functional form, \\(z\\) can be written as \\(z = f(x, y) = f(g(t), h(t))\\), with \\(x=g(t)\\) and \\(y=h(t)\\). Then we can define the derivative of \\(z\\) with respect to \\(t\\) as \\[\\begin{align*} \\frac{dz}{dt} = \\frac{\\partial z}{ \\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt} \\end{align*}\\] For the definition above, we have the dependent variable \\(z\\) and we have intermediate variables \\(x\\) and \\(y\\). Notice in the definition above that there is a mix of partial derivatives (\\(\\partial\\)) and ordinary derivatives (\\(d\\)). Example 26.2 Let \\(z = x^2 + e^y\\) and let \\(x = \\cos(t)\\) and \\(y = \\sin(t)\\) The results from above can also be extended to have more than two intermediate variables. Draw picture 26.2 The chain rule with several independent variables Often, functions will have more than one independent variables. Definition 26.2 (Chain rule for two independent variables) Let \\(z\\) be a differentiable function of two variables \\(x\\) and \\(y\\) so that \\(z = f(x, y)\\) and let \\(x=g(t, s)\\) be a function of \\(s\\) and \\(t\\) and \\(y=h(s, t)\\) a function of \\(s\\) and \\(t\\). Written in functional form, \\(z\\) can be written as \\(z = f(x, y) = f(g(s, t), h(s, t))\\), with \\(x=g(s, t)\\) and \\(y=h(s, t)\\). Then we can define the partial derivative of \\(z\\) with respect to \\(s\\) as \\[\\begin{align*} \\frac{\\partial z}{\\partial s} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial s} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial s} \\end{align*}\\] the partial derivative of \\(z\\) with respect to \\(t\\) as \\[\\begin{align*} \\frac{\\partial z}{\\partial t} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t} \\end{align*}\\] Example 26.3 Let \\(z = f(x, y) = x^2 e^y\\) and let \\(x = 2s - t\\) and \\(y = 4s^3-3t^2\\) 26.3 The chain rule in matrix notation To get a better understanding of the chain rule, it helps to show the chain rule using matrix notation. Using the matrix notation will enable you to apply the chain rule to any number of intermediate variables. For example, consider the extension of the definition for the chain rule of a function with one independent variable. Definition 26.3 (Matrix chain rule for one independent variable) Let \\(z\\) be a differentiable function of two variables \\(x\\) and \\(y\\) so that \\(z = f(x, y)\\) and let \\(x=g(t)\\) be a function of \\(t\\) and \\(y=h(t)\\) a function of \\(t\\). Written in functional form, \\(z\\) can be written as \\(z = f(x, y) = f(g(t), h(t))\\), with \\(x=g(t)\\) and \\(y=h(t)\\). Then we can define the derivative of \\(z\\) with respect to \\(t\\) as \\[\\begin{align*} \\frac{dz}{dt} = \\frac{\\partial z}{ \\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt} \\end{align*}\\] Written in matrix notation, this is \\[\\begin{align*} \\frac{dz}{dt} = \\begin{pmatrix} \\frac{\\partial z}{ \\partial x} &amp; \\frac{\\partial z}{\\partial y} \\end{pmatrix} \\begin{pmatrix} \\frac{dx}{dt} \\\\ \\frac{dy}{dt} \\end{pmatrix} = \\frac{\\partial z}{ \\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt} \\end{align*}\\] The definition above for the chain rule with two variables is given by Definition 26.4 (Chain rule for two independent variables) Let \\(z\\) be a differentiable function of two variables \\(x\\) and \\(y\\) so that \\(z = f(x, y)\\) and let \\(x=g(t, s)\\) be a function of \\(s\\) and \\(t\\) and \\(y=h(s, t)\\) a function of \\(s\\) and \\(t\\). Written in functional form, \\(z\\) can be written as \\(z = f(x, y) = f(g(s, t), h(s, t))\\), with \\(x=g(s, t)\\) and \\(y=h(s, t)\\). Then we can define the partial derivative of \\(z\\) with respect to \\(s\\) as \\[\\begin{align*} \\frac{\\partial z}{\\partial s} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial s} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial s} \\end{align*}\\] which, in matrix notation is \\[\\begin{align*} \\frac{dz}{dt} = \\begin{pmatrix} \\frac{\\partial z}{ \\partial x} &amp; \\frac{\\partial z}{\\partial y} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial x}{\\partial s} \\\\ \\frac{\\partial y}{\\partial s} \\end{pmatrix} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial s} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial s} \\end{align*}\\] The partial derivative of \\(z\\) with respect to \\(t\\) as \\[\\begin{align*} \\frac{\\partial z}{\\partial t} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t} \\end{align*}\\] which, in matrix notation is \\[\\begin{align*} \\frac{dz}{dt} = \\begin{pmatrix} \\frac{\\partial z}{ \\partial x} &amp; \\frac{\\partial z}{\\partial y} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial x}{\\partial t} \\\\ \\frac{\\partial y}{\\partial t} \\end{pmatrix} = \\frac{\\partial z}{ \\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t} \\end{align*}\\] This use of matrix notation for derivatives will be useful in understanding the gradient. "],["section-the-gradient-and-directional-derivatives.html", "Chapter 27 The gradient and directional derivatives 27.1 The Gradient", " Chapter 27 The gradient and directional derivatives Partial derivatives tell about how a rate of function changes in a particular direction (in the direction of a coordinate). Think about trying to find the maximum of a real-valued function (finding the minimum is equivalent to finding the maximum of the negative value of the function). Finding the maximum of a function is analogous to hiking up a mountain and trying to find the highest peak. Suppose you are standing on a mountain surface at the point \\((x, y, z)\\) in 3-dimensions where \\(z = f(x, y)\\) is the function that gives the height of the mountain at location \\((x, y)\\). If you are standing at the point \\((a, b)\\) in the \\((x, y)\\) coordinate system, you might want to get to the top of the mountain as quickly as possible. The direction that is the steepest uphill direction can be calculated using the concepts of the directional derivative and the gradient. Definition 27.1 (Directional Derivative) Given a function \\(f(x,y)\\) that is differentiable at \\((a, b)\\) and a unit vector \\(\\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}\\) in the \\(x-y\\) plane, the directional derivative of \\(f\\) at \\((a, b)\\) in the direction of \\(\\mathbf{v}\\) is \\[\\begin{align*} D_{\\mathbf{v}} f(a, b) = \\lim_{h \\rightarrow 0} \\frac{f(a + h v_1, b + h v_2) - f(a, b)}{h} \\end{align*}\\] assuming the limit exists. To understand how the directional derivative relates to partial derivatives, in the definition above, let \\(v_2 = 0\\) and to make \\(\\mathbf{v}\\) a unit vector, set \\(v_1 = 1\\) (\\(\\mathbf{v}\\) is the standard basis vector \\(\\mathbf{e}_1\\)). Then, the limit in the directional derivative definition above becomes \\[\\begin{align*} \\lim_{h \\rightarrow 0} \\frac{f(a + h, b) - f(a, b)}{h} \\end{align*}\\] which is the definition for the partial derivative of \\(f\\) in the \\(x\\) direction \\(f_x = \\frac{\\partial f}{\\partial x}\\) (Definition 25.1). Likewise, letting \\(\\mathbf{v} = \\mathbf{e}_2\\), the standard basis vector in the y direction, gives the partial derivative in the y direction \\(f_y = \\frac{\\partial f}{\\partial y}\\). Thus, one could pick any direction vector \\(\\mathbf{v}\\) and then calculate the partial derivative in that direction. Now observe that any line that goes through the point \\((a, b)\\) in the direction of the unit vector \\(\\mathbf{v} \\in \\mathcal{R}^2\\) can be written as the set of points \\(\\{(x = a + s v_1, y = b + s v_2) | s \\in \\mathcal{R} \\}\\) which forms a line through the point \\((a, b)\\) in the direction of \\(\\mathbf{v}\\). In this definition, the value \\(s\\) determines the length of the vector because \\(\\mathbf{v}\\) is a unit vector. At \\(s=0\\), this definition corresponds to the point \\((a, b)\\) and as \\(s\\) increases, the points \\((x, y)\\) are the set of points along the line that are distance \\(|s|\\) away from \\((a, b)\\). Notice that this set defines a function \\(g(s) = f(a + s v_1, b + s v_2) = f(x, y)\\) which is a single variable function of the two inputs \\(x\\) and \\(y\\) of \\(f(x, y)\\). Given this definition, the directional derivative of \\(f(x, y)\\) in the direction of \\(\\mathbf{v}\\) at the point \\((a, b)\\) is now given by \\[\\begin{align*} D_{\\mathbf{v}} f(a, b) &amp; = \\frac{d}{ds}g(s)|_{s=0} \\\\ &amp; = \\frac{\\partial f}{\\partial x} \\frac{dx}{ds} + \\frac{\\partial f}{\\partial y} \\frac{dy}{ds} |_{s=0} \\\\ &amp; = f_x(a, b) v_1 + f_y(a, b) v_2 \\\\ &amp; = \\begin{pmatrix} f_x(a, b) &amp; f_y(a, b) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\\\ \\end{align*}\\] which is the dot product of the vectors \\(\\begin{pmatrix} f_x(a, b) \\\\ f_y(a, b) \\end{pmatrix}\\) and \\(\\mathbf{v}\\). Notice that the vector \\(\\mathbf{v}\\) is a unit vector and therefore the directional derivative is a weighted sum of the partial derivatives in the \\(x\\) and \\(y\\) directions weighted by the vector \\(\\mathbf{v}\\) (weighted sums are sums where the coefficients sum to 1–in this case the sum is in the “distance” metric). As a consequence, we can find the directional derivative in any direction by changing the vector \\(\\mathbf{v}\\). Definition 27.2 Let \\(f(x, y)\\) be a differentiable function at \\((a, b)\\) and \\(\\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}\\) a unit vector in the \\(xy\\) plane. Then, the directional derivative of \\(f\\) at \\((a, b)\\) in the direction of \\(\\mathbf{v}\\) is \\[\\begin{align*} D_{\\mathbf{v}} f(a, b) &amp; = \\begin{pmatrix} \\frac{\\partial f(x, y)}{\\partial x}|_{(x,y) = (a, b)} &amp; \\frac{\\partial f(x, y)}{\\partial y}|_{(x,y) = (a, b)} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\\\ \\end{align*}\\] Example 27.1 Compute the partial derivative of \\(f(x, y) = 3x^2 + y^2\\) in the direction of \\(\\mathbf{u} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}}, - \\frac{\\sqrt{2}}{\\sqrt{3}} \\end{pmatrix}\\) and \\(\\mathbf{v} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}, - \\frac{1}{\\sqrt{2}} \\end{pmatrix}\\) at the point \\((3, 1)\\). calculate the directional derivatives graph the directional derivatives using contour plots and segments 27.1 The Gradient The directional derivative is a dot product of the partial derivatives and a unit vector. The gradient is similar, but rather than return a single value (a number), the gradient returns a vector at a point \\((a, b)\\). Definition 27.3 (The Gradient) Let \\(f(x, y)\\) be a differentiable function at \\((a, b)\\). Then, the gradient of \\(f\\) at \\((a, b)\\) is \\[\\begin{align*} \\nabla f(a, b) &amp; = \\begin{pmatrix} \\frac{\\partial f(x, y)}{\\partial x}|_{(x,y) = (a, b)} &amp; \\frac{\\partial f(x, y)}{\\partial y}|_{(x,y) = (a, b)} \\end{pmatrix} \\\\ &amp; = \\frac{\\partial f(x, y)}{\\partial x}|_{(x,y) = (a, b)} \\mathbf{e}_1 + \\frac{\\partial f(x, y)}{\\partial y}|_{(x,y) = (a, b)} \\mathbf{e}_2, \\end{align*}\\] where \\(\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\mathbf{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\) are the standard basis vectors in \\(\\mathcal{R}^2\\). Notice that the directional derivative at the point \\((a , b)\\) can be calculated using the gradient where \\[\\begin{align*} D_{\\mathbf{v}} f(a, b) &amp; = \\nabla f(a, b) \\cdot \\mathbf{v} \\\\ &amp; = \\begin{pmatrix} \\frac{\\partial f(x, y)}{\\partial x}|_{(x,y) = (a, b)} &amp; \\frac{\\partial f(x, y)}{\\partial y}|_{(x,y) = (a, b)} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\end{align*}\\] the directional derivative is the dot product of the gradient \\(\\nabla f(a, b)\\) at the point \\((a, b)\\) with the unit vector \\(\\mathbf{v}\\). Example 27.2 Compute the gradient of \\(f(x, y) = 3x^2 + y^2\\) at the point \\((3, 1)\\). calculate the gradient graph the gradient using contour plots and segments The gradient is critical in data science because is the tool that allows for finding the set of parameters for a given model that are “most likely” given the data. The gradient has the property in that at each point \\((a, b)\\) where \\(f(x, y)\\) is differentiable, the gradient points in the direction of the maximum rate of change. Theorem 27.1 (The gradient and rates of change) Let \\(f(x, y)\\) be a differentiable function at \\((a, b)\\) with \\(\\nabla f(a, b) \\neq 0\\). Then, \\(f\\) has its maximum rate of increase at the point \\((a, b)\\) in the direction of the gradient \\(\\nabla f(a, b)\\). Because the gradient is a weighted sum of the partial derivatives and the unit vector in the direction of the maximum change, the magnitude of the rate of change is \\(\\|\\nabla f(a, b)\\|\\) which is the length of the gradient vector. \\(f\\) has its maximum rate of decrease at the point \\((a, b)\\) in the direction of the gradient \\(-\\nabla f(a, b)\\). The rate of change in the direction of maximum rate of decrease is \\(-\\|\\nabla f(a, b)\\|\\). The directional derivative is 0 in any direction orthogonal to \\(\\nabla f(a, b)\\). "],["section-vectors-and-matrices.html", "Chapter 28 Vectors and matrices 28.1 Exercises", " Chapter 28 Vectors and matrices This can produce an error, so we wrap the function array_to_latex in a call to cat() A &lt;- matrix(c(3,4,5,6,7,9,4,5,122), ncol=3, byrow=TRUE) array_to_latex(A) [1] “\\begin{pmatrix} 3 &amp; 4 &amp; 5 \\\\ 6 &amp; 7 &amp; 9 \\\\ 4 &amp; 5 &amp; 122 \\end{pmatrix}” A &lt;- matrix(c(3,4,5,6,7,9,4,5,122), ncol=3, byrow=TRUE) cat(array_to_latex(A)) \\[\\begin{pmatrix} 3 &amp; 4 &amp; 5 \\\\ 6 &amp; 7 &amp; 9 \\\\ 4 &amp; 5 &amp; 122 \\end{pmatrix}\\] The fundamental objects in this text are scalars, vectors, and matrices. 28.0.1 Arrays Higher order arrays (for example, tensors in the tensorflow library) can be represented using subscript notation where \\([\\mathbf{A}_1 | \\mathbf{A}_2 | \\cdots \\mathbf{A}_n]\\) is a 3-dimensional array. Higher order arrays can be represented using additional subscripts. 28.0.2 Lists To add: vector addition, multiplication To add: matrix addition, multiplication To add: determinants 28.1 Exercises What is 3 + \\(\\begin{pmatrix} 4 \\\\ 7 \\\\ 3 \\end{pmatrix}\\)? Why can’t you add the following two vectors: \\[ \\begin{align*} \\mathbf{x} = \\begin{pmatrix} 14 \\\\ 3 \\\\ 3 \\\\ -5 \\end{pmatrix} &amp; &amp; \\mathbf{y} = \\begin{pmatrix} 4 \\\\ 7 \\\\ 3 \\end{pmatrix} \\end{align*} \\] Based on the notation, what type of object is \\(\\mathbf{x}&#39;\\)? \\(\\mathbf{x}&#39; \\mathbf{y}\\)? \\(\\mathbf{x}&#39; \\mathbf{A}\\)? \\(\\mathbf{A}&#39; \\mathbf{y}\\)? \\(\\mathbf{X}&#39; \\mathbf{Z}\\)? "]]
