# Linear Systems of Equations {#linear-systems-of-equations}

```{r, message = FALSE}
library(tidyverse)

# For 3-d plotting
# if devtools package not installed, install the package
if (!require(devtools)) {
    install.packages("devtools")
}
# if gg3D package not installed, install the package
if (!require(gg3D)) {
    devtools::install_github("AckerDWM/gg3D")
    library(gg3D)
}

# if dasc2594 package not installed, install the package
if (!require(dasc2594)) {
    devtools::install_github("jtipton25/dasc2594")
    library(dasc2594)
}
```

## Linear Systems of equations
### Linear equations

```{definition, name = "Linear Equations"}
Let $x_1, x_2, \ldots, x_n$ be **variables** with **coefficients** $a_1, a_2, \ldots, a_n$, and $b$ are fixed and known numbers. Then, we say

$$
\begin{align}
(\#eq:linear)
a_1 x_1 + a_2 x_2 + \cdots + a_n x_n & = b
\end{align}
$$

is a **linear equation**. 
```


```{example}
The equation for a line with slope $m$ and $y$-intercept $b$ is

$$
\begin{align*}
y & = m x + b,
\end{align*}
$$

is a linear equation because it can be re-written as 

$$
\begin{align*}
y - m x & = b,
\end{align*}
$$

where $a_1 = 1$, $a_2 = m$, $x_1 = y$ and $x_2 = x$.
```

```{example}
The equations

$$
\begin{align*}
\sqrt{19} x_1 & = (4 + \sqrt{2}) x_2 - x_3 - 9
& \mbox{ and } && 
-4 x_1 + 5 x_2 - 11 & = x_3
\end{align*}
$$

are both linear equations because they can be written as 

$$
\begin{align*}
\sqrt{19} x_1 - (4 + \sqrt{2}) x_2 + x_3 & = - 9 
& \mbox{ and } &&
-4 x_1 + 5 x_2 - x_3 & = 11,
\end{align*}
$$

respectively. 
```

```{example}
The equations

$$
\begin{align*}
x_1 & = x_2^2 + 3
& \mbox{ and } && 
x_1 + x_2 - x_1 x_2 & = 16
\end{align*}
$$

are not linear equations because they do not meet the form of \@ref(eq:linear) (The first equation above has a quadratic power of $x_2$ and the second equation has a product of $x_1$ and $x_2$).
```



### Systems of linear equations

```{definition, name = "System of equations"}
A set of two or more linear equations that each contain the same set of variables is called a **system of linear equations**. 
```


```{example}
The equations

$$
\begin{alignat*}{4}
x_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\
4 x_1 & {}+{} & 5 x_2 & {}+{} & 2 x_3 & {}={} & 9.
\end{alignat*}
$$

are a system of equations. Note that in the second equation, the coefficient for $x_3$ is 0, meaning we could re-write the above example as

$$
\begin{alignat*}{4}
x_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\
4 x_1 & {}+{} & 5 x_2 & {}+{} & 0 x_3 & {}={} & 9.
\end{alignat*}
$$
```



For the following, are these linear equations?

```{example}
$x_1 + 3 x_1 x_2 = 5$
```

```{solution}
This is a linear equation because the equation can be written as $a_1 x_1 + a_2 x_2 = b$ where $a_1 = 1$, $a_2 = 3$ and $b = 5$
```

```{example}
$5x + 7y + 8z = 11.2$
```

```{solution}
This is a linear equation because the equation can be written as $a_1 x_1 + a_2 x_2 + a_3 x_3= b$ where $a_1 = 5$, $a_2 = 7$, $a_3 = 8$ and $b = 11.2$. The variables $x_1 = x$, $x_2 = y$, and $x_3 = z$.
```

```{example}
$\frac{1}{4} y + \sqrt{2} z = 2^6$
```    
    
    
```{solution}
This is a linear equation because the equation can be written as $a_1 x_1 + a_2 x_2 = b$ where $a_1 = \frac{1}{4}$, $a_2 = \sqrt{2}$, and $b = 2^6$. The variables $x_1 = y$, $x_2 = z$ enter the equation linearly.
```


```{example}
$x + 4 y^2 = 9$
```

```{solution}
This is not a linear equation because the equation cannot be written as $a_1 x_1 + a_2 x_2 = b$ because $x_1 = x$ and $x_2 = y$ so that the equation is written as $a_1 x_1 + a_2 x_2^2 = b$ where $a_1 = 1$, $a_2 = 4$, and $b = 9$. The variable $x_2 = y$ enters the equation in a non-linear (quadratic) manner.
```

### Solutions of linear systems

A fundamental question when presented with a linear system of equations is whether the system has a solution. 

```{definition, name = "Solution of systems of equations"}
A solution to a system means that there are numbers $(s_1, s_2, \ldots, s_n)$ that each of the variables $x_1, x_2, \ldots, x_n$ take that allow for all the equations to simultaneously be true.
```

```{r, echo = FALSE}
set.seed(1)
A <- matrix(sample(-10:10, 6, replace = TRUE), 2, 3)
```

:::{.example}
Is $\begin{pmatrix}s_1 = 5 \\ s_2 = 3 \end{pmatrix}$ a solution to the system of equations
\begin{alignat*}{3}
x_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\
4 x_1 & {}+{} & 5 x_2 & {}+{} & 0 x_3 & {}={} & 9.
\end{alignat*}}
:::

:::{.example}
Consider the system of equations

$$
\begin{alignat*}{3}
x   & {}+{} & 4 y & {}={} & 8 \\
4 x & {}+{} & 5 y & {}={} & 7.
\end{alignat*}
$$

To find if a solution to this equation exists, we can do some algebra and take 4 times the top equation and then subtract the bottom equation, replacing the bottom equation with this new sum like


$$
\begin{alignat*}{3}
x   & {}+{} & 4 y & {}={} & 8 \\
4 x - 4 * (x) & {}+{} & 5 y - 4 * (4y) & {}={} & 7 - 4 * (8),
\end{alignat*}
$$
where the part of the equations in `()` is the top equation. This system of equations now simplifies to 

$$
\begin{alignat*}{3}
x & {}+{} & 4 y & {}={} & 8 \\
0 & {}+{} & - 11y & {}={} & -25,
\end{alignat*}
$$

which gives $y = \frac{25}{11}$. Plugging this value into the top equation gives

$$
\begin{alignat*}{3}
x & {}+{} & 4 \frac{25}{11} & {}={} & 8 \\
0 & {}+{} & y & {}={} & \frac{25}{11},
\end{alignat*}
$$

where we can solve $x = 8 - \frac{100}{11} = -\frac{12}{11}$ giving the solution of the form

$$
\begin{alignat*}{3}
x & {}+{} & 0 & {}={} & -\frac{12}{11} \\
0 & {}+{} & y & {}={} & \frac{25}{11},
\end{alignat*}
$$

In this case, the system of equation has the solution $x = -\frac{12}{11}$ and $y = \frac{25}{11}$. While finding the solution can be done algebraically, what does this mean visually (geometrically)? The original equations were 

$$
\begin{alignat*}{3}
x   & {}+{} & 4 y & {}={} & 8 \\
4 x & {}+{} & 5 y & {}={} & 7,
\end{alignat*}
$$

which, writing $y$ as a function of $x$ define two lines:

1) $y = -\frac{x}{4} + 2$
2) $y = -\frac{4x}{5} + \frac{7}{5}$

Let's plot these equations in `R` and see what they look like

```{r one-solution, fig.cap = "Linear system of equations with one solution"}
# define some grid points to evaluate the line
x <- seq(-2, 2, length = 1000)
dat <- data.frame(
    x = c(x, x),
    y = c(-x / 4 + 2, - 4 / 5 * x + 7/5),
    equation = factor(rep(c(1, 2), each = 1000))
)
glimpse(dat)

dat %>%
    ggplot(aes(x = x, y = y, color = equation, group = equation)) +
    geom_line() +
    scale_color_viridis_d(end = 0.8) +
    # solution x = -12/11, y = 25/11
    geom_point(aes(x = -12/11, y = 25/11), color = "red", size = 2) +
    ggtitle("Linear system of equations")
```

From this plot, it is clear that the solution to the system of equations is the location where the two lines intersect!
:::

### Types of solutions

Typically, there are 3 cases for the solutions to a system of linear equations

1) There are no solutions
2) There is one solution (Figure \@ref(fig:one-solution))
3) There are infinitely many solutions

```{definition}
A linear system of equations is called **consistent** if the system has either one or infinitely many solutions and is called **inconsistent** if the system has no solution.
```

#### There are no solutions:

Consider the system of linear equations

$$
\begin{alignat*}{3}
x   & {}+{} & 4 y & {}={} & 8 \\
4 x & {}+{} & 16 y & {}={} & 18.
\end{alignat*}
$$

```{r no-solution, fig.cap = "Linear system of equations with no solution"}
# define some grid points to evaluate the line
x <- seq(-2, 2, length = 1000)
dat <- data.frame(
    x = c(x, x),
    y = c(-x / 4 + 8 / 4, - x / 4 + 18 / 4),
    equation = factor(rep(c(1, 2), each = 1000))
)
glimpse(dat)

dat %>%
    ggplot(aes(x = x, y = y, color = equation, group = equation)) +
    geom_line() +
    scale_color_viridis_d(end = 0.8) +
    # solution x = -12/11, y = 25/11
    ggtitle("Linear system of equations")
```

In this case, the linear equations are parallel lines and will never intersect so therefore there is no solution.

#### There is one solution:

We have seen this example in Figure \@ref(fig:one-solution)

#### There are infinitely many solutions:

Consider the system of linear equations

$$
\begin{alignat*}{3}
x   & {}+{} & 4 y & {}={} & 8 \\
4 x & {}+{} & 16 y & {}={} & 32.
\end{alignat*}
$$

```{r infinite-solutions, fig.cap = "Linear system of equations with no solution"}
# define some grid points to evaluate the line
x <- seq(-2, 2, length = 1000)
dat <- data.frame(
    x = c(x, x),
    y = c(-x / 4 + 8 / 4, - 4 * x / 16 + 32 / 16),
    equation = factor(rep(c(1, 2), each = 1000))
)
glimpse(dat)

dat %>%
    ggplot(aes(x = x, y = y, color = equation, group = equation)) +
    geom_line() +
    scale_color_viridis_d(end = 0.8) +
    # solution x = -12/11, y = 25/11
    ggtitle("Linear system of equations")
```

In this case, the linear equations are perfectly overlapping lines and always intersect so therefore there are infinitely many solutions (all points on the line).

```{definition}
Two linear systems of equations are called **equivalent** if both systems share the same **solution set**.
```

For example, the system of equations
$$
\begin{alignat*}{4}
x_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\
4 x_1 & {}+{} & 5 x_2 & {}+{} & 2 x_3 & {}={} & 9
\end{alignat*}
$$

and the system of equations
$$
\begin{alignat*}{4}
2x_1   & {}+{} & 8 x_2 & {}-{} & 2 x_3 & {}={} & 22 \\
8 x_1 & {}+{} & 10 x_2 & {}+{} & 4 x_3 & {}={} & 18.
\end{alignat*}
$$

have the same solution set (the second set of equations is just 2 times the first set of equations).


* **Exercise/Lab: generate some equations, plot them, and determine if there is a solution. Then try to solve these using algebra.**


* **Exercises:** for the following systems of equations, determine if a solution(s) exist and if so, solve for the solution

```{exercise}
For the following system of equations, determine if a solution(s) exist and if so, solve for the solution
$$
\begin{alignat*}{3}
4 x_1 & {}+{} & 5 x_2 & {}={} & 8 \\
9 x_1 & {}-{} & 3 x_2 & {}={} & 4.
\end{alignat*}
$$
```


:::{.solution}
Multiply the first equation by $\frac{1}{4}$ and the second equation by $\frac{1}{9}$ to get 

$$
\begin{alignat*}{3}
x_1 & {}+{} & \frac{5}{4} x_2 & {}={} & 2 \\
x_1 & {}-{} &  \frac{1}{3}x_2 & {}={} & \frac{4}{9}.
\end{alignat*}
$$


Subtract the first equation from the second equation

$$
\begin{alignat*}{3}
x_1 & {}+{} & \frac{5}{4} x_2 & {}={} & 2 \\
0 & {}-{} &  (-\frac{1}{3} - \frac{5}{4}) x_2 & {}={} & \frac{4}{9} - 2,
\end{alignat*}
$$
which reduces to 
$$
\begin{alignat*}{3}
x_1 & {}+{} & \frac{5}{4} x_2 & {}={} & 2 \\
0 & {}-{} &  -\frac{19}{12} x_2 & {}={} & -\frac{14}{9},
\end{alignat*}
$$
so that, dividing both sides of the second equation by $-\frac{19}{12}$ gives $x_2 = \frac{56}{57}$. Plugging this value of $x_2$ into the first equation gives

$$
\begin{alignat*}{3}
x_1 & {}+{} & \frac{5}{4} \left(\frac{56}{57}\right) & {}={} & 2 \\
0 & {}-{} &   x_2 & {}={} & \frac{56}{57}
\end{alignat*}
$$

and subtracting $\frac{5}{4} \frac{56}{57}$ from both sides of the first equation gives $x_1 = \frac{44}{57}$. You can check these solutions by verifying that the following two equations hold

$$
\begin{alignat*}{3}
4 \left(\frac{44}{57}\right) & {}+{} & 5  \left(\frac{56}{57}\right) & {}={} & 8 \\
9 \left(\frac{44}{57}\right) & {}-{} & 3 \left(\frac{56}{57}\right)& {}={} & 4
\end{alignat*}
$$
which can be done in `R` using

```{r}
x1 <- 44/57
x2 <- 56/57
all.equal(4 * x1 + 5 * x2, 8)
all.equal(9 * x1 - 3 * x2, 4)
```
:::


```{exercise}
For the following system of equations, determine if a solution(s) exist and if so, solve for the solution
$$
\begin{alignat*}{4}
7 x_1 & {}+{} & 3 x_2 & {}+{} & 4 x_3 & {}={} & 5\\
4 x_1 & {}-{} & 5 x_2 &&        & {}={} & -2
\end{alignat*}
$$
```


```{exercise}
For the following system of equations, determine if a solution(s) exist and if so, solve for the solution
$$
\begin{alignat*}{3}
4 x_1 & {}-{} & 2 x_2 & {}={} & 8\\
2 x_1 & {}+{} & x_2 & {}={} & 7 \\
-3 x_1 & {}+{} & 6 x_2 &{}={} & 11
\end{alignat*}
$$
```


### Elementary row and column operations on matrices

The elementary row (column) operations include 

1) swaps: swapping two rows (columns),
2) sums: replacing a row (column) by the sum itself and a multiple of another row (column)
3) scalar multiplication: replacing a row (column) by a scalar multiple times itself

Note that these operations are exactly what we used to solve the equation using algebra above (except for swapping rows).



* **Add in examples in class here**



### The Augmented matrix form of a system of equations

Consider the linear system of equations 

$$
\begin{alignat*}{4}
x_1   & {}+{} & 4 x_2 & {}-{} & x_3 & {}={} & 11 \\
4 x_1 & {}+{} & 5 x_2 & {}+{} & 2 x_3 & {}={} & 9.
\end{alignat*}
$$
The augmented matrix representation of this system of linear equations is given by the matrix
$$
\begin{align*}
\begin{pmatrix}
1 & 4 & - 1 & 11 \\
4 & 5 & 2   &  9
\end{pmatrix},
\end{align*}
$$
where the first column of the matrix represents the variable $x_1$, the second column of the matrix represents the variable $x_2$, the third column of the matrix represents the variable $x_3$, and the fourth column of the matrix represents the constant terms. We can express the augmented form in `R` using a matrix
```{r}
augmented_matrix <- matrix(c(1, 4, 4, 5, -1, 2, 11, 9), 2, 4)
augmented_matrix
```
and to make clear the respective variables, we can add in column names as a matrix attribute using the `colnames()` function
```{r}
colnames(augmented_matrix) <- c("x1", "x2", "x3", "constants")
augmented_matrix
```

which adds labels to each of the columns.

Now, using elementary row operations on the matrix, we can attempt to find solutions to the system of equations. First, we multiply the first row by -4 and add it to the second row of the matrix and replace the second row with this sum
```{r}
augmented_matrix[2, ] <- -4 * augmented_matrix[1, ] + augmented_matrix[2, ]
augmented_matrix
```
Next, scale the second row to have a leading value of 1 by dividing by -11
```{r}
augmented_matrix[2, ] <- augmented_matrix[2, ] / (-11)
augmented_matrix
```
We can then multiply the second row by -4 and add it to the first row and replace the first row with this value.
```{r}
augmented_matrix[1, ] <- augmented_matrix[1, ] - 4 * augmented_matrix[2, ]
augmented_matrix
```

Notice how the matrix has a "triangular" form (The lower part of the "triangle" is made of 0s and the upper part has numbers).

The triangular form tells us that There are infinitely many solutions to this system of equation. The infinite solutions are subject to the requirements that 
$$x_1 = - \frac{19}{11} - \frac{13}{11} x_3$$
and
$$x_2 = \frac{35}{11} + \frac{6}{11} x_3.$$
To get this into a reasonable form, we will solve these equations as a function of $x_1$. Solving the first equation for $x_3$ gives
$$x_3 = - \frac{19}{13} -\frac{11}{13} x_1.$$
Then, plugging this into $x_3$ in the second equation gives 
$$
\begin{align*}
x_2 & = \frac{35}{11} + \frac{6}{11} \left( - \frac{19}{13} -\frac{11}{13} x_1 \right) \\
& = \frac{341}{143} - \frac{6}{13} x_1
\end{align*}
$$
which defines a linear relationship between $x_1$ and $x_2$. Notice that in these last two solutions, $x_1$ is a "free variable" and $x_2$ and $x_3$ are "determined" by $x_1$. 


In the plot below, the two planes (red and blue) are the geometric plots of the linear equations in the system of equations (the red plane is the top equation and the blue plane is the bottom equation). The purple line is the equation for the solution given the free variable $x_3$ and lies at the intersection of the two planes, much like the point in the two lines in figure **linking reference here** lies at the intersection of the two points. 


```{r}
# uses gg3D library
n <- 60
x1 <- x2 <- seq(-10, 10, length = n)
region <- expand.grid(x1 = x1, x2 = x2)
df <- data.frame(
    x1 = region$x1,
    x2 = region$x2,
    x3 = - 11 + (region$x1 + 4 * region$x2)
)

df2 <- data.frame(
    x1 = region$x1,
    x2 = region$x2,
    x3 = (9 - 4 * region$x1 - 5 * region$x2) / 2
) 

df_solution <- data.frame(
    x1 = x1, 
    x2 = 341 / 143 - 6 / 13 * x1,
    x3 = -19/13 - 11/13 * x1
) 

# theta and phi set up the "perspective/viewing angle" of the 3D plot
theta <- 63
phi <- -12
ggplot(df, aes(x1, x2, z = x3)) +
    axes_3D(theta = theta, phi = phi) +
    stat_wireframe(alpha = 0.25, color = "red", theta = theta, phi = phi) +
    stat_wireframe(data = df2, aes(x = x1, y = x2, z = x3), alpha = 0.25, color = "blue", theta = theta, phi = phi) +
    stat_3D(data = df_solution, aes(x1, x2, z = x3), geom = "line", theta = theta, phi = phi, color = "purple") +
    theme_void() +
    theme(legend.position = "none") +
    labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), angle=c(0, 0, 90), theta = theta, phi = phi) 
```

### Existence and Uniqueness

```{definition}
A system of linear equations is said to be consistent if at least one solution exists. The linear system of equations is said to have a **unique** solution if only one solution exists. 
```

* Example: (do in class) Is the system of linear equations consistent? IF the system is consistent, does it have a unique solution?

```{r, echo = FALSE}
A <- matrix(c(16, 5, 9, 4, 2, 11, 7, 14, 3, 10, 6, 15), 4, 3)
b <- c(13, 8, 12, 1)
```

$$
`r dasc2594::array_to_system(A, b)`
$$

* Example: (do in class) Is the system of linear equations consistent? If the system is consistent, does it have a unique solution?

```{r, echo = FALSE}
A <- matrix(c(1, 1, 3, 2, 3, 2, 3, 2, 1), 3, 3)
b <- c(5, 2, 7)
```

$$
`r dasc2594::array_to_system(A, b)`
$$

## Reduce row echelon form

Reducing a matrix to row echelon form is a useful technique for working with matrices. The row echelon form can be used to solve systems of equations, as well as determine other properties of a matrix that are yet to be discussed, including rank, invertibility, column/row spaces, etc. 


```{definition}

A matrix is said to be in **echelon** form if 

1) all nonzero rows are above any rows of zeros (all rows consisting entirely of zeros are at the bottom)

2) the leading entry/coefficient of a nonzero row (called the **pivot**) is always *strictly* to the right of the leading entry/coefficient of the row above
```

* Example: **echelon matrix example in class**

```{definition}
A matrix is in **reduced row echelon form** if it is in echelon form and

1) the leading entry/coefficient of each row is 1

2) The leading entry/coefficient of 1 is the only nonzero entry in its column.
```

* Example: **rref matrix example in class**

```{definition}
Echelon matrices have the property of being **upper diagonal**. A matrix is said to be **upper diagonal** if all entries of the matrix at or above the diagonal are nonzero. 
```

* Example: **lower and non-lower diagonal matrices 

```{definition}
Two matrices are **row-equivalent** if one matrix can be transformed to the other through elementary row operations.
```

```{theorem}
A nonzero matrix can be transformed into more than one echelon forms. However, the reduced row echelon form of a nonzero matrix is unique.
```

* **Exercise:** using elementary row operations, calculate the reduced row echelon form of the following matrices
1) **fill in later**
2) **fill in later**
3) **fill in later**


### Pivot positions

The leading entry/coefficients of a row echelon form matrix are called pivots. The positions of the pivot positions are the same for any row echelon form of a matrix. In reduced row echelon form, these pivot positions take the value 1.

```{definition}
In a matrix that is in reduced echelon form, the pivot position is the first nonzero element of each row. The column in which the pivot position occurs is called a pivot column.
```

* **Example: pivot position and pivot columns**

### Finding the reduced row echelon form

Calculating the reduced row echelon form is known as Gaussian elimination, which is named after [Johann Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss). This algorithm uses elementary row operations to calculate the reduced row echelon form. The following steps perform the Gaussian elimination algorithm.

1) Start with the left-most nonzero column, which is a pivot column
2) If the top row is zero, swap rows so that the top row is nonzero so that the top row has a nonzero element in the pivot position.
3) Use row multiplication and addition to zero out all positions in the pivot column below the top row (pivot position).
4) Ignore this top row and repeat steps 1-3 until there are no more nonzero rows to apply steps 1-3 on. At the end of this step, the matrix is in row echelon form.
5) Starting at the right-most pivot column, use elementary row operations to zero out all positions above each pivot and to make each pivot position 1. At the end of this step, the matrix is in reduced row echelon form.

* **Example: in class**

```{r}
# pracma library
# rref example in class
```

### Using reduced row echelon forms to solve systems of linear equations

When a system of linear equations is expressed as an augmented matrix, the reduced row echelon form can be used to find solutions to those systems of equations. Consider the systems of equations
```{r, echo = FALSE}
A <- matrix(c(3, 2, 4, 8, -4, 5, -4, -1, 0), 3, 3)
b <- c(6, 8, 9)
```

$$
`r dasc2594::array_to_system(A, b)`
$$

which can be written in the augmented matrix form as
$$
`r dasc2594::array_to_latex(cbind(A, b))`
$$

In `R`, this is the matrix

```{r}
# define matrix
```
Calculating the reduced row echelon form, gives
```{r}
# calculate rref of augmented matrix
```
which gives the solution ...

* **Exercise: calculate the RREF for the augmented matrix in the example above by hand**

Another example where we find a solution is

$$
\begin{align*}
5 x_1 && + && 4 x_2 && - && 2 x_3 && = & 0 \\
-3 x_1 && - && 2 x_2 && - && 4 x_3 && = & 1 \\
\end{align*}
$$

**Do same steps**

```{definition}
In a system of linear equations that is underdetermined (fewer equations than unknowns), the **determined/basic variables** are those variable that have a 1 in the respective columns when in reduced row echelon form (i.e., variables in a pivot position). The variables that are not in a pivot position are called **free variable**.
```

* **Example in class**

### Existence and uniquenss from reduced row echelon form

The row echelon form is useful to determine if a system of linear equations is consistent (the system of equations has a solution). To check if a solution to a linear system of equations exists, convert the system of equations to an augmented matrix form. Then, reduce the augmented matrix to row echelon form using elementary matrix operations. As long as there is not an equation of the form 
$$
0 = \mbox{constant}
$$
for some constant number not equal to 0, the system of linear equations is consistent. If the augmented matrix can be written in reduced row echelon form with no free variables, the solution to the linear system of equations is unique. These results give rise to the theorem

```{theorem}
A linear system of equations is consistent (has a solution) if the furthest right column (the constant column) is not a pivot column. If the system of equations is consistent, (i.e., the furthest right column is not a pivot column), the solution is unique if there are no free variables and there are infinitely many solutions if there is at least one free variable.
```

* **Example: consistent system of equations**

```{r, echo = FALSE, results = 'asis'}
set.seed(1)
A <- matrix(sample(-10:10, 12), 3, 4)
cat(array_to_latex(A))
```

* **Example: inconsistent system of equations**


```{r, echo = FALSE, results = 'asis'}
set.seed(1)
A <- matrix(sample(-10:10, 16), 4, 4)
cat(array_to_latex(A))
```









